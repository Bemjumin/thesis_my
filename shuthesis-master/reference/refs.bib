@misc{230607209DataCopilotBridging,
  title = {[2306.07209] {{Data-Copilot}}: {{Bridging Billions}} of {{Data}} and {{Humans}} with {{Autonomous Workflow}}},
  urldate = {2024-03-27},
  howpublished = {https://ar5iv.labs.arxiv.org/html/2306.07209}
}

@article{abdiPrincipalComponentAnalysis2010,
  title = {Principal Component Analysis: {{Principal}} Component Analysis},
  shorttitle = {Principal Component Analysis},
  author = {Abdi, Herv{\'e} and Williams, Lynne J.},
  year = {2010},
  month = jul,
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume = {2},
  number = {4},
  pages = {433--459},
  issn = {19395108},
  doi = {10.1002/wics.101},
  urldate = {2023-04-26},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\WYUMKN3B\Abdi 和 Williams - 2010 - Principal component analysis Principal component .pdf}
}

@misc{alyFEVEROUSFactExtraction2021,
  title = {{{FEVEROUS}}: {{Fact Extraction}} and {{VERification Over Unstructured}} and {{Structured}} Information},
  shorttitle = {{{FEVEROUS}}},
  author = {Aly, Rami and Guo, Zhijiang and Schlichtkrull, Michael and Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Cocarascu, Oana and Mittal, Arpit},
  year = {2021},
  month = oct,
  number = {arXiv:2106.05707},
  eprint = {2106.05707},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.05707},
  urldate = {2024-12-18},
  abstract = {Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18\% of the claims.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\JJAA6ICZ\\Aly 等 - 2021 - FEVEROUS Fact Extraction and VERification Over Un.pdf;C\:\\Users\\AA\\Zotero\\storage\\TMWYRFFQ\\2106.html}
}

@incollection{anguitaHumanActivityRecognition2012,
  title = {Human {{Activity Recognition}} on {{Smartphones Using}} a {{Multiclass Hardware-Friendly Support Vector Machine}}},
  booktitle = {Ambient {{Assisted Living}} and {{Home Care}}},
  author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and {Reyes-Ortiz}, Jorge L.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bravo, Jos{\'e} and Herv{\'a}s, Ram{\'o}n and Rodr{\'i}guez, Marcela},
  year = {2012},
  volume = {7657},
  pages = {216--223},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35395-6_30},
  urldate = {2023-04-25},
  isbn = {978-3-642-35394-9 978-3-642-35395-6}
}

@inproceedings{anguitaPublicDomainDataset2013,
  title = {A Public Domain Dataset for Human Activity Recognition Using Smartphones.},
  booktitle = {Esann},
  author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and {Reyes-Ortiz}, Jorge Luis},
  year = {2013},
  volume = {3},
  pages = {3},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\ZG6WJQCR\Anguita et al_2013_A public domain dataset for human activity recognition using smartphones.pdf}
}

@misc{ArXivorgEPrintArchive,
  title = {{{arXiv}}.Org e-{{Print}} Archive},
  urldate = {2025-02-17},
  howpublished = {https://arxiv.org/},
  file = {C:\Users\AA\Zotero\storage\AIKAD9KW\arxiv.org.html}
}

@article{baDeepNetsReally2014,
  title = {Do Deep Nets Really Need to Be Deep?},
  author = {Ba, Jimmy and Caruana, Rich},
  year = {2014},
  journal = {Advances in neural information processing systems},
  volume = {27},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\VCRWRERT\\Ba_Caruana_2014_Do deep nets really need to be deep.pdf;C\:\\Users\\AA\\Zotero\\storage\\6MVNL9YG\\ea8fcd92d59581717e06eb187f10666d-Abstract.html}
}

@misc{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2025-01-03},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\32XDLRPH\\Bahdanau 等 - 2016 - Neural Machine Translation by Jointly Learning to .pdf;C\:\\Users\\AA\\Zotero\\storage\\QSL5AUYG\\1409.html}
}

@misc{baiQwenTechnicalReport2023,
  title = {Qwen {{Technical Report}}},
  author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and Hui, Binyuan and Ji, Luo and Li, Mei and Lin, Junyang and Lin, Runji and Liu, Dayiheng and Liu, Gao and Lu, Chengqiang and Lu, Keming and Ma, Jianxin and Men, Rui and Ren, Xingzhang and Ren, Xuancheng and Tan, Chuanqi and Tan, Sinan and Tu, Jianhong and Wang, Peng and Wang, Shijie and Wang, Wei and Wu, Shengguang and Xu, Benfeng and Xu, Jin and Yang, An and Yang, Hao and Yang, Jian and Yang, Shusheng and Yao, Yang and Yu, Bowen and Yuan, Hongyi and Yuan, Zheng and Zhang, Jianwei and Zhang, Xingxuan and Zhang, Yichang and Zhang, Zhenru and Zhou, Chang and Zhou, Jingren and Zhou, Xiaohuan and Zhu, Tianhang},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16609},
  eprint = {2309.16609},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.16609},
  urldate = {2025-02-20},
  abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\FW92SAFL\\Bai 等 - 2023 - Qwen Technical Report.pdf;C\:\\Users\\AA\\Zotero\\storage\\Q4N27QH5\\2309.html}
}

@misc{balaguerRAGVsFinetuning2024,
  title = {{{RAG}} vs {{Fine-tuning}}: {{Pipelines}}, {{Tradeoffs}}, and a {{Case Study}} on {{Agriculture}}},
  shorttitle = {{{RAG}} vs {{Fine-tuning}}},
  author = {Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz de Freitas and Filho, Roberto de M. Estev{\~a}o and Hendry, Todd and Holstein, Daniel and Marsman, Jennifer and Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O. and Padilha, Rafael and Sharp, Morris and Silva, Bruno and Sharma, Swati and Aski, Vijay and Chandra, Ranveer},
  year = {2024},
  month = jan,
  number = {arXiv:2401.08406},
  eprint = {2401.08406},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-24},
  abstract = {There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47\% to 72\%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\AA\Zotero\storage\4W4CFD4A\Balaguer 等 - 2024 - RAG vs Fine-tuning Pipelines, Tradeoffs, and a Ca.pdf}
}

@misc{bangMultitaskMultilingualMultimodal2023,
  title = {A {{Multitask}}, {{Multilingual}}, {{Multimodal Evaluation}} of {{ChatGPT}} on {{Reasoning}}, {{Hallucination}}, and {{Interactivity}}},
  author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  year = {2023},
  month = nov,
  number = {arXiv:2302.04023},
  eprint = {2302.04023},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.04023},
  urldate = {2024-12-21},
  abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\74X84FLD\\Bang 等 - 2023 - A Multitask, Multilingual, Multimodal Evaluation o.pdf;C\:\\Users\\AA\\Zotero\\storage\\YFJ6FHUD\\2302.html}
}

@misc{bansalLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Annotators}}: {{Enhancing Generalization}} of {{NLP Models}} at {{Minimal Cost}}},
  shorttitle = {Large {{Language Models}} as {{Annotators}}},
  author = {Bansal, Parikshit and Sharma, Amit},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15766},
  eprint = {2306.15766},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gains in accuracy for both the training and target domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\AB5GB8I6\\Bansal 和 Sharma - 2023 - Large Language Models as Annotators Enhancing Gen.pdf;C\:\\Users\\AA\\Zotero\\storage\\4SAFZ6D4\\2306.html}
}

@article{baurModelOrderReduction2014,
  title = {Model {{Order Reduction}} for {{Linear}} and {{Nonlinear Systems}}: {{A System-Theoretic Perspective}}},
  shorttitle = {Model {{Order Reduction}} for {{Linear}} and {{Nonlinear Systems}}},
  author = {Baur, Ulrike and Benner, Peter and Feng, Lihong},
  year = {2014},
  month = dec,
  journal = {Archives of Computational Methods in Engineering},
  volume = {21},
  number = {4},
  pages = {331--358},
  issn = {1134-3060, 1886-1784},
  doi = {10.1007/s11831-014-9111-2},
  urldate = {2023-05-26},
  abstract = {In the past decades, Model Order Reduction (MOR) has demonstrated its robustness and wide applicability for simulating large-scale mathematical models in engineering and the sciences. Recently, MOR has been intensively further developed for increasingly complex dynamical systems. Wide applications of MOR have been found not only in simulation, but also in optimization and control. In this survey paper, we review some popular MOR methods for linear and nonlinear large-scale dynamical systems, mainly used in electrical and control engineering, in computational electromagnetics, as well as in micro- and nanoelectro-mechanical systems design. This complements recent surveys on generating reduced-order models for parameterdependent problems (Benner et al. in 2013; Boyaval et al. in Arch Comput Methods Eng 17(4):435--454, 2010; Rozza et al. Arch Comput Methods Eng 15(3):229--275, 2008) which we do not consider here. Besides reviewing existing methods and the computational techniques needed to implement them, open issues are discussed, and some new results are proposed.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\7CXT6P3T\Baur 等 - 2014 - Model Order Reduction for Linear and Nonlinear Sys.pdf}
}

@misc{BCEmbeddingREADME_zhmdMaster,
  title = {{{BCEmbedding}}/{{README}}\_zh.Md at Master {$\cdot$} Netease-Youdao/{{BCEmbedding}}},
  urldate = {2025-02-20},
  howpublished = {https://github.com/netease-youdao/BCEmbedding/blob/master/README\_zh.md},
  file = {C:\Users\AA\Zotero\storage\M5GKKVJ4\README_zh.html}
}

@article{benner$mathcalH_2$QuasiOptimalModel2018,
  title = {\${\textbackslash}mathcal {{H}}\_2\$-{{Quasi-Optimal Model Order Reduction}} for {{Quadratic-Bilinear Control Systems}}},
  author = {Benner, Peter and Goyal, Pawan and Gugercin, Serkan},
  year = {2018},
  month = jan,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {39},
  number = {2},
  pages = {983--1032},
  issn = {0895-4798, 1095-7162},
  doi = {10.1137/16M1098280},
  urldate = {2023-05-26},
  abstract = {We investigate the optimal model reduction problem for large-scale quadratic-bilinear (QB) control systems. Our contributions are threefold. First, we discuss the variational analysis and the Volterra series formulation for QB systems. We then define the H2-norm for a QB system based on the kernels of the underlying Volterra series and propose a truncated H2-norm as well. Next, we derive first-order necessary conditions for an optimal approximation, where optimality is measured in terms of the truncated H2-norm of the error system. We then propose an iterative model reduction algorithm, which upon convergence yields a reduced-order system that approximately satisfies the newly derived optimality conditions. We also discuss an efficient computation of the reduced Hessian, using the special Kronecker structure of the Hessian of the system. We illustrate the efficiency of the proposed method by means of several numerical examples resulting from semidiscretized nonlinear partial differential equations and show its competitiveness with existing model reduction schemes such as moment-matching and balanced truncation for QB systems by comparing accuracy in the time-domain simulations and in the truncated H2-norm.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\XGYMAWS5\Benner 等 - 2018 - $mathcal H_2$-Quasi-Optimal Model Order Reduction.pdf}
}

@article{besselinkModelReductionNonlinear2014,
  title = {Model {{Reduction}} for {{Nonlinear Systems}} by {{Incremental Balanced Truncation}}},
  author = {Besselink, Bart and Van De Wouw, Nathan and Scherpen, Jacquelien M. A. and Nijmeijer, Henk},
  year = {2014},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {59},
  number = {10},
  pages = {2739--2753},
  issn = {0018-9286, 1558-2523},
  doi = {10.1109/TAC.2014.2326548},
  urldate = {2023-05-26},
  abstract = {In this paper, the method of incremental balanced truncation is introduced as a tool for model reduction of nonlinear systems. Incremental balanced truncation provides an extension of balanced truncation for linear systems towards the nonlinear case and differs from existing nonlinear balancing techniques in the definition of two novel energy functions. These incremental observability and incremental controllability functions form the basis for a model reduction procedure in which the preservation of stability properties is guaranteed. In particular, the property of incremental stability, which provides a notion of stability for systems with nonzero inputs, is preserved. Moreover, a computable error bound is given. Next, an extension towards so-called generalized incremental balanced truncation is proposed, which provides a reduction technique with increased computational feasibility at the cost of a (potentially) larger error bound. The proposed reduction technique is illustrated by means of application to an example of an electronic circuit with nonlinear elements.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\FCWAQ2SU\Besselink 等 - 2014 - Model Reduction for Nonlinear Systems by Increment.pdf}
}

@inproceedings{bielefeldtDeepTestUsingMachine2024,
  title = {{{DeepTest}}: {{Using Machine Learning}} for {{Generating}} New {{Testsequences}}},
  shorttitle = {{{DeepTest}}},
  booktitle = {2024 13th {{Mediterranean Conference}} on {{Embedded Computing}} ({{MECO}})},
  author = {Bielefeldt, Jens and Basener, Kai-Uwe and Krajewski, Roman and Wiesbrock, Hans-Werner and Reichenbach, Marc and H{\"u}bner, Michael},
  year = {2024},
  pages = {1--9},
  publisher = {IEEE},
  urldate = {2025-02-19}
}

@article{biradarReducedorderModelingLinear2016,
  title = {Reduced-Order Modeling of Linear Time Invariant Systems Using Big Bang Big Crunch Optimization and Time Moment Matching Method},
  author = {Biradar, Shivanagouda and Hote, Yogesh V. and Saxena, Sahaj},
  year = {2016},
  month = aug,
  journal = {Applied Mathematical Modelling},
  volume = {40},
  number = {15-16},
  pages = {7225--7244},
  issn = {0307904X},
  doi = {10.1016/j.apm.2016.03.006},
  urldate = {2023-05-26},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\ZPHCH2PI\Biradar 等 - 2016 - Reduced-order modeling of linear time invariant sy.pdf}
}

@article{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
  urldate = {2023-12-18},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\RDGMRA2V\Brown 等 - 2020 - Language models are few-shot learners.pdf}
}

@inproceedings{buciluaModelCompression2006,
  title = {Model Compression},
  booktitle = {Proceedings of the 12th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Bucilu{\v a}, Cristian and Caruana, Rich and {Niculescu-Mizil}, Alexandru},
  year = {2006},
  pages = {535--541},
  doi = {10.1145/1150402.1150464},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LXLFQ2RQ\\Buciluǎ et al_2006_Model compression.pdf;C\:\\Users\\AA\\Zotero\\storage\\F3B98TGQ\\1150402.html}
}

@article{caoBRITSBidirectionalRecurrent,
  title = {{{BRITS}}: {{Bidirectional Recurrent Imputation}} for {{Time Series}}},
  author = {Cao, Wei and Wang, Dong and Li, Jian and Zhou, Hao and Li, Lei and Li, Yitan},
  abstract = {Time series are ubiquitous in many classification/regression applications. However, the time series data in real applications may contain many missing values. Hence, given multiple (possibly correlated) time series data, it is important to fill in missing values and at the same time to predict their class labels. Existing imputation methods often impose strong assumptions of the underlying data generating process, such as linear dynamics in the state space. In this paper, we propose a novel method, called BRITS, based on recurrent neural networks for missing value imputation in time series data. Our proposed method directly learns the missing values in a bidirectional recurrent dynamical system, without any specific assumption. The imputed values are treated as variables of RNN graph and can be effectively updated during backpropagation. BRITS has three advantages: (a) it can handle multiple correlated missing values in time series; (b) it generalizes to time series with nonlinear dynamics underlying; (c) it provides a data-driven imputation procedure and applies to general settings with missing data. We evaluate our model on three real-world datasets, including an air quality dataset, a healthcare dataset, and a localization dataset for human activity. Experiments show that our model outperforms the state-of-the-art methods in both imputation and classification/regression.},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\ASXZES3G\Cao 等 - BRITS Bidirectional Recurrent Imputation for Time.pdf}
}

@article{cauwenberghsAnalogVLSIRecurrent1996,
  title = {An Analog {{VLSI}} Recurrent Neural Network Learning a Continuous-Time Trajectory},
  author = {Cauwenberghs, G.},
  year = {1996},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {7},
  number = {2},
  pages = {346--361},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/72.485671},
  urldate = {2023-04-12},
  abstract = {Real-time algorithms for gradient descent supervised learning in recurrent dynamical neural networks fail to support scalable VLSI (very large scale integration) implementation, due to their complexity which grows sharply with the network dimension. We present an alternative implementation in analog VLSI, which employs a stochastic perturbative algorithm to observe the gradient of the error index directly on the network in random directions of the parameter space, thereby avoiding the tedious task of deriving the gradient from an explicit model of the network dynamics. The network contains six fully recurrent neurons with continuous-time dynamics, providing 42 free parameters which comprise connection strengths and thresholds. The chip implementing the network includes local provisions supporting both the learning and storage of the parameters, integrated in a scalable architecture which can be readily expanded for applications of learning recurrent dynamical networks requiring larger dimensionality. We describe and characterize the functional elements comprising the implemented recurrent network and integrated learning system, and include experimental results obtained from training the network to produce two outputs following a circular trajectory, representing a quadrature-phase oscillator.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\ZRPVTPZZ\Cauwenberghs - 1996 - An analog VLSI recurrent neural network learning a.pdf}
}

@misc{changSurveyEvaluationLarge2023,
  title = {A {{Survey}} on {{Evaluation}} of {{Large Language Models}}},
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
  year = {2023},
  month = oct,
  number = {arXiv:2307.03109},
  eprint = {2307.03109},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\AA\Zotero\storage\9L9DNNIY\2307.html}
}

@article{chatterjeeIntroductionProperOrthogonal2000,
  title = {An Introduction to the Proper Orthogonal Decomposition},
  author = {Chatterjee, Anindya},
  year = {2000},
  journal = {Current science},
  pages = {808--817},
  publisher = {JSTOR},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\YJXYKBZH\\Chatterjee_2000_An introduction to the proper orthogonal decomposition.pdf;C\:\\Users\\AA\\Zotero\\storage\\9WAW9TKW\\24103957.html}
}

@inproceedings{chaturantabutDiscreteEmpiricalInterpolation2009,
  title = {Discrete {{Empirical Interpolation}} for Nonlinear Model Reduction},
  booktitle = {Proceedings of the 48h {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}}) Held Jointly with 2009 28th {{Chinese Control Conference}}},
  author = {Chaturantabut, Saifon and Sorensen, Danny C.},
  year = {2009},
  month = dec,
  pages = {4316--4321},
  publisher = {IEEE},
  address = {Shanghai, China},
  doi = {10.1109/CDC.2009.5400045},
  urldate = {2023-05-26},
  abstract = {A dimension reduction method called Discrete Empirical Interpolation (DEIM) is proposed and shown to dramatically reduce the computational complexity of the popular Proper Orthogonal Decomposition (POD) method for constructing reduced-order models for unsteady and/or parametrized nonlinear partial differential equations (PDEs). In the presence of a general nonlinearity, the standard POD-Galerkin technique reduces dimension in the sense that far fewer variables are present, but the complexity of evaluating the nonlinear term remains that of the original problem. Here we describe DEIM as a modification of POD that reduces the complexity as well as the dimension of general nonlinear systems of ordinary differential equations (ODEs). It is, in particular, applicable to ODEs arising from finite difference discretization of unsteady time dependent PDE and/or parametrically dependent steady state problems. Our contribution is a greatly simplified description of Empirical Interpolation in a finite dimensional setting. The method possesses an error bound on the quality of approximation. An application of DEIM to a finite difference discretization of the 1-D FitzHugh-Nagumo equations is shown to reduce the dimension from 1024 to order 5 variables with negligible error over a long-time integration that fully captures non-linear limit cycle behavior. We also demonstrate applicability in higher spatial dimensions with similar state space dimension reduction and accuracy results.},
  isbn = {978-1-4244-3871-6},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\5E2LWJYI\Chaturantabut 和 Sorensen - 2009 - Discrete Empirical Interpolation for nonlinear mod.pdf}
}

@article{chaturantabutNonlinearModelReduction2010,
  title = {Nonlinear Model Reduction via Discrete Empirical Interpolation},
  author = {Chaturantabut, Saifon and Sorensen, Danny C.},
  year = {2010},
  journal = {SIAM Journal on Scientific Computing},
  volume = {32},
  number = {5},
  pages = {2737--2764},
  publisher = {SIAM},
  doi = {10.1137/090766498},
  file = {C\:\\Users\\AA\\Zotero\\storage\\PUTTSN7X\\Chaturantabut_Sorensen_2010_Nonlinear model reduction via discrete empirical interpolation.pdf;C\:\\Users\\AA\\Zotero\\storage\\BY5A4VM7\\090766498.html}
}

@inproceedings{chenCLIP2SceneLabelefficient3D2023,
  title = {{{CLIP2Scene}}: {{Towards Label-efficient 3D Scene Understanding}} by {{CLIP}}},
  shorttitle = {{{CLIP2Scene}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Runnan and Liu, Youquan and Kong, Lingdong and Zhu, Xinge and Ma, Yuexin and Li, Yikang and Hou, Yuenan and Qiao, Yu and Wang, Wenping},
  year = {2023},
  pages = {7020--7030},
  urldate = {2023-11-20},
  file = {C:\Users\AA\Zotero\storage\44ES32NF\Chen 等 - 2023 - CLIP2Scene Towards Label-efficient 3D Scene Under.pdf}
}

@inproceedings{chenCompressingNeuralNetworks2015,
  title = {Compressing Neural Networks with the Hashing Trick},
  booktitle = {International Conference on Machine Learning},
  author = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
  year = {2015},
  pages = {2285--2294},
  publisher = {PMLR},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\R3SW7AE2\\Chen et al_2015_Compressing neural networks with the hashing trick.pdf;C\:\\Users\\AA\\Zotero\\storage\\UWB9XEP4\\chenc15.html}
}

@misc{chenDenseRetrievalWhat2024,
  title = {Dense {{X Retrieval}}: {{What Retrieval Granularity Should We Use}}?},
  shorttitle = {Dense {{X Retrieval}}},
  author = {Chen, Tong and Wang, Hongwei and Chen, Sihao and Yu, Wenhao and Ma, Kaixin and Zhao, Xinran and Zhang, Hongming and Yu, Dong},
  year = {2024},
  month = oct,
  number = {arXiv:2312.06648},
  eprint = {2312.06648},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.06648},
  urldate = {2024-12-21},
  abstract = {Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {C\:\\Users\\AA\\Zotero\\storage\\2MB7XWNJ\\Chen 等 - 2024 - Dense X Retrieval What Retrieval Granularity Shou.pdf;C\:\\Users\\AA\\Zotero\\storage\\HGHZB9WI\\2312.html}
}

@misc{chengBindingLanguageModels2023,
  title = {Binding {{Language Models}} in {{Symbolic Languages}}},
  author = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  year = {2023},
  month = feb,
  number = {arXiv:2210.02875},
  eprint = {2210.02875},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-19},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\DI3YCNY7\\Cheng et al_2023_Binding Language Models in Symbolic Languages.pdf;C\:\\Users\\AA\\Zotero\\storage\\6EVS4B6X\\2210.html}
}

@misc{chengUPRISEUniversalPrompt2023,
  title = {{{UPRISE}}: {{Universal Prompt Retrieval}} for {{Improving Zero-Shot Evaluation}}},
  shorttitle = {{{UPRISE}}},
  author = {Cheng, Daixuan and Huang, Shaohan and Bi, Junyu and Zhan, Yuefeng and Liu, Jianfeng and Wang, Yujing and Sun, Hao and Wei, Furu and Deng, Denvy and Zhang, Qi},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08518},
  eprint = {2303.08518},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08518},
  urldate = {2024-12-21},
  abstract = {Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\56PFNX2K\\Cheng 等 - 2023 - UPRISE Universal Prompt Retrieval for Improving Z.pdf;C\:\\Users\\AA\\Zotero\\storage\\DNVSIK6M\\2303.html}
}

@misc{chenLabelfreeNodeClassification2023,
  title = {Label-Free {{Node Classification}} on {{Graphs}} with {{Large Language Models}} ({{LLMS}})},
  author = {Chen, Zhikai and Mao, Haitao and Wen, Hongzhi and Han, Haoyu and Jin, Wei and Zhang, Haiyang and Liu, Hui and Tang, Jiliang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.04668},
  eprint = {2310.04668},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9\% on a vast-scale dataset {\textbackslash}products with a cost less than 1 dollar.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\ZS43KD7G\\Chen 等 - 2023 - Label-free Node Classification on Graphs with Larg.pdf;C\:\\Users\\AA\\Zotero\\storage\\VG2WQMQ7\\2310.html}
}

@misc{chenLargeLanguageModels2023,
  title = {Large {{Language Models}} Are Few(1)-Shot {{Table Reasoners}}},
  author = {Chen, Wenhu},
  year = {2023},
  month = jan,
  number = {arXiv:2210.06710},
  eprint = {2210.06710},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.06710},
  urldate = {2024-03-21},
  abstract = {Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\P28VZWZA\\Chen_2023_Large Language Models are few(1)-shot Table Reasoners.pdf;C\:\\Users\\AA\\Zotero\\storage\\9IAI32IX\\2210.html}
}

@article{chenNeuralOrdinaryDifferential,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\FTDE6VZQ\Chen 等 - Neural Ordinary Differential Equations.pdf}
}

@misc{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  number = {arXiv:1806.07366},
  eprint = {1806.07366},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\3F93BB68\Chen 等 - 2019 - Neural Ordinary Differential Equations.pdf}
}

@misc{chenPhoenixDemocratizingChatGPT2023,
  title = {Phoenix: {{Democratizing ChatGPT}} across {{Languages}}},
  shorttitle = {Phoenix},
  author = {Chen, Zhihong and Jiang, Feng and Chen, Junying and Wang, Tiannan and Yu, Fei and Chen, Guiming and Zhang, Hongbo and Liang, Juhao and Zhang, Chen and Zhang, Zhiyi and Li, Jianquan and Wan, Xiang and Wang, Benyou and Li, Haizhou},
  year = {2023},
  month = apr,
  number = {arXiv:2304.10453},
  eprint = {2304.10453},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.10453},
  urldate = {2024-12-19},
  abstract = {This paper presents our efforts to democratize ChatGPT across language. We release a large language model "Phoenix", achieving competitive performance among open-source English and Chinese models while excelling in languages with limited resources (covering both Latin and non-Latin languages). We believe this work will be beneficial to make ChatGPT more accessible, especially in countries where people cannot use ChatGPT due to restrictions from OpenAI or local goverments. Our data, code, and models are available at https://github.com/FreedomIntelligence/LLMZoo.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\63VWLNA9\\Chen 等 - 2023 - Phoenix Democratizing ChatGPT across Languages.pdf;C\:\\Users\\AA\\Zotero\\storage\\9HD5UYAZ\\2304.html}
}

@misc{chenProgramThoughtsPrompting2023,
  title = {Program of {{Thoughts Prompting}}: {{Disentangling Computation}} from {{Reasoning}} for {{Numerical Reasoning Tasks}}},
  shorttitle = {Program of {{Thoughts Prompting}}},
  author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
  year = {2023},
  month = oct,
  number = {arXiv:2211.12588},
  eprint = {2211.12588},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-19},
  abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12{\textbackslash}\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\E2W9APZW\\Chen et al_2023_Program of Thoughts Prompting.pdf;C\:\\Users\\AA\\Zotero\\storage\\EVQB5Y95\\2211.html}
}

@misc{chenTabFactLargescaleDataset2020,
  title = {{{TabFact}}: {{A Large-scale Dataset}} for {{Table-based Fact Verification}}},
  shorttitle = {{{TabFact}}},
  author = {Chen, Wenhu and Wang, Hongmin and Chen, Jianshu and Zhang, Yunkai and Wang, Hong and Li, Shiyang and Zhou, Xiyou and Wang, William Yang},
  year = {2020},
  month = jun,
  number = {arXiv:1909.02164},
  eprint = {1909.02164},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-22},
  abstract = {The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in {\textbackslash}url\{https://github.com/wenhuchen/Table-Fact-Checking\}.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\TILSM2WZ\\Chen et al_2020_TabFact.pdf;C\:\\Users\\AA\\Zotero\\storage\\KDEMYBHC\\1909.html}
}

@misc{chenTabFactLargescaleDataset2020a,
  title = {{{TabFact}}: {{A Large-scale Dataset}} for {{Table-based Fact Verification}}},
  shorttitle = {{{TabFact}}},
  author = {Chen, Wenhu and Wang, Hongmin and Chen, Jianshu and Zhang, Yunkai and Wang, Hong and Li, Shiyang and Zhou, Xiyou and Wang, William Yang},
  year = {2020},
  month = jun,
  number = {arXiv:1909.02164},
  eprint = {1909.02164},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.02164},
  urldate = {2024-12-18},
  abstract = {The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains under-explored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which are labeled as either ENTAILED or REFUTED. TabFact is challenging since it involves both soft linguistic reasoning and hard symbolic reasoning. To address these reasoning challenges, we design two different models: Table-BERT and Latent Program Algorithm (LPA). Table-BERT leverages the state-of-the-art pre-trained language model to encode the linearized tables and statements into continuous vectors for verification. LPA parses statements into programs and executes them against the tables to obtain the returned binary value for verification. Both methods achieve similar accuracy but still lag far behind human performance. We also perform a comprehensive analysis to demonstrate great future opportunities. The data and code of the dataset are provided in {\textbackslash}url\{https://github.com/wenhuchen/Table-Fact-Checking\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\5KVUQKYB\\Chen 等 - 2020 - TabFact A Large-scale Dataset for Table-based Fac.pdf;C\:\\Users\\AA\\Zotero\\storage\\89CSK5A6\\1909.html}
}

@article{cheRecurrentNeuralNetworks2018,
  title = {Recurrent {{Neural Networks}} for {{Multivariate Time Series}} with {{Missing Values}}},
  author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
  year = {2018},
  month = apr,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {6085},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24271-9},
  urldate = {2023-04-12},
  abstract = {Abstract                            Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a.,               informative               missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely               GRU               -               D               , as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e.,               masking               and               time interval               , and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\6KK9SM23\Che 等 - 2018 - Recurrent Neural Networks for Multivariate Time Se.pdf}
}

@inproceedings{chevalDEEPSECDecidingEquivalence2018,
  title = {{{DEEPSEC}}: Deciding Equivalence Properties in Security Protocols Theory and Practice},
  shorttitle = {{{DEEPSEC}}},
  booktitle = {2018 {{IEEE}} Symposium on Security and Privacy ({{SP}})},
  author = {Cheval, Vincent and Kremer, Steve and Rakotonirina, Itsaka},
  year = {2018},
  pages = {529--546},
  publisher = {IEEE},
  urldate = {2025-02-19},
  file = {C:\Users\AA\Zotero\storage\5SVJ9TQB\Cheval 等 - 2018 - DEEPSEC deciding equivalence properties in securi.pdf}
}

@misc{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  number = {arXiv:1406.1078},
  eprint = {1406.1078},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.1078},
  urldate = {2023-05-08},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\3RYWFV2A\\Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf;C\:\\Users\\AA\\Zotero\\storage\\AZZM8U86\\1406.html}
}

@misc{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  month = oct,
  number = {arXiv:1409.1259},
  eprint = {1409.1259},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1259},
  urldate = {2023-05-08},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\XA2PKKST\\Cho et al_2014_On the Properties of Neural Machine Translation.pdf;C\:\\Users\\AA\\Zotero\\storage\\T7XIWQ2X\\1409.html}
}

@article{chowdheryPalmScalingLanguage2023,
  title = {Palm: {{Scaling}} Language Modeling with Pathways},
  shorttitle = {Palm},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {240},
  pages = {1--113},
  urldate = {2023-12-26},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\PHYUNEU2\Chowdhery 等 - 2023 - Palm Scaling language modeling with pathways.pdf}
}

@misc{chungEmpiricalEvaluationGated2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  number = {arXiv:1412.3555},
  eprint = {1412.3555},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.3555},
  urldate = {2023-05-08},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\T73PS3H8\\Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf;C\:\\Users\\AA\\Zotero\\storage\\K48L42EW\\1412.html}
}

@misc{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and {Castro-Ros}, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = dec,
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-14},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\AA\Zotero\storage\9QQFDANY\Chung 等 - 2022 - Scaling Instruction-Finetuned Language Models.pdf}
}

@article{cuiDriveLLMChartingPath2023,
  title = {{{DriveLLM}}: {{Charting The Path Toward Full Autonomous Driving}} with {{Large Language Models}}},
  shorttitle = {{{DriveLLM}}},
  author = {Cui, Yaodong and Huang, Shucheng and Zhong, Jiaming and Liu, Zhenan and Wang, Yutong and Sun, Chen and Li, Bai and Wang, Xiao and Khajepour, Amir},
  year = {2023},
  journal = {IEEE Transactions on Intelligent Vehicles},
  pages = {1--15},
  issn = {2379-8904, 2379-8858},
  doi = {10.1109/TIV.2023.3327715},
  urldate = {2023-11-16},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\I3TQWD2S\Cui 等 - 2023 - DriveLLM Charting The Path Toward Full Autonomous.pdf}
}

@article{cuiJointStructuredPruning2021,
  title = {Joint Structured Pruning and Dense Knowledge Distillation for Efficient Transformer Model Compression},
  author = {Cui, Baiyun and Li, Yingming and Zhang, Zhongfei},
  year = {2021},
  month = oct,
  journal = {Neurocomputing},
  volume = {458},
  pages = {56--69},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.05.084},
  urldate = {2023-06-16},
  abstract = {In this paper, we develop a novel Joint Model Compression (referred to as JMC) method by combining structured pruning and dense knowledge distillation techniques to significantly compress original large language model into a deep compressed shallow network. In particular, a new Direct Importance-aware Structured Pruning (referred as DISP) approach is proposed to structurally prune the redundant structures in the Transformer networks directly based on the corresponding parameter matrices in the model. Besides, a Dense Knowledge Distillation (referred to as DKD) method is developed with a many-to-one layer mapping strategy to leverage more comprehensive layer-wise linguistic knowledge for the distillation. Further, the proposed structured pruning and dense knowledge distillation are integrated together to perform the joint compression, which enables us to achieve a significant compression without sacrificing model accuracy. The extensive experimental results across four NLP tasks on seven datasets demonstrate its effectiveness and superiority to the baselines, while maintaining similar performance to original large model with further remarkable benefits for inference-time speedup and memory efficiency.},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\ENMUDA27\\Cui et al_2021_Joint structured pruning and dense knowledge distillation for efficient.pdf;C\:\\Users\\AA\\Zotero\\storage\\9NWXVMQI\\S0925231221008390.html}
}

@article{cybenkotApproximationSuperpositionsSigmoidal,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenkot, G},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\DUK4FI92\Cybenkot - Approximation by superpositions of a sigmoidal fun.pdf}
}

@misc{DataCopilotBridgingBillions,
  title = {Data-{{Copilot}}: {{Bridging Billions}} of {{Data}} and {{Humans}} with {{Autonomous Workflow}}},
  shorttitle = {Data-{{Copilot}}},
  journal = {ar5iv},
  urldate = {2024-03-27},
  abstract = {Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it nece{\dots}},
  howpublished = {https://ar5iv.labs.arxiv.org/html/2306.07209},
  langid = {english}
}

@article{delgadoDynamicRecurrentNeural1995,
  title = {Dynamic Recurrent Neural Network for System Identification and Control},
  author = {Delgado, A. and Kambhampati, C. and Warwick, K.},
  year = {1995},
  month = jul,
  journal = {IEE Proceedings - Control Theory and Applications},
  volume = {142},
  number = {4},
  pages = {307--314},
  issn = {1350-2379, 1359-7035},
  doi = {10.1049/ip-cta:19951873},
  urldate = {2023-05-26},
  abstract = {A dynamic recurrent neural network (DRNN) that can be viewed as a generalisation of the Hopfield neural network is proposed to identify and control a class of control a c n e systems. In this approach, the identified network is used in the context of the differential geometric control to synthesise a state feedback that cancels the nonlinear terms of the plant yielding a linear plant which can then be controlled using a standard PID controller.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\2JRRI577\Delgado 等 - 1995 - Dynamic recurrent neural network for system identi.pdf}
}

@incollection{dengIntroductionQueryUnderstanding2020,
  title = {An {{Introduction}} to {{Query Understanding}}},
  booktitle = {Query {{Understanding}} for {{Search Engines}}},
  author = {Deng, Hongbo and Chang, Yi},
  editor = {Chang, Yi and Deng, Hongbo},
  year = {2020},
  volume = {46},
  pages = {1--13},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-58334-7_1},
  urldate = {2024-07-19},
  isbn = {978-3-030-58333-0 978-3-030-58334-7},
  langid = {english}
}

@article{dengModelCompressionHardware2020,
  title = {Model {{Compression}} and {{Hardware Acceleration}} for {{Neural Networks}}: {{A Comprehensive Survey}}},
  shorttitle = {Model {{Compression}} and {{Hardware Acceleration}} for {{Neural Networks}}},
  author = {Deng, By Lei and Li, Guoqi and Han, Song and Shi, Luping and Xie, Yuan},
  year = {2020},
  month = apr,
  journal = {Proceedings of the IEEE},
  volume = {108},
  number = {4},
  pages = {485--532},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2020.2976475},
  urldate = {2023-04-12},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\YA6NW72R\Deng 等 - 2020 - Model Compression and Hardware Acceleration for Ne.pdf}
}

@inproceedings{dengStructureGroundedPretrainingTexttoSQL2021,
  title = {Structure-{{Grounded Pretraining}} for {{Text-to-SQL}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Deng, Xiang and Awadallah, Ahmed Hassan and Meek, Christopher and Polozov, Oleksandr and Sun, Huan and Richardson, Matthew},
  year = {2021},
  eprint = {2010.12773},
  primaryclass = {cs},
  pages = {1337--1350},
  doi = {10.18653/v1/2021.naacl-main.105},
  urldate = {2024-12-18},
  abstract = {Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (StruG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel prediction tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT-LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. The Spider-Realistic dataset is available at https://doi.org/10.5281/zenodo.5205322.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\ZFHWF886\\Deng 等 - 2021 - Structure-Grounded Pretraining for Text-to-SQL.pdf;C\:\\Users\\AA\\Zotero\\storage\\DI4SJWCU\\2010.html}
}

@article{denilPredictingParametersDeep2013,
  title = {Predicting Parameters in Deep Learning},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and De Freitas, Nando},
  year = {2013},
  journal = {Advances in neural information processing systems},
  volume = {26},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\EZUK78J3\\Denil et al_2013_Predicting parameters in deep learning.pdf;C\:\\Users\\AA\\Zotero\\storage\\ISNKWVEC\\7fec306d1e665bc9c748b5d2b99a6e97-Abstract.html}
}

@article{dentonExploitingLinearStructure,
  title = {Exploiting {{Linear Structure Within Convolutional Networks}} for {{Efficient Evaluation}}},
  author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2{\texttimes}, while keeping the accuracy within 1\% of the original model.},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\RVTEAQGR\Denton 等 - Exploiting Linear Structure Within Convolutional N.pdf}
}

@article{devlinBertPretrainingDeep2018,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.04805},
  eprint = {1810.04805},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\H28FYF87\\Devlin et al_2018_Bert.pdf;C\:\\Users\\AA\\Zotero\\storage\\P22IWAQ5\\1810.html}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-12-22},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\N8DE9ZHH\\Devlin 等 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\AA\\Zotero\\storage\\F8DNJ3DE\\1810.html}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-18},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\CQWMKB4K\\Devlin 等 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\AA\\Zotero\\storage\\FBEBIH4M\\1810.html}
}

@misc{dingPromptLearningFineGrainedEntity2021,
  title = {Prompt-{{Learning}} for {{Fine-Grained Entity Typing}}},
  author = {Ding, Ning and Chen, Yulin and Han, Xu and Xu, Guangwei and Xie, Pengjun and Zheng, Hai-Tao and Liu, Zhiyuan and Li, Juanzi and Kim, Hong-Gee},
  year = {2021},
  month = aug,
  number = {arXiv:2108.10604},
  eprint = {2108.10604},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-14},
  abstract = {As an effective approach to tune pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using {\textbackslash}textit\{cloze\}-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on three fine-grained entity typing benchmarks (with up to 86 classes) under fully supervised, few-shot and zero-shot settings show that prompt-learning methods significantly outperform fine-tuning baselines, especially when the training data is insufficient.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\PP4FKS5L\\Ding 等 - 2021 - Prompt-Learning for Fine-Grained Entity Typing.pdf;C\:\\Users\\AA\\Zotero\\storage\\4IZGBRVY\\2108.html}
}

@article{donesApplicationBalancedTruncation2011,
  title = {Application of {{Balanced Truncation}} to {{Nonlinear Systems}}},
  author = {Dones, Ivan and Skogestad, Sigurd and Preisig, Heinz A.},
  year = {2011},
  month = sep,
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {50},
  number = {17},
  pages = {10093--10101},
  issn = {0888-5885, 1520-5045},
  doi = {10.1021/ie200706d},
  urldate = {2023-05-26},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\R9YH99GW\Dones 等 - 2011 - Application of Balanced Truncation to Nonlinear Sy.pdf}
}

@article{dongPiecewisePolynomialNonlinear,
  title = {Piecewise {{Polynomial Nonlinear Model Reduction}}},
  author = {Dong, Ning and Roychowdhury, Jaijeet},
  doi = {10.1145/775832.775957},
  abstract = {We present a novel, general approach towards model-order reduction (MOR) of nonlinear systems that combines good global and local approximation properties. The nonlinear system is first approximated as piecewise polynomials over a number of regions, following which each region is reduced via polynomial model-reduction methods. Our approach, dubbed PWP, generalizes recent piecewise linear approaches and ties them with polynomial-based MOR, thereby combining their advantages. In particular, reduced models obtained by our approach reproduce small-signal distortion and intermodulation properties well, while at the same time retaining fidelity in large-swing and large-signal analyses, e.g., transient simulations. Thus our reduced models can be used as drop-in replacements for time-domain as well as frequency-domain simulations, with small or large excitations. By exploiting sparsity in system polynomial coefficients, we are able to make the polynomial reduction procedure linear in the size of the original system. We provide implementation details and illustrate PWP with an example.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\TXIH3G95\Dong 和 Roychowdhury - Piecewise Polynomial Nonlinear Model Reduction.pdf}
}

@misc{dongSurveyIncontextLearning2023,
  title = {A {{Survey}} on {{In-context Learning}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Li, Lei and Sui, Zhifang},
  year = {2023},
  month = jun,
  number = {arXiv:2301.00234},
  eprint = {2301.00234},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-07},
  abstract = {With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\AA\Zotero\storage\W875UBEH\Dong 等 - 2023 - A Survey on In-context Learning.pdf}
}

@article{dosovitskiyImageWorth16x162020,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  shorttitle = {An Image Is Worth 16x16 Words},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.11929},
  eprint = {2010.11929},
  urldate = {2023-12-28},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\FB2E9XDL\Dosovitskiy 等 - 2020 - An image is worth 16x16 words Transformers for im.pdf}
}

@misc{duffNumericalComputationNew2019,
  title = {Numerical Computation and New Output Bounds for Time-Limited Balanced Truncation of Discrete-Time Systems},
  author = {Duff, Igor Pontes and K{\"u}rschner, Patrick},
  year = {2019},
  month = feb,
  number = {arXiv:1902.01652},
  eprint = {1902.01652},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {In this paper, balancing based model order reduction (MOR) for large-scale linear discrete-time time-invariant systems in prescribed finite time intervals is studied. The first main topic is the development of error bounds regarding the approximated output vector within the time limits. The influence of different components in the established bounds will be highlighted. After that, the second part of the article proposes strategies that enable an efficient numerical execution of time-limited balanced truncation for large-scale systems. Numerical experiments illustrate the performance of the proposed techniques.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\7MRHKRRR\Duff 和 Kürschner - 2019 - Numerical computation and new output bounds for ti.pdf}
}

@article{eProposalMachineLearning2017,
  title = {A {{Proposal}} on {{Machine Learning}} via {{Dynamical Systems}}},
  author = {E, Weinan},
  year = {2017},
  month = mar,
  journal = {Communications in Mathematics and Statistics},
  volume = {5},
  number = {1},
  pages = {1--11},
  issn = {2194-6701, 2194-671X},
  doi = {10.1007/s40304-017-0103-z},
  urldate = {2023-05-26},
  abstract = {We discuss the idea of using continuous dynamical systems to model general high-dimensional nonlinear functions used in machine learning. We also discuss the connection with deep learning.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\Y85KL23P\E - 2017 - A Proposal on Machine Learning via Dynamical Syste.pdf}
}

@article{ergejGLOBALLINEARIZATIONNONLINEAR,
  title = {{{GLOBAL LINEARIZATION OF NONLINEAR SYSTEMS}} --- {{A SURVEY}}},
  author = {Ergej, S},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\T6UIUBD3\Ergej - GLOBAL LINEARIZATION OF NONLINEAR SYSTEMS — A SURV.pdf}
}

@misc{fangLargeLanguageModelsLLMs2024,
  title = {Large {{Language Models}}({{LLMs}}) on {{Tabular Data}}: {{Prediction}}, {{Generation}}, and {{Understanding}} -- {{A Survey}}},
  shorttitle = {Large {{Language Models}}({{LLMs}}) on {{Tabular Data}}},
  author = {Fang, Xi and Xu, Weijie and Tan, Fiona Anting and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos},
  year = {2024},
  month = feb,
  number = {arXiv:2402.17944},
  eprint = {2402.17944},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-16},
  abstract = {Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\HXNFGK8K\\Fang et al_2024_Large Language Models(LLMs) on Tabular Data.pdf;C\:\\Users\\AA\\Zotero\\storage\\CTRNYJAA\\2402.html}
}

@misc{fanHierarchicalNeuralStory2018,
  title = {Hierarchical {{Neural Story Generation}}},
  author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  year = {2018},
  month = may,
  number = {arXiv:1805.04833},
  eprint = {1805.04833},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.04833},
  urldate = {2025-01-03},
  abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LK3PDJUY\\Fan 等 - 2018 - Hierarchical Neural Story Generation.pdf;C\:\\Users\\AA\\Zotero\\storage\\AAEMQX5T\\1805.html}
}

@misc{fengTrendsIntegrationKnowledge2023,
  title = {Trends in {{Integration}} of {{Knowledge}} and {{Large Language Models}}: {{A Survey}} and {{Taxonomy}} of {{Methods}}, {{Benchmarks}}, and {{Applications}}},
  shorttitle = {Trends in {{Integration}} of {{Knowledge}} and {{Large Language Models}}},
  author = {Feng, Zhangyin and Ma, Weitao and Yu, Weijiang and Huang, Lei and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and {liu}, Ting},
  year = {2023},
  month = dec,
  number = {arXiv:2311.05876},
  eprint = {2311.05876},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {Large language models (LLMs) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations. In order to address these challenges, researchers have pursued two primary strategies, knowledge editing and retrieval augmentation, to enhance LLMs by incorporating external information from different aspects. Nevertheless, there is still a notable absence of a comprehensive survey. In this paper, we propose a review to discuss the trends in integration of knowledge and large language models, including taxonomy of methods, benchmarks, and applications. In addition, we conduct an in-depth analysis of different methods and point out potential research directions in the future. We hope this survey offers the community quick access and a comprehensive overview of this research area, with the intention of inspiring future research endeavors.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\UMVFHJ7H\\Feng et al_2023_Trends in Integration of Knowledge and Large Language Models.pdf;C\:\\Users\\AA\\Zotero\\storage\\EPDV2FVX\\2311.html}
}

@article{floridiGPT3ItsNature2020,
  title = {{{GPT-3}}: {{Its Nature}}, {{Scope}}, {{Limits}}, and {{Consequences}}},
  shorttitle = {{{GPT-3}}},
  author = {Floridi, Luciano and Chiriatti, Massimo},
  year = {2020},
  month = dec,
  journal = {Minds and Machines},
  volume = {30},
  number = {4},
  pages = {681--694},
  issn = {1572-8641},
  doi = {10.1007/s11023-020-09548-1},
  urldate = {2023-05-08},
  abstract = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\LUR6R95A\Floridi_Chiriatti_2020_GPT-3.pdf}
}

@article{funahashiApproximateRealizationContinuous1989,
  title = {On the Approximate Realization of Continuous Mappings by Neural Networks},
  author = {Funahashi, Ken-Ichi},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {3},
  pages = {183--192},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90003-8},
  urldate = {2023-05-26},
  abstract = {In this paper, we prove that any continuous mapping can be approximately realized by RumelhartHinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\KC4GJZ7Y\Funahashi - 1989 - On the approximate realization of continuous mappi.pdf}
}

@article{funahashiApproximationDynamicalSystems1993,
  title = {Approximation of Dynamical Systems by Continuous Time Recurrent Neural Networks},
  author = {Funahashi, Ken-ichi and Nakamura, Yuichi},
  year = {1993},
  month = jan,
  journal = {Neural Networks},
  volume = {6},
  number = {6},
  pages = {801--806},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80125-X},
  urldate = {2023-04-12},
  abstract = {In this paper, we prove that any finite time trajectory of a given n-dimensional dynamical system can be approximately realized by the internal state of the output units of a continuous time recurrent neural network with n output units, some hidden units, and an appropriate initial condition. The essential idea ofthe proof is to embed the n-dimensional dynamical system into a higher dimensional one which defines a recurrent neural network. As a corollary, we also show that any continuous curve can be approximated by the output o f a recurrent neural network.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\XH84QKWM\Funahashi 和 Nakamura - 1993 - Approximation of dynamical systems by continuous t.pdf}
}

@misc{ganRobustnessTexttoSQLModels2021,
  title = {Towards {{Robustness}} of {{Text-to-SQL Models}} against {{Synonym Substitution}}},
  author = {Gan, Yujian and Chen, Xinyun and Huang, Qiuping and Purver, Matthew and Woodward, John R. and Xie, Jinxia and Huang, Pengsheng},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01065},
  eprint = {2106.01065},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.01065},
  urldate = {2024-12-18},
  abstract = {Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case adversarial attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\UFXIBBPN\\Gan 等 - 2021 - Towards Robustness of Text-to-SQL Models against S.pdf;C\:\\Users\\AA\\Zotero\\storage\\5NEIHT2E\\2106.html}
}

@misc{gaoMakingPretrainedLanguage2021,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  year = {2021},
  month = jun,
  number = {arXiv:2012.15723},
  eprint = {2012.15723},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF--better few-shot fine-tuning of language models--a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\VP6YIFPR\\Gao 等 - 2021 - Making Pre-trained Language Models Better Few-shot.pdf;C\:\\Users\\AA\\Zotero\\storage\\QKJ5BARX\\2012.html}
}

@misc{gaoRetrievalAugmentedGenerationLarge2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  year = {2024},
  month = mar,
  number = {arXiv:2312.10997},
  eprint = {2312.10997},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10997},
  urldate = {2024-12-19},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\TTQEYFKM\\Gao 等 - 2024 - Retrieval-Augmented Generation for Large Language .pdf;C\:\\Users\\AA\\Zotero\\storage\\74GIE9BQ\\2312.html}
}

@article{gatysNeuralAlgorithmArtistic2015,
  title = {A Neural Algorithm of Artistic Style},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  year = {2015},
  journal = {arXiv preprint arXiv:1508.06576},
  eprint = {1508.06576},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\T4XRQ7TU\\Gatys et al_2015_A neural algorithm of artistic style.pdf;C\:\\Users\\AA\\Zotero\\storage\\554P3THL\\1508.html}
}

@misc{GoogleFanYi,
  title = {Google 翻译},
  urldate = {2024-03-26},
  howpublished = {https://translate.google.com/?hl=zh-CN\&sl=auto\&tl=zh-CN\&text=Two\%20important\%20aspects\%20of\%20semantic\%20parsing\%20for\%20question\%20answering\%20are\%20the\%20breadth\%20of\%20the\%20knowledge\%20source\%20and\%20the\%20depth\%20of\%0Alogical\%20compositionality.\%20While\%20existing\%20work\%20trades\%20off\%20one\%20aspect\%20for\%20another\%2C\%20this\%20paper\%20simultaneously\%20makes\%20progress\%20\%0Aon\%20both\%20fronts\%20through\%20a\%20new\%20task\%3A\%20answering\%20complex\%20questions\%20on\%20semi-structured\%20tables\%20using\%20question-answer\%20pairs\%20as\%20\%0Asupervision.\%20The\%20central\%20challenge\%20arises\%20from\%20two\%20compounding\%20factors\%3A\%20the\%20broader\%20domain\%20results\%20in\%20an\%20open-ended\%20set\%20\%0Aof\%20relations\%2C\%20and\%20the\%20deeper\%20compositionality\%20results\%20in\%20a\%20combinatorial\%20explosion\%20in\%20the\%20space\%20of\%20logical\%20forms.\%20We\%20\%0Apropose\%20a\%20logical-form\%20driven\%20parsing\%20algorithm\%20guided\%20by\%20strong\%20typing\%20constraints\%20and\%20show\%20that\%20it\%20obtains\%20significant\%0A\%20improvements\%20over\%20natural\%20baselines.\%20For\%20evaluation\%2C\%20we\%20created\%20a\%20new\%20dataset\%20of\%2022\%2C033\%20complex\%20questions\%20on\%20Wikipedia\%0A\%20\%20tables\%2C\%20which\%20is\%20made\%20publicly\%20available.\%0A\&op=translate},
  file = {C:\Users\AA\Zotero\storage\3SDADZR2\translate.google.com.html}
}

@article{goosLectureNotesComputer,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\C5MYXPRJ\Goos 等 - Lecture Notes in Computer Science.pdf}
}

@misc{habibaNeuralOrdinaryDifferential2020,
  title = {Neural {{Ordinary Differential Equation}} Based {{Recurrent Neural Network Model}}},
  author = {Habiba, Mansura and Pearlmutter, Barak A.},
  year = {2020},
  month = may,
  number = {arXiv:2005.09807},
  eprint = {2005.09807},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-04-12},
  abstract = {Neural differential equations are a promising new member in the neural network family. They show the potential of differential equations for time series data analysis. In this paper, the strength of the ordinary differential equation (ODE) is explored with a new extension. The main goal of this work is to answer the following questions: (i) can ODE be used to redefine the existing neural network model? (ii) can Neural ODEs solve the irregular sampling rate challenge of existing neural network models for a continuous time series, i.e., length and dynamic nature, (iii) how to reduce the training and evaluation time of existing Neural ODE systems? This work leverages the mathematical foundation of ODEs to redesign traditional RNNs such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The main contribution of this paper is to illustrate the design of two new ODE-based RNN models (GRU-ODE model and LSTM-ODE) which can compute the hidden state and cell state at any point of time using an ODE solver. These models reduce the computation overhead of hidden state and cell state by a vast amount. The performance evaluation of these two new models for learning continuous time series with irregular sampling rate is then demonstrated. Experiments show that these new ODE based RNN models require less training time than Latent ODEs and conventional Neural ODEs. They can achieve higher accuracy quickly, and the design of the neural network is simpler than, previous neural ODE systems.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\MRGNWF33\Habiba 和 Pearlmutter - 2020 - Neural Ordinary Differential Equation based Recurr.pdf}
}

@misc{hanComprehensiveSurveyVector2023,
  title = {A {{Comprehensive Survey}} on {{Vector Database}}: {{Storage}} and {{Retrieval Technique}}, {{Challenge}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Vector Database}}},
  author = {Han, Yikun and Liu, Chunjiang and Wang, Pengfei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11703},
  eprint = {2310.11703},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-19},
  abstract = {A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases},
  file = {C\:\\Users\\AA\\Zotero\\storage\\75WW9LBQ\\Han 等 - 2023 - A Comprehensive Survey on Vector Database Storage.pdf;C\:\\Users\\AA\\Zotero\\storage\\C57FJYVY\\2310.html}
}

@misc{hanDeepCompressionCompressing2016,
  title = {Deep {{Compression}}: {{Compressing Deep Neural Networks}} with {{Pruning}}, {{Trained Quantization}} and {{Huffman Coding}}},
  shorttitle = {Deep {{Compression}}},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  year = {2016},
  month = feb,
  number = {arXiv:1510.00149},
  eprint = {1510.00149},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-12},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce ``deep compression'', a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35{\texttimes} to 49{\texttimes} without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9{\texttimes} to 13{\texttimes}; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35{\texttimes}, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49{\texttimes} from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3{\texttimes} to 4{\texttimes} layerwise speedup and 3{\texttimes} to 7{\texttimes} better energy efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\GVJVLVR8\Han 等 - 2016 - Deep Compression Compressing Deep Neural Networks.pdf}
}

@article{hanLearningBothWeights2015,
  title = {Learning Both Weights and Connections for Efficient Neural Network},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  year = {2015},
  journal = {Advances in neural information processing systems},
  volume = {28},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\QB3XNN8Y\\Han et al_2015_Learning both weights and connections for efficient neural network.pdf;C\:\\Users\\AA\\Zotero\\storage\\4WXIYJ2N\\ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html}
}

@article{hanSurveyMultilabelClassification2023,
  title = {A Survey of Multi-Label Classification Based on Supervised and Semi-Supervised Learning},
  author = {Han, Meng and Wu, Hongxin and Chen, Zhiqiang and Li, Muhang and Zhang, Xilong},
  year = {2023},
  month = mar,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {14},
  number = {3},
  pages = {697--724},
  issn = {1868-8071, 1868-808X},
  doi = {10.1007/s13042-022-01658-9},
  urldate = {2023-11-29},
  abstract = {Multi-label classification algorithms based on supervised learning use all the labeled data to train classifiers. However, in real life, many of the data are unlabeled, and it is costly to label all the data needed. Multi-label classification algorithms based on semi-supervised learning can use both labeled and unlabeled data to train classifiers, resulting in better-performing models. In this paper, we first review supervised learning classification algorithms in terms of label non-correlation and label correlation and semi-supervised learning classification algorithms in terms of inductive methods and transductive methods. After that, multi-label classification algorithms are introduced from the application areas of image, text, music and video. Subsequently, evaluation metrics and datasets are briefly introduced. Finally, research directions in complex concept drift, label complex correlation, feature selection and class imbalance are presented.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\FEDPX5AW\Han 等 - 2023 - A survey of multi-label classification based on su.pdf}
}

@misc{heAnnoLLMMakingLarge2023,
  title = {{{AnnoLLM}}: {{Making Large Language Models}} to {{Be Better Crowdsourced Annotators}}},
  shorttitle = {{{AnnoLLM}}},
  author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
  year = {2023},
  month = mar,
  number = {arXiv:2303.16854},
  eprint = {2303.16854},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-29},
  abstract = {Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\N9MRI7HD\\He 等 - 2023 - AnnoLLM Making Large Language Models to Be Better.pdf;C\:\\Users\\AA\\Zotero\\storage\\YGMSKBSX\\2303.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  urldate = {2025-01-03},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\VY6VZAJ6\He 等 - 2016 - Deep residual learning for image recognition.pdf}
}

@misc{heGRetrieverRetrievalAugmentedGeneration2024,
  title = {G-{{Retriever}}: {{Retrieval-Augmented Generation}} for {{Textual Graph Understanding}} and {{Question Answering}}},
  shorttitle = {G-{{Retriever}}},
  author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V. and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
  year = {2024},
  month = may,
  number = {arXiv:2402.07630},
  eprint = {2402.07630},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.07630},
  urldate = {2024-12-21},
  abstract = {Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.{\textasciitilde}{\textbackslash}footnote\{Our codes and datasets are available at: {\textbackslash}url\{https://github.com/XiaoxinHe/G-Retriever\}\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\T5FLXKWY\\He 等 - 2024 - G-Retriever Retrieval-Augmented Generation for Te.pdf;C\:\\Users\\AA\\Zotero\\storage\\R59I3YDT\\2402.html}
}

@misc{heLearningSymmetricCollaborative2017,
  title = {Learning {{Symmetric Collaborative Dialogue Agents}} with {{Dynamic Knowledge Graph Embeddings}}},
  author = {He, He and Balakrishnan, Anusha and Eric, Mihail and Liang, Percy},
  year = {2017},
  month = apr,
  number = {arXiv:1704.07130},
  eprint = {1704.07130},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-21},
  abstract = {We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LJCQCZAG\\He 等 - 2017 - Learning Symmetric Collaborative Dialogue Agents w.pdf;C\:\\Users\\AA\\Zotero\\storage\\4DLRVW63\\1704.html}
}

@misc{hendrycksMeasuringMassiveMultitask2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.03300},
  urldate = {2024-12-21},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\FXS7S9TP\\Hendrycks 等 - 2021 - Measuring Massive Multitask Language Understanding.pdf;C\:\\Users\\AA\\Zotero\\storage\\AKDRELLS\\2009.html}
}

@misc{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02531},
  eprint = {1503.02531},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-09-17},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\MIK23WZM\Hinton 等 - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {MIT press},
  doi = {10.1162/neco.1997.9.8.1735},
  file = {C\:\\Users\\AA\\Zotero\\storage\\2J76S7YC\\Hochreiter_Schmidhuber_1997_Long short-term memory.pdf;C\:\\Users\\AA\\Zotero\\storage\\DWDVUZTW\\6795963.html}
}

@misc{holtzmanCuriousCaseNeural2020,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09751},
  eprint = {1904.09751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.09751},
  urldate = {2025-01-03},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\HHCFCL4M\\Holtzman 等 - 2020 - The Curious Case of Neural Text Degeneration.pdf;C\:\\Users\\AA\\Zotero\\storage\\ZKI5S524\\1904.html}
}

@article{hongCogAgentVisualLanguage,
  title = {{{CogAgent}}: {{A Visual Language Model}} for {{GUI Agents}}},
  author = {Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and Tang, Jie},
  abstract = {People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120{$\RightArrowBar$}1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks---Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM .},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\3L8IAETZ\Hong 等 - CogAgent A Visual Language Model for GUI Agents.pdf}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2023-05-26},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\YLVQIYFS\Hornik 等 - 1989 - Multilayer feedforward networks are universal appr.pdf}
}

@inproceedings{huangLearningDeepStructured2013,
  title = {Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Conference}} on Information \& Knowledge Management - {{CIKM}} '13},
  author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
  year = {2013},
  pages = {2333--2338},
  publisher = {ACM Press},
  address = {San Francisco, California, USA},
  doi = {10.1145/2505515.2505665},
  urldate = {2024-09-02},
  isbn = {978-1-4503-2263-8},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\7I3W9WLP\\Huang 等 - 2013 - Learning deep structured semantic models for web s.pdf;C\:\\Users\\AA\\Zotero\\storage\\MZKJBQ4I\\huang2013.pdf.pdf}
}

@misc{huangPartialDifferentialEquations2022,
  title = {Partial {{Differential Equations Meet Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Partial {{Differential Equations Meet Deep Neural Networks}}},
  author = {Huang, Shudong and Feng, Wentao and Tang, Chenwei and Lv, Jiancheng},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05567},
  eprint = {2211.05567},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.05567},
  urldate = {2025-01-07},
  abstract = {Many problems in science and engineering can be represented by a set of partial differential equations (PDEs) through mathematical modeling. Mechanism-based computation following PDEs has long been an essential paradigm for studying topics such as computational fluid dynamics, multiphysics simulation, molecular dynamics, or even dynamical systems. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. At the same time, solving PDEs efficiently has been a long-standing challenge. Generally, except for a few differential equations for which analytical solutions are directly available, many more equations must rely on numerical approaches such as the finite difference method, finite element method, finite volume method, and boundary element method to be solved approximately. These numerical methods usually divide a continuous problem domain into discrete points and then concentrate on solving the system at each of those points. Though the effectiveness of these traditional numerical methods, the vast number of iterative operations accompanying each step forward significantly reduces the efficiency. Recently, another equally important paradigm, data-based computation represented by deep learning, has emerged as an effective means of solving PDEs. Surprisingly, a comprehensive review for this interesting subfield is still lacking. This survey aims to categorize and review the current progress on Deep Neural Networks (DNNs) for PDEs. We discuss the literature published in this subfield over the past decades and present them in a common taxonomy, followed by an overview and classification of applications of these related methods in scientific research and engineering scenarios. The origin, developing history, character, sort, as well as the future trends in each potential direction of this subfield are also introduced.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\32W4F5I4\\Huang 等 - 2022 - Partial Differential Equations Meet Deep Neural Ne.pdf;C\:\\Users\\AA\\Zotero\\storage\\H7TSQ57S\\2211.html}
}

@misc{huKnowledgeDistillationZoo2023,
  title = {Knowledge-{{Distillation-Zoo}}},
  author = {Hu, Aber},
  year = {2023},
  month = sep,
  urldate = {2023-09-17},
  abstract = {Pytorch implementation of various Knowledge Distillation (KD) methods.}
}

@article{huntGlobalTransformationsNonlinear1983,
  title = {Global Transformations of Nonlinear Systems},
  author = {Hunt, L. and {Renjeng Su} and Meyer, G.},
  year = {1983},
  month = jan,
  journal = {IEEE Transactions on Automatic Control},
  volume = {28},
  number = {1},
  pages = {24--31},
  issn = {0018-9286},
  doi = {10.1109/TAC.1983.1103137},
  urldate = {2023-05-26},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\W32TPNCM\Hunt 等 - 1983 - Global transformations of nonlinear systems.pdf}
}

@article{izacardFewshotLearningRetrieval2022,
  title = {Few-Shot Learning with Retrieval Augmented Language Models},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and {Dwivedi-Yu}, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  year = {2022},
  journal = {arXiv preprint arXiv:2208.03299},
  volume = {1},
  number = {2},
  eprint = {2208.03299},
  pages = {4},
  urldate = {2024-12-21},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\9EYL6FTB\Izacard 等 - 2022 - Few-shot learning with retrieval augmented languag.pdf}
}

@inproceedings{jacobQuantizationTrainingNeural2018,
  title = {Quantization and {{Training}} of {{Neural Networks}} for {{Efficient Integer-Arithmetic-Only Inference}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  year = {2018},
  pages = {2704--2713},
  urldate = {2023-04-26},
  file = {C:\Users\AA\Zotero\storage\DKNS5XEL\Jacob et al_2018_Quantization and Training of Neural Networks for Efficient.pdf}
}

@article{janardhananModelOrderReduction,
  title = {Model {{Order Reduction}} and {{Controller Design Techniques}}},
  author = {Janardhanan, Dr S},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\K7QW78CK\Janardhanan - Model Order Reduction and Controller Design Techni.pdf}
}

@article{jiangHowCanWe2020,
  title = {How Can We Know What Language Models Know?},
  author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
  year = {2020},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {423--438},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}},
  doi = {10.1162/tacl_a_00324},
  urldate = {2023-12-26},
  file = {C:\Users\AA\Zotero\storage\MX4M3E8F\96460.html}
}

@misc{jiangStructGPTGeneralFramework2023,
  title = {{{StructGPT}}: {{A General Framework}} for {{Large Language Model}} to {{Reason}} over {{Structured Data}}},
  shorttitle = {{{StructGPT}}},
  author = {Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2023},
  month = oct,
  number = {arXiv:2305.09645},
  eprint = {2305.09645},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-16},
  abstract = {In this paper, we study how to improve the zero-shot reasoning ability of large language models{\textasciitilde}(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an {\textbackslash}emph\{Iterative Reading-then-Reasoning{\textasciitilde}(IRR)\} approach for solving question answering tasks based on structured data, called {\textbackslash}textbf\{StructGPT\}. In our approach, we construct the specialized function to collect relevant evidence from structured data ({\textbackslash}ie {\textbackslash}emph\{reading\}), and let LLMs concentrate the reasoning task based on the collected information ({\textbackslash}ie {\textbackslash}emph\{reasoning\}). Specially, we propose an {\textbackslash}emph\{invoking-linearization-generation\} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/RUCAIBox/StructGPT\}.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\FTKX6NE2\\Jiang et al_2023_StructGPT.pdf;C\:\\Users\\AA\\Zotero\\storage\\FEJPL6KI\\2305.html}
}

@misc{jiBeaverTailsImprovedSafety2023,
  title = {{{BeaverTails}}: {{Towards Improved Safety Alignment}} of {{LLM}} via a {{Human-Preference Dataset}}},
  shorttitle = {{{BeaverTails}}},
  author = {Ji, Jiaming and Liu, Mickel and Dai, Juntao and Pan, Xuehai and Zhang, Chi and Bian, Ce and Zhang, Chi and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  year = {2023},
  month = nov,
  number = {arXiv:2307.04657},
  eprint = {2307.04657},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-29},
  abstract = {In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\R3RM8NB8\\Ji 等 - 2023 - BeaverTails Towards Improved Safety Alignment of .pdf;C\:\\Users\\AA\\Zotero\\storage\\L7XS2MP3\\2307.html}
}

@inproceedings{kandpalLargeLanguageModels2023,
  title = {Large Language Models Struggle to Learn Long-Tail Knowledge},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  year = {2023},
  pages = {15696--15707},
  publisher = {PMLR},
  urldate = {2024-12-21},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\Z6LQ4S4U\Kandpal 等 - 2023 - Large language models struggle to learn long-tail .pdf}
}

@misc{karpukhinDensePassageRetrieval2020,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  author = {Karpukhin, Vladimir and O{\u g}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  year = {2020},
  month = sep,
  number = {arXiv:2004.04906},
  eprint = {2004.04906},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-21},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\N25H9GQE\\Karpukhin 等 - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf;C\:\\Users\\AA\\Zotero\\storage\\FBKF64BC\\2004.html}
}

@misc{karpukhinDensePassageRetrieval2020a,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  author = {Karpukhin, Vladimir and O{\u g}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  year = {2020},
  month = sep,
  number = {arXiv:2004.04906},
  eprint = {2004.04906},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-02},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\AFXZNM7R\\Karpukhin 等 - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf;C\:\\Users\\AA\\Zotero\\storage\\T8YBUL92\\2004.html}
}

@misc{karpukhinDensePassageRetrieval2020b,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  author = {Karpukhin, Vladimir and O{\u g}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  year = {2020},
  month = sep,
  number = {arXiv:2004.04906},
  eprint = {2004.04906},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.04906},
  urldate = {2024-12-21},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\9UDB2P85\\Karpukhin 等 - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf;C\:\\Users\\AA\\Zotero\\storage\\2WCT2EHY\\2004.html}
}

@misc{keysanCanYouText2023,
  title = {Can You Text What Is Happening? {{Integrating}} Pre-Trained Language Encoders into Trajectory Prediction Models for Autonomous Driving},
  shorttitle = {Can You Text What Is Happening?},
  author = {Keysan, Ali and Look, Andreas and Kosman, Eitan and G{\"u}rsun, Gonca and Wagner, J{\"o}rg and Yao, Yu and Rakitsch, Barbara},
  year = {2023},
  month = sep,
  number = {arXiv:2309.05282},
  eprint = {2309.05282},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-17},
  abstract = {In autonomous driving tasks, scene understanding is the first step towards predicting the future behavior of the surrounding traffic participants. Yet, how to represent a given scene and extract its features are still open research questions. In this study, we propose a novel text-based representation of traffic scenes and process it with a pre-trained language encoder. First, we show that text-based representations, combined with classical rasterized image representations, lead to descriptive scene embeddings. Second, we benchmark our predictions on the nuScenes dataset and show significant improvements compared to baselines. Third, we show in an ablation study that a joint encoder of text and rasterized images outperforms the individual encoders confirming that both representations have their complementary strengths.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\59JGVP4K\\Keysan 等 - 2023 - Can you text what is happening Integrating pre-tr.pdf;C\:\\Users\\AA\\Zotero\\storage\\4NL2UIS4\\2309.html}
}

@misc{kongOpenTabAdvancingLarge2024,
  title = {{{OpenTab}}: {{Advancing Large Language Models}} as {{Open-domain Table Reasoners}}},
  shorttitle = {{{OpenTab}}},
  author = {Kong, Kezhi and Zhang, Jiani and Shen, Zhengyuan and Srinivasan, Balasubramaniam and Lei, Chuan and Faloutsos, Christos and Rangwala, Huzefa and Karypis, George},
  year = {2024},
  month = apr,
  number = {arXiv:2402.14361},
  eprint = {2402.14361},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-16},
  abstract = {Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5\% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\HUGM5K5E\\Kong et al_2024_OpenTab.pdf;C\:\\Users\\AA\\Zotero\\storage\\XMA3LQUY\\2402.html}
}

@article{kramerStabilityDomainsQuadraticBilinear2021,
  title = {Stability {{Domains}} for {{Quadratic-Bilinear Reduced-Order Models}}},
  author = {Kramer, Boris},
  year = {2021},
  month = jan,
  journal = {SIAM Journal on Applied Dynamical Systems},
  volume = {20},
  number = {2},
  pages = {981--996},
  issn = {1536-0040},
  doi = {10.1137/20M1364849},
  urldate = {2023-05-26},
  abstract = {We propose a computational approach to estimate the stability domain of quadratic-bilinear reducedorder models (ROMs), which are low-dimensional approximations of large-scale dynamical systems. For nonlinear ROMs, it is not only important to show that the origin is locally asymptotically stable, but also to quantify if the operative range of the ROM is included in the region of convergence. While accuracy and structure preservation remain the main focus of development for nonlinear ROMs, computational methods that go beyond the existing highly conservative analytical results have been lacking thus far. In this work, for a given quadratic Lyapunov function, we first derive an analytical estimate of the stability domain, which is rather conservative but can be evaluated efficiently. With the goal to enlarge this estimate, we provide an optimal ellipsoidal estimate of the stability domain by solving a convex optimization problem. This provides us with valuable information about stability properties of the ROM, an important aspect of predictive simulation. We do not assume a specific ROM method, so a particular appeal is that the approach is applicable to quadratic-bilinear models obtained via data-driven approaches, where ROM stability properties cannot---per definition---be derived from the full-order model. Numerical results for a LQG-balanced ROM of Burgers' equation, a proper orthogonal decomposition ROM of FitzHugh-Nagumo, and a non-intrusive ROM of Burgers' equation demonstrate the scalability and quantitative advantages of the proposed approach. The optimization-based estimates of the stability domain are found to be up to four orders of magnitude less conservative than analytical estimates.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\66BRE9NG\Kramer - 2021 - Stability Domains for Quadratic-Bilinear Reduced-O.pdf}
}

@misc{kuzmanChatGPTBeginningEnd2023,
  title = {{{ChatGPT}}: {{Beginning}} of an {{End}} of {{Manual Linguistic Data Annotation}}? {{Use Case}} of {{Automatic Genre Identification}}},
  shorttitle = {{{ChatGPT}}},
  author = {Kuzman, Taja and Mozeti{\v c}, Igor and Ljube{\v s}i{\'c}, Nikola},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03953},
  eprint = {2303.03953},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-29},
  abstract = {ChatGPT has shown strong capabilities in natural language generation tasks, which naturally leads researchers to explore where its abilities end. In this paper, we examine whether ChatGPT can be used for zero-shot text classification, more specifically, automatic genre identification. We compare ChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on datasets, manually annotated with genres. The models are compared on test sets in two languages: English and Slovenian. Results show that ChatGPT outperforms the fine-tuned model when applied to the dataset which was not seen before by either of the models. Even when applied on Slovenian language as an underresourced language, ChatGPT's performance is no worse than when applied to English. However, if the model is fully prompted in Slovenian, the performance drops significantly, showing the current limitations of ChatGPT usage on smaller languages. The presented results lead us to questioning whether this is the beginning of an end of laborious manual annotation campaigns even for smaller languages, such as Slovenian.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\AA\Zotero\storage\248T7SFL\Kuzman 等 - 2023 - ChatGPT Beginning of an End of Manual Linguistic .pdf}
}

@misc{kweonOpenWikiTableDatasetOpen2023,
  title = {Open-{{WikiTable}}: {{Dataset}} for {{Open Domain Question Answering}} with {{Complex Reasoning}} over {{Table}}},
  shorttitle = {Open-{{WikiTable}}},
  author = {Kweon, Sunjun and Kwon, Yeonsu and Cho, Seonhee and Jo, Yohan and Choi, Edward},
  year = {2023},
  month = may,
  number = {arXiv:2305.07288},
  eprint = {2305.07288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07288},
  urldate = {2024-12-18},
  abstract = {Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate exploring over multiple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex reasoning over tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be applicable in the open-domain setting. As each question is coupled with both textual answers and SQL queries, Open-WikiTable opens up a wide range of possibilities for future research, as both reader and parser methods can be applied. The dataset and code are publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\2TDRVCW5\\Kweon 等 - 2023 - Open-WikiTable Dataset for Open Domain Question A.pdf;C\:\\Users\\AA\\Zotero\\storage\\LSCWVRQX\\2305.html}
}

@inproceedings{laubComputationSystemBalancing1986,
  title = {Computation of System Balancing Transformations},
  booktitle = {1986 25th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Laub, A. and Heath, M. and Paige, C. and Ward, R.},
  year = {1986},
  month = dec,
  pages = {548--553},
  publisher = {IEEE},
  address = {Athens, Greece},
  doi = {10.1109/CDC.1986.267343},
  urldate = {2023-05-26},
  abstract = {An algorithm is presented in this paper for computingstatespace balancing transformations directly from a state-space realization. The algorithm requires no ``squaring up'' or unnecessary matrix products. Various algorithmic aspects are discussed in detail. A key feature of the algorithm is the determination ofa contragredient transformation through computing the singular value decomposition of acertain product of matrices without explicitly forming the product. Other contragredient transformation applications are also described. It is further shown that a similar approach may be taken. involving the generalized singular value decomposition,to the classical simultaneous diagonalization problem. These SVD-based simultaneousdiagonalization algorithms provide a computational alternative to existing methods for solving certain classes of symmetric positive definite generalized eigenvalue problems.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\GR5TPYTB\Laub 等 - 1986 - Computation of system balancing transformations.pdf}
}

@misc{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  author = {Lester, Brian and {Al-Rfou}, Rami and Constant, Noah},
  year = {2021},
  month = sep,
  number = {arXiv:2104.08691},
  eprint = {2104.08691},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\R3ESL9YD\\Lester 等 - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf;C\:\\Users\\AA\\Zotero\\storage\\A3NG8TEE\\2104.html}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  number = {arXiv:1910.13461},
  eprint = {1910.13461},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\5F9E33GU\\Lewis 等 - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf;C\:\\Users\\AA\\Zotero\\storage\\54GKSC8W\\1910.html}
}

@article{lewisRetrievalaugmentedGenerationKnowledgeintensive2020,
  title = {Retrieval-Augmented Generation for Knowledge-Intensive Nlp Tasks},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {9459--9474},
  urldate = {2024-12-21},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\HTPDJ66C\Lewis 等 - 2020 - Retrieval-augmented generation for knowledge-inten.pdf}
}

@misc{liangTaskMatrixAICompletingTasks2023,
  title = {{{TaskMatrix}}.{{AI}}: {{Completing Tasks}} by {{Connecting Foundation Models}} with {{Millions}} of {{APIs}}},
  shorttitle = {{{TaskMatrix}}.{{AI}}},
  author = {Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and Wang, Yun and Shou, Linjun and Gong, Ming and Duan, Nan},
  year = {2023},
  month = mar,
  number = {arXiv:2303.16434},
  eprint = {2303.16434},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation models to propose task solution outlines and then automatically match some of the sub-tasks in the outlines to the off-the-shelf models and systems with special functionalities to complete them. Inspired by this, we introduce TaskMatrix.AI as a new AI ecosystem that connects foundation models with millions of APIs for task completion. Unlike most previous work that aimed to improve a single AI model, TaskMatrix.AI focuses more on using existing foundation models (as a brain-like central system) and APIs of other AI models and systems (as sub-task solvers) to achieve diversified tasks in both digital and physical domains. As a position paper, we will present our vision of how to build such an ecosystem, explain each key component, and use study cases to illustrate both the feasibility of this vision and the main challenges we need to address next.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\SMAWI3QY\\Liang 等 - 2023 - TaskMatrix.AI Completing Tasks by Connecting Foun.pdf;C\:\\Users\\AA\\Zotero\\storage\\KC2UWQLN\\2303.html}
}

@misc{liDeepLearningDynamical2020,
  title = {Deep {{Learning}} via {{Dynamical Systems}}: {{An Approximation Perspective}}},
  shorttitle = {Deep {{Learning}} via {{Dynamical Systems}}},
  author = {Li, Qianxiao and Lin, Ting and Shen, Zuowei},
  year = {2020},
  month = jun,
  number = {arXiv:1912.10382},
  eprint = {1912.10382},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2023-04-12},
  abstract = {We build on the dynamical systems approach to deep learning, where deep residual networks are idealized as continuous-time dynamical systems, from the approximation perspective. In particular, we establish general sufficient conditions for universal approximation using continuous-time deep residual networks, which can also be understood as approximation theories in Lp using flow maps of dynamical systems. In specific cases, rates of approximation in terms of the time horizon are also established. Overall, these results reveal that composition function approximation through flow maps present a new paradigm in approximation theory and contributes to building a useful mathematical framework to investigate deep learning.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\GBV46G7I\Li 等 - 2020 - Deep Learning via Dynamical Systems An Approximat.pdf}
}

@misc{liGeneralTextEmbeddings2023,
  title = {Towards {{General Text Embeddings}} with {{Multi-stage Contrastive Learning}}},
  author = {Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03281},
  eprint = {2308.03281},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-02},
  abstract = {We present GTE, a general-purpose text embedding model trained with multi-stage contrastive learning. In line with recent advancements in unifying various NLP tasks into a single format, we train a unified text embedding model by employing contrastive learning over a diverse mixture of datasets from multiple sources. By significantly increasing the number of training data during both unsupervised pre-training and supervised fine-tuning stages, we achieve substantial performance gains over existing embedding models. Notably, even with a relatively modest parameter count of 110M, GTE\$\_{\textbackslash}text\{base\}\$ outperforms the black-box embedding API provided by OpenAI and even surpasses 10x larger text embedding models on the massive text embedding benchmark. Furthermore, without additional fine-tuning on each programming language individually, our model outperforms previous best code retrievers of similar size by treating code as text. In summary, our model achieves impressive results by effectively harnessing multi-stage contrastive learning, offering a powerful and efficient text embedding model with broad applicability across various NLP and code-related tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\AA\Zotero\storage\J6JMMJVR\Li 等 - 2023 - Towards General Text Embeddings with Multi-stage C.pdf}
}

@misc{linCommonGenConstrainedText2020,
  title = {{{CommonGen}}: {{A Constrained Text Generation Challenge}} for {{Generative Commonsense Reasoning}}},
  shorttitle = {{{CommonGen}}},
  author = {Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
  year = {2020},
  month = nov,
  number = {arXiv:1911.03705},
  eprint = {1911.03705},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., \{dog, frisbee, catch, throw\}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., "a man throws a frisbee and his dog catches it"). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance. Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA by generating additional context.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\AA\\Zotero\\storage\\2TMTJMBN\\Lin 等 - 2020 - CommonGen A Constrained Text Generation Challenge.pdf;C\:\\Users\\AA\\Zotero\\storage\\EQ5PA75E\\1911.html}
}

@inproceedings{lingDeepsecUniformPlatform2019,
  title = {Deepsec: {{A}} Uniform Platform for Security Analysis of Deep Learning Model},
  shorttitle = {Deepsec},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Ling, Xiang and Ji, Shouling and Zou, Jiaxu and Wang, Jiannan and Wu, Chunming and Li, Bo and Wang, Ting},
  year = {2019},
  pages = {673--690},
  publisher = {IEEE},
  urldate = {2025-02-19},
  file = {C:\Users\AA\Zotero\storage\EI5H9K7Y\Ling 等 - 2019 - Deepsec A uniform platform for security analysis .pdf}
}

@article{lingrasFuzzyroughRoughfuzzySerial2001,
  title = {Fuzzy-Rough and Rough-Fuzzy Serial Combinations in Neurocomputing},
  author = {Lingras, Pawan},
  year = {2001},
  month = feb,
  journal = {Neurocomputing},
  series = {Rough-Neuro Computing},
  volume = {36},
  number = {1},
  pages = {29--44},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(00)00334-9},
  urldate = {2023-05-15},
  abstract = {Rough and neo-fuzzy neurons are two different ways of introducing semantic structures in a neural network. Both have been shown to be useful in practical applications. A rough neuron consists of an upper and a lower neuron. Rough neurons can be used to effectively represent an interval or a set of values. The rough neural networks provide more flexible architectures than exclusive interval-based neural networks. Neofuzzy neurons are used to elaborate on a value by partitioning the crisp value into fuzzy segments for processing by a neural network. Previous work has shown that fuzzy values can be used to describe the difference in output of a rough neuron. This paper provides a more comprehensive introduction to serial combinations of rough and neofuzzy neurons in neurocomputing. The neofuzzy neurons are used to augment output of a rough neuron. On the other hand, rough neurons are shown to be useful for extending the expressive powers of a neofuzzy neuron. The first type of serial combination is termed a rough-fuzzy subnet. The latter serial combination is called a fuzzy-rough subnet. This paper describes the architectures of fuzzy-rough and rough-fuzzy subnets. A discussion on potential applications of the subnets is also provided along with examples.},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\263RTHW6\\Lingras_2001_Fuzzy-rough and rough-fuzzy serial combinations in neurocomputing.pdf;C\:\\Users\\AA\\Zotero\\storage\\AQHPSJQ4\\S0925231200003349.html}
}

@misc{liODETransformerOrdinary2021,
  title = {{{ODE Transformer}}: {{An Ordinary Differential Equation-Inspired Model}} for {{Neural Machine Translation}}},
  shorttitle = {{{ODE Transformer}}},
  author = {Li, Bei and Du, Quan and Zhou, Tao and Zhou, Shuhan and Zeng, Xin and Xiao, Tong and Zhu, Jingbo},
  year = {2021},
  month = apr,
  number = {arXiv:2104.02308},
  eprint = {2104.02308},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.02308},
  urldate = {2023-06-24},
  abstract = {It has been found that residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODEs). In this paper, we explore a deeper relationship between Transformer and numerical methods of ODEs. We show that a residual block of layers in Transformer can be described as a higher-order solution to ODEs. This leads us to design a new architecture (call it ODE Transformer) analogous to the Runge-Kutta method that is well motivated in ODEs. As a natural extension to Transformer, ODE Transformer is easy to implement and parameter efficient. Our experiments on three WMT tasks demonstrate the genericity of this model, and large improvements in performance over several strong baselines. It achieves 30.76 and 44.11 BLEU scores on the WMT'14 En-De and En-Fr test data. This sets a new state-of-the-art on the WMT'14 En-Fr task.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\T24CU7XJ\\Li et al_2021_ODE Transformer.pdf;C\:\\Users\\AA\\Zotero\\storage\\QY9ICQQY\\2104.html}
}

@misc{liPrefixTuningOptimizingContinuous2021,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  author = {Li, Xiang Lisa and Liang, Percy},
  year = {2021},
  month = jan,
  number = {arXiv:2101.00190},
  eprint = {2101.00190},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1{\textbackslash}\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\7AKG2RE9\\Li 和 Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf;C\:\\Users\\AA\\Zotero\\storage\\83D6VY9N\\2101.html}
}

@article{liSheetCopilotBringingSoftware2024,
  title = {{{SheetCopilot}}: {{Bringing Software Productivity}} to the {{Next Level}} through {{Large Language Models}}},
  shorttitle = {{{SheetCopilot}}},
  author = {Li, Hongxin and Su, Jingran and Chen, Yuntao and Li, Qing and ZHANG, ZHAO-XIANG},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  urldate = {2024-04-01},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\P6LBLJBD\Li et al_2024_SheetCopilot.pdf}
}

@inproceedings{liTrainBigThen2020,
  title = {Train {{Big}}, {{Then Compress}}: {{Rethinking Model Size}} for {{Efficient Training}} and {{Inference}} of {{Transformers}}},
  shorttitle = {Train {{Big}}, {{Then Compress}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey},
  year = {2020},
  month = nov,
  pages = {5958--5968},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-06-16},
  abstract = {Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\DF5XNBM9\\Li 等 - 2020 - Train Big, Then Compress Rethinking Model Size fo.pdf;C\:\\Users\\AA\\Zotero\\storage\\YXSBFXCI\\Li et al_2020_Train Big, Then Compress.pdf}
}

@article{liuASFSNovelStreaming2023,
  title = {{{ASFS}}: {{A}} Novel Streaming Feature Selection for Multi-Label Data Based on Neighborhood Rough Set},
  shorttitle = {{{ASFS}}},
  author = {Liu, Jinghua and Lin, Yaojin and Du, Jixiang and Zhang, Hongbo and Chen, Ziyi and Zhang, Jia},
  year = {2023},
  month = jan,
  journal = {Applied Intelligence},
  volume = {53},
  number = {2},
  pages = {1707--1724},
  issn = {1573-7497},
  doi = {10.1007/s10489-022-03366-x},
  urldate = {2023-11-29},
  abstract = {Neighborhood rough set based online streaming feature selection methods have aroused wide concern in recent years and played a vital role in processing high-dimensional data. However, most of the existing methods are directly applied to handle single-label data, or to handle multi-label data by converting multi-label data into a combination of multiple single-label datasets, which ignores that the label set of multi-label data is an integral whole. In this paper, we propose a novel online streaming feature selection for multi-label learning via the neighborhoorough set model, in which feature significance, feature redundancy, and label space integrity are taken into account, simultaneously. To be specific, we first define a new adaptive neighborhood relation to avoid the setting of neighborhood parameter and restructure the neighborhood rough set model to be suitable for processing multi-label data directly. Based on this model, we introduce a evaluation criterion to select features that are important relative to label set and the currently selected features, and present an optimization objective function to update the selected feature subset and filter out redundant features. Comparative experiments on different types of data sets explicitly verify the advantages of the proposed method.},
  langid = {english},
  keywords = {Adaptive neighborhood,Multi-label feature selection,Neighborhood rough set,Streaming features},
  file = {C:\Users\AA\Zotero\storage\4CPZIAD7\Liu 等 - 2023 - ASFS A novel streaming feature selection for mult.pdf}
}

@misc{liuDARTSDifferentiableArchitecture2019,
  title = {{{DARTS}}: {{Differentiable Architecture Search}}},
  shorttitle = {{{DARTS}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  year = {2019},
  month = apr,
  number = {arXiv:1806.09055},
  eprint = {1806.09055},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.09055},
  urldate = {2023-07-08},
  abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\IIQNXLZ5\\Liu et al_2019_DARTS.pdf;C\:\\Users\\AA\\Zotero\\storage\\5WTF6VX2\\1806.html}
}

@article{liuFilterPruningQuantifying2023,
  title = {Filter Pruning by Quantifying Feature Similarity and Entropy of Feature Maps},
  author = {Liu, Yajun and Fan, Kefeng and Wu, Dakui and Zhou, Wenju},
  year = {2023},
  month = aug,
  journal = {Neurocomputing},
  volume = {544},
  pages = {126297},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.126297},
  urldate = {2023-05-15},
  abstract = {Filter pruning can effectively reduce the time cost and computing resources of convolutional neural networks (CNNs), and is well applied to lightweight edge devices. However, most of the current pruning methods focus on the inherent properties of the filters themselves to prune the network, and pay less attention to the connection between the filters and the feature maps. Feature similarity (FSIM) utilizes the fact that the human visual system is more sensitive to the underlying features of the images to more accurately assess image quality. We discover that FSIM is also suitable for evaluating feature maps of CNNs. In addition, the information richness in the feature maps reflects the degree of importance of the filters. Based on the above research, we propose to quantify the importance of feature maps with FSIM and two-dimensional entropy (2D Entropy) indicator to further guide filter pruning (FSIM-E). The FSIM-E is executed on CIFAR-10 and ILSVRC-2012 to demonstrate that FSIM-E can effectively compress and accelerate the network model. For example, for ResNet-110 on CIFAR-10, FSIM-E prunes 71.1\% of the FLOPs and 66.5\% of the parameters, while improving the accuracy by 0.1\%. With ResNet-50, FSIM-E can achieve 57.2\% pruning rate of FLOPs and 53.1\% pruning rate of parameters on ILSVRC-2012 with loss of only 0.42\% of Top-5 accuracy.},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\RL7LY9JK\\Liu et al_2023_Filter pruning by quantifying feature similarity and entropy of feature maps.pdf;C\:\\Users\\AA\\Zotero\\storage\\I5HIVPRV\\S0925231223004204.html}
}

@article{liuGPTUnderstandsToo2023,
  title = {{{GPT}} Understands, Too},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  year = {2023},
  month = aug,
  journal = {AI Open},
  pages = {S2666651023000141},
  issn = {26666510},
  doi = {10.1016/j.aiopen.2023.08.012},
  urldate = {2024-02-03},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\BSPJ6HML\Liu 等 - 2023 - GPT understands, too.pdf}
}

@article{liuGPTUnderstandsToo2023a,
  title = {{{GPT}} Understands, Too},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  year = {2023},
  journal = {AI Open},
  publisher = {Elsevier},
  doi = {10.1016/j.aiopen.2023.08.012},
  urldate = {2024-02-03}
}

@article{liuGPTUnderstandsToo2023b,
  title = {{{GPT}} Understands, Too},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  year = {2023},
  month = aug,
  journal = {AI Open},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2023.08.012},
  urldate = {2024-02-03},
  abstract = {Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance---e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.},
  file = {C:\Users\AA\Zotero\storage\ASXXRD3W\S2666651023000141.html}
}

@inproceedings{liuLearningEfficientConvolutional2017,
  title = {Learning {{Efficient Convolutional Networks}} through {{Network Slimming}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  year = {2017},
  month = oct,
  pages = {2755--2763},
  publisher = {IEEE},
  address = {Venice},
  doi = {10.1109/ICCV.2017.298},
  urldate = {2023-04-12},
  abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20{\texttimes} reduction in model size and a 5{\texttimes} reduction in computing operations.},
  isbn = {978-1-5386-1032-9},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\2DZ2Y7LR\Liu 等 - 2017 - Learning Efficient Convolutional Networks through .pdf}
}

@article{liuPretrainPromptPredict2023,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2023},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {9},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3560815},
  urldate = {2023-12-08},
  abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub ``prompt-based learning.'' Unlike traditional supervised learning, which trains a model to take in an input                                x                              and predict an output                                y                              as               P               (                                y{\textbar}x                              ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input                                x                              is modified using a               template               into a textual string               prompt                                x{$\prime$}                              that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string                                {\^x}                              , from which the final output                                y                              can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be               pre-trained               on massive amounts of raw text, and by defining a new prompting function the model is able to perform               few-shot               or even               zero-shot               learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,~the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website               NLPedia--Pretrain               including constantly updated survey and paperlist.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\EVPXCZ8E\Liu 等 - 2023 - Pre-train, Prompt, and Predict A Systematic Surve.pdf}
}

@misc{liuPTuningV2Prompt2022,
  title = {P-{{Tuning}} v2: {{Prompt Tuning Can Be Comparable}} to {{Fine-tuning Universally Across Scales}} and {{Tasks}}},
  shorttitle = {P-{{Tuning}} V2},
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  year = {2022},
  month = mar,
  number = {arXiv:2110.07602},
  eprint = {2110.07602},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning {\textbackslash}cite\{li2021prefix,qin2021learning\} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\KQBHHVLQ\\Liu 等 - 2022 - P-Tuning v2 Prompt Tuning Can Be Comparable to Fi.pdf;C\:\\Users\\AA\\Zotero\\storage\\RT65XHSR\\2110.html}
}

@article{liuRethinkingValueNetwork2018,
  title = {Rethinking the Value of Network Pruning},
  author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.05270},
  eprint = {1810.05270},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\D987KMTI\\Liu et al_2018_Rethinking the value of network pruning.pdf;C\:\\Users\\AA\\Zotero\\storage\\6B3DV37J\\1810.html}
}

@misc{liuUnsupervisedMultiviewPedestrian2023,
  title = {Unsupervised {{Multi-view Pedestrian Detection}}},
  author = {Liu, Mengyin and Zhu, Chao and Ren, Shiqi and Yin, Xu-Cheng},
  year = {2023},
  month = may,
  number = {arXiv:2305.12457},
  eprint = {2305.12457},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-20},
  abstract = {With the prosperity of the video surveillance, multiple visual sensors have been applied for an accurate localization of pedestrians in a specific area, which facilitate various applications like intelligent safety or new retailing. However, previous methods rely on the supervision from the human annotated pedestrian positions in every video frame and camera view, which is a heavy burden in addition to the necessary camera calibration and synchronization. Therefore, we propose in this paper an Unsupervised Multi-view Pedestrian Detection approach (UMPD) to eliminate the need of annotations to learn a multi-view pedestrian detector. 1) Firstly, Semantic-aware Iterative Segmentation (SIS) is proposed to extract discriminative visual representations of the input images from different camera views via an unsupervised pretrained model, then convert them into 2D segments of pedestrians, based on our proposed iterative Principal Component Analysis and the zero-shot semantic classes from the vision-language pretrained models. 2) Secondly, we propose Vertical-aware Differential Rendering (VDR) to not only learn the densities and colors of 3D voxels by the masks of SIS, images and camera poses, but also constraint the voxels to be vertical towards the ground plane, following the physical characteristics of pedestrians. 3) Thirdly, the densities of 3D voxels learned by VDR are projected onto Bird-Eyes-View as the final detection results. Extensive experiments on popular multi-view pedestrian detection benchmarks, i.e., Wildtrack and MultiviewX, show that our proposed UMPD approach, as the first unsupervised method to our best knowledge, performs competitively with the previous state-of-the-art supervised techniques. Code will be available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
  file = {C\:\\Users\\AA\\Zotero\\storage\\HBXIS22D\\Liu 等 - 2023 - Unsupervised Multi-view Pedestrian Detection.pdf;C\:\\Users\\AA\\Zotero\\storage\\S6BR6LGN\\2305.html}
}

@misc{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = apr,
  number = {arXiv:2304.08485},
  eprint = {2304.08485},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-10},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\Y8VCB63Y\\Liu 等 - 2023 - Visual Instruction Tuning.pdf;C\:\\Users\\AA\\Zotero\\storage\\WDGKP879\\2304.html}
}

@inproceedings{liuVLPDContextAwarePedestrian2023,
  title = {{{VLPD}}: {{Context-Aware Pedestrian Detection}} via {{Vision-Language Semantic Self-Supervision}}},
  shorttitle = {{{VLPD}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Mengyin and Jiang, Jie and Zhu, Chao and Yin, Xu-Cheng},
  year = {2023},
  pages = {6662--6671},
  urldate = {2023-11-20},
  file = {C:\Users\AA\Zotero\storage\YETQAMCC\Liu 等 - 2023 - VLPD Context-Aware Pedestrian Detection via Visio.pdf}
}

@misc{luFiniteLayerNeural2020,
  title = {Beyond {{Finite Layer Neural Networks}}: {{Bridging Deep Architectures}} and {{Numerical Differential Equations}}},
  shorttitle = {Beyond {{Finite Layer Neural Networks}}},
  author = {Lu, Yiping and Zhong, Aoxiao and Li, Quanzheng and Dong, Bin},
  year = {2020},
  month = mar,
  number = {arXiv:1710.10121},
  eprint = {1710.10121},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multistep method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress ({$>$} 50\%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\CT43U9MY\Lu 等 - 2020 - Beyond Finite Layer Neural Networks Bridging Deep.pdf}
}

@misc{luLearningCompactRecurrent2016,
  title = {Learning {{Compact Recurrent Neural Networks}}},
  author = {Lu, Zhiyun and Sindhwani, Vikas and Sainath, Tara N.},
  year = {2016},
  month = apr,
  number = {arXiv:1604.02594},
  eprint = {1604.02594},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, have produced state-of-the-art results on a variety of speech recognition tasks. However, these models are often too large in size for deployment on mobile devices with memory and latency constraints. In this work, we study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. Our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance. A hybrid strategy of using structured matrices in the bottom layers and shared low-rank factors on the top layers is found to be particularly effective, reducing the parameters of a standard LSTM by 75\%, at a small cost of 0.3\% increase in WER, on a 2,000-hr English Voice Search task.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\UMB25G2Q\Lu 等 - 2016 - Learning Compact Recurrent Neural Networks.pdf}
}

@misc{luoAugmentedLargeLanguage2023,
  title = {Augmented {{Large Language Models}} with {{Parametric Knowledge Guiding}}},
  author = {Luo, Ziyang and Xu, Can and Zhao, Pu and Geng, Xiubo and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  year = {2023},
  month = may,
  number = {arXiv:2305.04757},
  eprint = {2305.04757},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.04757},
  urldate = {2024-12-21},
  abstract = {Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source "white-box" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of "black-box" LLMs on a range of domain knowledge-intensive tasks that require factual (+7.9\%), tabular (+11.9\%), medical (+3.0\%), and multimodal (+8.1\%) knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\YJAXFC2S\\Luo 等 - 2023 - Augmented Large Language Models with Parametric Kn.pdf;C\:\\Users\\AA\\Zotero\\storage\\QE4BY5VZ\\2305.html}
}

@misc{luoDivideConquerEntailmentaware2023,
  title = {Divide \& {{Conquer}} for {{Entailment-aware Multi-hop Evidence Retrieval}}},
  author = {Luo, Fan and Surdeanu, Mihai},
  year = {2023},
  month = nov,
  number = {arXiv:2311.02616},
  eprint = {2311.02616},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.02616},
  urldate = {2024-12-21},
  abstract = {Lexical and semantic matches are commonly used as relevance measurements for information retrieval. Together they estimate the semantic equivalence between the query and the candidates. However, semantic equivalence is not the only relevance signal that needs to be considered when retrieving evidences for multi-hop questions. In this work, we demonstrate that textual entailment relation is another important relevance dimension that should be considered. To retrieve evidences that are either semantically equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval. We propose two ensemble models, EAR and EARnest, which tackle each of the sub-tasks separately and then jointly re-rank sentences with the consideration of the diverse relevance signals. Experimental results on HotpotQA verify that our models not only significantly outperform all the single retrieval models it is based on, but is also more effective than two intuitive ensemble baseline models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {C\:\\Users\\AA\\Zotero\\storage\\PWGCNVQV\\Luo 和 Surdeanu - 2023 - Divide & Conquer for Entailment-aware Multi-hop Ev.pdf;C\:\\Users\\AA\\Zotero\\storage\\F9N7TWWN\\2311.html}
}

@misc{luoValleyVideoAssistant2023,
  title = {Valley: {{Video Assistant}} with {{Large Language}} Model {{Enhanced abilitY}}},
  shorttitle = {Valley},
  author = {Luo, Ruipu and Zhao, Ziwang and Yang, Min and Dong, Junwei and Li, Da and Lu, Pengcheng and Wang, Tao and Hu, Linmei and Qiu, Minghui and Wei, Zhongyu},
  year = {2023},
  month = oct,
  number = {arXiv:2306.07207},
  eprint = {2306.07207},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-10},
  abstract = {Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced abilitY. The Valley consists of a LLM, a temporal modeling module, a visual encoder, and a simple projection module designed to bridge visual and textual modes. To empower Valley with video comprehension and instruction-following capabilities, we construct a video instruction dataset and adopt a two-stage tuning procedure to train it. Specifically, we employ ChatGPT to facilitate the construction of task-oriented conversation data encompassing various tasks, including multi-shot captions, long video descriptions, action recognition, causal relationship inference, etc. Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to align visual and textual modalities and improve the instruction-following capability of Valley. Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\AA\\Zotero\\storage\\GXQ7VCHW\\Luo 等 - 2023 - Valley Video Assistant with Large Language model .pdf;C\:\\Users\\AA\\Zotero\\storage\\D242LUF8\\2306.html}
}

@misc{luUnderstandingImprovingTransformer2019,
  title = {Understanding and {{Improving Transformer From}} a {{Multi-Particle Dynamic System Point}} of {{View}}},
  author = {Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  year = {2019},
  month = jun,
  number = {arXiv:1906.02762},
  eprint = {1906.02762},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.02762},
  urldate = {2023-06-24},
  abstract = {The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is "Macaron-like", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at https://github.com/zhuohan123/macaron-net},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\CI5IFSUC\\Lu et al_2019_Understanding and Improving Transformer From a Multi-Particle Dynamic System.pdf;C\:\\Users\\AA\\Zotero\\storage\\GNDXSLI2\\1906.html}
}

@article{maMultilabelClassificationLabelSpecific2019,
  title = {Multilabel {{Classification With Label-Specific Features}} and {{Classifiers}}: {{A Coarse-}} and {{Fine-Tuned Framework}}},
  shorttitle = {Multilabel {{Classification With Label-Specific Features}} and {{Classifiers}}},
  author = {Ma, Jianghong and Zhang, Haijun and Chow, Tommy},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Cybernetics},
  volume = {PP},
  pages = {1--15},
  doi = {10.1109/TCYB.2019.2932439},
  abstract = {Multilabel classification deals with instances assigned with multiple labels simultaneously. It focuses on learning a mapping from feature space to label a space for out-of-sample extrapolation. The mapping can be seen as a feature selection process in the feature domain or as a classifier training process in the classifier domain. The existing methods do not effectively learn the mapping when combining these two domains together. In this article, we derive a mechanism to extract label-specific features in local and global levels. We also derive a mechanism to train label-specific classifiers in individual and joint levels. Extracting features globally and training classifiers jointly can be seen as a dual process of learning the mapping function on two domains in a coarse-tuned way, while extracting features locally and training classifiers individually can be seen as a dual process of learning the mapping function on two domains in a fine-tuned way. The two-level feature selection and the two-level classifier training are derived to make the entire mapping learning process robust. Finally, extensive experimental results on several benchmarks under four domains are presented to demonstrate the effectiveness of the proposed approach.},
  file = {C:\Users\AA\Zotero\storage\Q48FETQA\ma2019.pdf.pdf}
}

@article{manikandanLanguageModelsAre2023,
  title = {Language Models Are Weak Learners},
  author = {Manikandan, Hariharan and Jiang, Yiding and Kolter, J. Zico},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {50907--50931},
  urldate = {2024-04-16},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\FRWP8EZS\Manikandan et al_2023_Language models are weak learners.pdf}
}

@inproceedings{marelliSICKCureEvaluation2014,
  title = {A {{SICK}} Cure for the Evaluation of Compositional Distributional Semantic Models.},
  booktitle = {Lrec},
  author = {Marelli, Marco and Menini, Stefano and Baroni, Marco and Bentivogli, Luisa and Bernardi, Raffaella and Zamparelli, Roberto},
  year = {2014},
  pages = {216--223},
  publisher = {Reykjavik},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\CDUJZNTB\Marelli et al_2014_A SICK cure for the evaluation of compositional distributional semantic models.pdf}
}

@misc{maTemplatefreePromptTuning2022,
  title = {Template-Free {{Prompt Tuning}} for {{Few-shot NER}}},
  author = {Ma, Ruotian and Zhou, Xin and Gui, Tao and Tan, Yiding and Li, Linyang and Zhang, Qi and Huang, Xuanjing},
  year = {2022},
  month = nov,
  number = {arXiv:2109.13532},
  eprint = {2109.13532},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.13532},
  urldate = {2023-12-14},
  abstract = {Prompt-based methods have been successfully applied in sentence-level few-shot learning tasks, mostly owing to the sophisticated design of templates and label words. However, when applied to token-level labeling tasks such as NER, it would be time-consuming to enumerate the template queries over all potential entity spans. In this work, we propose a more elegant method to reformulate NER tasks as LM problems without any templates. Specifically, we discard the template construction process while maintaining the word prediction paradigm of pre-training models to predict a class-related pivot word (or label word) at the entity position. Meanwhile, we also explore principled ways to automatically search for appropriate label words that the pre-trained models can easily adapt to. While avoiding complicated template-based process, the proposed LM objective also reduces the gap between different objectives used in pre-training and fine-tuning, thus it can better benefit the few-shot performance. Experimental results demonstrate the effectiveness of the proposed method over bert-tagger and template-based method under few-shot setting. Moreover, the decoding speed of the proposed method is up to 1930.12 times faster than the template-based method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\AWNMQ3HF\\Ma 等 - 2022 - Template-free Prompt Tuning for Few-shot NER.pdf;C\:\\Users\\AA\\Zotero\\storage\\KY4NFULX\\2109.html}
}

@misc{minRecentAdvancesNatural2021,
  title = {Recent {{Advances}} in {{Natural Language Processing}} via {{Large Pre-Trained Language Models}}: {{A Survey}}},
  shorttitle = {Recent {{Advances}} in {{Natural Language Processing}} via {{Large Pre-Trained Language Models}}},
  author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heinz, Ilana and Roth, Dan},
  year = {2021},
  month = nov,
  number = {arXiv:2111.01243},
  eprint = {2111.01243},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\AA\Zotero\storage\CNLUS4E6\Min 等 - 2021 - Recent Advances in Natural Language Processing via.pdf}
}

@article{mohseniModelReductionNonlinear2018,
  title = {Model {{Reduction}} of {{Nonlinear Systems}} by {{Trajectory Piecewise Linear Based}} on {{Output-Weighting Models}}: {{A Balanced-Truncation Methodology}}},
  shorttitle = {Model {{Reduction}} of {{Nonlinear Systems}} by {{Trajectory Piecewise Linear Based}} on {{Output-Weighting Models}}},
  author = {Mohseni, Seyed Saleh and Yazdanpanah, Mohamad Javad and Ranjbar Noei, Abolfazl},
  year = {2018},
  month = jun,
  journal = {Iranian Journal of Science and Technology, Transactions of Electrical Engineering},
  volume = {42},
  number = {2},
  pages = {195--206},
  issn = {2228-6179, 2364-1827},
  doi = {10.1007/s40998-018-0058-4},
  urldate = {2023-05-26},
  abstract = {This paper extends some methods in model order reduction (MOR) of nonlinear models by trajectory piecewise-linear models based on output weighting (TPWLOW). The devised methods generalize the balanced-truncation method as the core for the proposed algorithms. These algorithms preserve the stability in some mentioned conditions. An error bound of reduction is calculated for these algorithms. In addition, a Krylov--TBR combinational MOR is also casted which benefits from the advantages of both balanced-truncation and Krylov subspace methods. The results show that in balancedtruncation-based algorithms, the efficiency of the first proposed algorithm fairly exceeds the second one. In addition, it is shown that the Krylov--TBR algorithm can reach the same efficiency despite remarkable lower order, provided that the designer can find a stable subspace. The robustness of TPWLOW-reduced models to linearization point movement is also evaluated, which shows the less sensitivity of the proposed methods in comparison with formerly introduced trajectory piecewise-linear models.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\J8NFDXZT\Mohseni 等 - 2018 - Model Reduction of Nonlinear Systems by Trajectory.pdf}
}

@misc{mozerDiscreteEventContinuous2017,
  title = {Discrete {{Event}}, {{Continuous Time RNNs}}},
  author = {Mozer, Michael C. and Kazakov, Denis and Lindsey, Robert V.},
  year = {2017},
  month = oct,
  number = {arXiv:1710.04110},
  eprint = {1710.04110},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-11},
  abstract = {We investigate recurrent neural network architectures for event-sequence processing. Event sequences, characterized by discrete observations stamped with continuous-valued times of occurrence, are challenging due to the potentially wide dynamic range of relevant time scales as well as interactions between time scales. We describe four forms of inductive bias that should benefit architectures for event sequences: temporal locality, position and scale homogeneity, and scale interdependence. We extend the popular gated recurrent unit (GRU) architecture to incorporate these biases via intrinsic temporal dynamics, obtaining a continuous-time GRU. The CT-GRU arises by interpreting the gates of a GRU as selecting a time scale of memory, and the CT-GRU generalizes the GRU by incorporating multiple time scales of memory and performing context-dependent selection of time scales for information storage and retrieval. Event time-stamps drive decay dynamics of the CT-GRU, whereas they serve as generic additional inputs to the GRU. Despite the very different manner in which the two models consider time, their performance on eleven data sets we examined is essentially identical. Our surprising results point both to the robustness of GRU and LSTM architectures for handling continuous time, and to the potency of incorporating continuous dynamics into neural architectures.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LGRD8M6V\\Mozer et al_2017_Discrete Event, Continuous Time RNNs.pdf;C\:\\Users\\AA\\Zotero\\storage\\ESKHU88J\\1710.html}
}

@inproceedings{nakkiranCompressingDeepNeural2015,
  title = {Compressing {{Deep Neural Networks}} Using a {{Rank-Constrained Topology}}},
  booktitle = {Proceedings of {{Annual Conference}} of the {{International Speech Communication Association}} ({{Interspeech}})},
  author = {Nakkiran, Preetum and Alvarez, Raziel and Prabhavalkar, Rohit and Parada, Carolina},
  year = {2015},
  pages = {1473--1477},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\A47793XQ\Nakkiran et al_2015_Compressing Deep Neural Networks using a Rank-Constrained Topology.pdf}
}

@article{nanFeTaQAFreeformTable2022,
  title = {{{FeTaQA}}: {{Free-form Table Question Answering}}},
  shorttitle = {{{FeTaQA}}},
  author = {Nan, Linyong and Hsieh, Chiachun and Mao, Ziming and Lin, Xi Victoria and Verma, Neha and Zhang, Rui and Kry{\'s}ci{\'n}ski, Wojciech and Schoelkopf, Hailey and Kong, Riley and Tang, Xiangru and Mutuma, Mutethia and Rosand, Ben and Trindade, Isabel and Bandaru, Renusree and Cunningham, Jacob and Xiong, Caiming and Radev, Dragomir and Radev, Dragomir},
  year = {2022},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {35--49},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00446},
  urldate = {2024-03-22},
  abstract = {Existing table question answering datasets contain abundant factual questions that primarily evaluate a QA system's comprehension of query and tabular data. However, restricted by their short-form answers, these datasets fail to include question--answer interactions that represent more advanced and naturally occurring information needs: questions that ask for reasoning and integration of information pieces retrieved from a structured knowledge source. To complement the existing datasets and to reveal the challenging nature of the table-based question answering task, we introduce FeTaQA, a new dataset with 10K Wikipedia-based \{table, question, free-form answer, supporting table cells\} pairs. FeTaQA is collected from noteworthy descriptions of Wikipedia tables that contain information people tend to seek; generation of these descriptions requires advanced processing that humans perform on a daily basis: Understand the question and table, retrieve, integrate, infer, and conduct text planning and surface realization to generate an answer. We provide two benchmark methods for the proposed task: a pipeline method based on semantic parsing-based QA systems and an end-to-end method based on large pretrained text generation models, and show that FeTaQA poses a challenge for both methods.},
  file = {C\:\\Users\\AA\\Zotero\\storage\\UAFL48LP\\Nan et al_2022_FeTaQA.pdf;C\:\\Users\\AA\\Zotero\\storage\\NVBF7MD7\\FeTaQA-Free-form-Table-Question-Answering.html}
}

@article{nanFetaqaFreeformTable2022a,
  title = {Fetaqa: {{Free-form}} Table Question Answering},
  shorttitle = {Fetaqa},
  author = {Nan, Linyong and Hsieh, Chiachun and Mao, Ziming and Lin, Xi Victoria and Verma, Neha and Zhang, Rui and Kry{\'s}ci{\'n}ski, Wojciech and Schoelkopf, Hailey and Kong, Riley and Tang, Xiangru},
  year = {2022},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {35--49},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA {\dots}},
  doi = {10.1162/tacl_a_00446},
  urldate = {2024-03-22},
  file = {C:\Users\AA\Zotero\storage\27QFSUIV\109273.html}
}

@misc{narangExploringSparsityRecurrent2017,
  title = {Exploring {{Sparsity}} in {{Recurrent Neural Networks}}},
  author = {Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  year = {2017},
  month = nov,
  number = {arXiv:1704.05119},
  eprint = {1704.05119},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {Recurrent Neural Networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8{\texttimes} and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse matrix multiply. Benchmarks show that using our technique model size can be reduced by 90\% and speed-up is around 2{\texttimes} to 7{\texttimes}.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\6SACDHRX\Narang 等 - 2017 - Exploring Sparsity in Recurrent Neural Networks.pdf}
}

@article{narendraIdentificationControlDynamical1990,
  title = {Identification and Control of Dynamical Systems Using Neural Networks},
  author = {Narendra, K.S. and Parthasarathy, K.},
  year = {1990},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {1},
  number = {1},
  pages = {4--27},
  issn = {10459227},
  doi = {10.1109/72.80202},
  urldate = {2023-05-26},
  abstract = {The paper demonstrates that neural networks can be used effectively for the identification and control of nonlinear dynamical systems. The emphasis of the paper is on models for both identification and control. Static and dynamic back-propagation methods for the adjustment of parameters are discussed. In the models that are introduced, multilayer and recurrent networks are interconnected in novel configurations and hence there is a real need to study them in a unified fashion. Simulation results reveal that the identification and adaptive control schemes suggested are practically feasible. Basic concepts and definitions are introduced throughout the paper, and theoretical questions which have to be addressed are also described.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\V7E5P358\Narendra 和 Parthasarathy - 1990 - Identification and control of dynamical systems us.pdf}
}

@article{neilPhasedLSTMAccelerating,
  title = {Phased {{LSTM}}: {{Accelerating Recurrent Network Training}} for {{Long}} or {{Event-based Sequences}}},
  author = {Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
  abstract = {Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\42KMC7HN\Neil 等 - Phased LSTM Accelerating Recurrent Network Traini.pdf}
}

@article{nomuraStructureInhomogeneousTurbulence1993,
  title = {The Structure of Inhomogeneous Turbulence in Variable Density Nonpremixed Flames},
  author = {Nomura, K. K. and Elghobashi, S. E.},
  year = {1993},
  month = nov,
  journal = {Theoretical and Computational Fluid Dynamics},
  volume = {5--5},
  number = {4-5},
  pages = {153--175},
  issn = {0935-4964, 1432-2250},
  doi = {10.1007/BF00271656},
  urldate = {2024-06-11},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\6VPQ5QEB\\nomura1993.pdf.pdf;C\:\\Users\\AA\\Zotero\\storage\\NAPZYF8V\\Nomura_Elghobashi_1993_The structure of inhomogeneous turbulence in variable density nonpremixed flames.pdf}
}

@article{oaksPiecewiseLinearControl1976,
  title = {Piecewise {{Linear Control}} of {{Nonlinear Systems}}},
  author = {Oaks, Orville Jay and Cook, Gerald},
  year = {1976},
  month = feb,
  journal = {IEEE Transactions on Industrial Electronics and Control Instrumentation},
  volume = {IECI-23},
  number = {1},
  pages = {56--63},
  issn = {0018-9421},
  doi = {10.1109/TIECI.1976.351348},
  urldate = {2023-05-26},
  abstract = {Using a Taylor series expansion, nonlinear control systems are approximated about desired equilibrium points in state space as linear control systems. The techniques of the linear regulator problem are then used to calculate the linear state variable feedback gains needed to keep the system at the desired equilibrium point. Liapunov's second method is the basis for determining stable regions of operation about the equilibrium points. When driving the system from one equilibrium point to another, the piecewise linearization method is also used but with intermediate equilibrium points successively utilized. Enough equilibrium points are used so that the system is stable throughout the region of interest. Thus, a nonlinear system is controlled using linear state variable feedback. A small digital computer is required to implement this strategy.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\K85DH695\Oaks 和 Cook - 1976 - Piecewise Linear Control of Nonlinear Systems.pdf}
}

@misc{ogundareIndustrialEngineeringLarge2023,
  title = {Industrial {{Engineering}} with {{Large Language Models}}: {{A}} Case Study of {{ChatGPT}}'s Performance on {{Oil}} \& {{Gas}} Problems},
  shorttitle = {Industrial {{Engineering}} with {{Large Language Models}}},
  author = {Ogundare, Oluwatosin and Madasu, Srinath and Wiggins, Nathanial},
  year = {2023},
  month = apr,
  number = {arXiv:2304.14354},
  eprint = {2304.14354},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {Large Language Models (LLMs) have shown great potential in solving complex problems in various fields, including oil and gas engineering and other industrial engineering disciplines like factory automation, PLC programming etc. However, automatic identification of strong and weak solutions to fundamental physics equations governing several industrial processes remain a challenging task. This paper identifies the limitation of current LLM approaches, particularly ChatGPT in selected practical problems native to oil and gas engineering but not exclusively. The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\7XP7UNYQ\\Ogundare 等 - 2023 - Industrial Engineering with Large Language Models.pdf;C\:\\Users\\AA\\Zotero\\storage\\G3RI6WNH\\2304.html}
}

@misc{OpenAIPlatform,
  title = {{{OpenAI Platform}}},
  urldate = {2024-12-19},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  howpublished = {https://platform.openai.com},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\ZTV4QNQU\gpt-3-5.html}
}

@article{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {27730--27744},
  urldate = {2024-01-15},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\B59ZJ3P7\Ouyang 等 - 2022 - Training language models to follow instructions wi.pdf}
}

@article{ouyangTrainingLanguageModels2022a,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex},
  year = {2022},
  journal = {Advances in neural information processing systems},
  volume = {35},
  pages = {27730--27744},
  urldate = {2024-12-19},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\9KSEN8NH\Ouyang 等 - 2022 - Training language models to follow instructions wi.pdf}
}

@misc{papernotTechnicalReportCleverHans2018,
  title = {Technical {{Report}} on the {{CleverHans}} v2.1.0 {{Adversarial Examples Library}}},
  author = {Papernot, Nicolas and Faghri, Fartash and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Kurakin, Alexey and Xie, Cihang and Sharma, Yash and Brown, Tom and Roy, Aurko and Matyasko, Alexander and Behzadan, Vahid and Hambardzumyan, Karen and Zhang, Zhishuai and Juang, Yi-Lin and Li, Zhi and Sheatsley, Ryan and Garg, Abhibhav and Uesato, Jonathan and Gierke, Willi and Dong, Yinpeng and Berthelot, David and Hendricks, Paul and Rauber, Jonas and Long, Rujun and McDaniel, Patrick},
  year = {2018},
  month = jun,
  number = {arXiv:1610.00768},
  eprint = {1610.00768},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.00768},
  urldate = {2025-02-19},
  abstract = {CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\GLML789M\\Papernot 等 - 2018 - Technical Report on the CleverHans v2.1.0 Adversar.pdf;C\:\\Users\\AA\\Zotero\\storage\\ZYVK976L\\1610.html}
}

@misc{pasupatCompositionalSemanticParsing2015,
  title = {Compositional {{Semantic Parsing}} on {{Semi-Structured Tables}}},
  author = {Pasupat, Panupong and Liang, Percy},
  year = {2015},
  month = aug,
  number = {arXiv:1508.00305},
  eprint = {1508.00305},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-22},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\I8FHUDMQ\\Pasupat_Liang_2015_Compositional Semantic Parsing on Semi-Structured Tables.pdf;C\:\\Users\\AA\\Zotero\\storage\\ZVJSMQKL\\1508.html}
}

@misc{pasupatCompositionalSemanticParsing2015a,
  title = {Compositional {{Semantic Parsing}} on {{Semi-Structured Tables}}},
  author = {Pasupat, Panupong and Liang, Percy},
  year = {2015},
  month = aug,
  number = {arXiv:1508.00305},
  eprint = {1508.00305},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-20},
  abstract = {Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\R5YIGKQR\\Pasupat_Liang_2015_Compositional Semantic Parsing on Semi-Structured Tables.pdf;C\:\\Users\\AA\\Zotero\\storage\\FKCZXDGZ\\1508.html}
}

@article{pearlmutterLearningStateSpace,
  title = {Learning State Space Trajectories in Recurrent Neural Networks},
  author = {Pearlmutter, Barak},
  abstract = {We describe a number of procedures for finding dE/dwij where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and w,y are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize {\pounds}, so these procedures form the kernels of connectionist learning algorithms. Simulations in which networks are taught to move through limit cycles are shown. We also describe a number of elaborations of the basic idea, such as mutable time delays and teacher forcing, and conclude with a complexity analysis. This type of network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.},
  langid = {english},
  keywords = {Multiple DOI},
  file = {C:\Users\AA\Zotero\storage\8BPE4WPY\Pearlmutter - Learning state space trajectories in recurrent neu.pdf}
}

@inproceedings{peiDeepXploreAutomatedWhitebox2017,
  title = {{{DeepXplore}}: {{Automated Whitebox Testing}} of {{Deep Learning Systems}}},
  shorttitle = {{{DeepXplore}}},
  booktitle = {Proceedings of the 26th {{Symposium}} on {{Operating Systems Principles}}},
  author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
  year = {2017},
  month = oct,
  pages = {1--18},
  publisher = {ACM},
  address = {Shanghai China},
  doi = {10.1145/3132747.3132785},
  urldate = {2025-02-19},
  isbn = {978-1-4503-5085-3},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\525TJGZR\\Pei 等 - 2017 - DeepXplore Automated Whitebox Testing of Deep Lea.pdf;C\:\\Users\\AA\\Zotero\\storage\\UKXL5627\\pei2017.pdf.pdf}
}

@misc{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {1802.05365},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\AA\Zotero\storage\S9AV34XA\Peters 等 - 2018 - Deep contextualized word representations.pdf}
}

@misc{petersTuneNotTune2019,
  title = {To {{Tune}} or {{Not}} to {{Tune}}? {{Adapting Pretrained Representations}} to {{Diverse Tasks}}},
  shorttitle = {To {{Tune}} or {{Not}} to {{Tune}}?},
  author = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
  year = {2019},
  month = jun,
  number = {arXiv:1903.05987},
  eprint = {1903.05987},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\4HR8PIPV\\Peters 等 - 2019 - To Tune or Not to Tune Adapting Pretrained Repres.pdf;C\:\\Users\\AA\\Zotero\\storage\\E4SHNFCL\\1903.html}
}

@misc{petroniLanguageModelsKnowledge2019,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  author = {Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
  year = {2019},
  month = sep,
  number = {arXiv:1909.01066},
  eprint = {1909.01066},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-03},
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\FNRMPIKC\\Petroni 等 - 2019 - Language Models as Knowledge Bases.pdf;C\:\\Users\\AA\\Zotero\\storage\\JCCVBBSY\\1909.html}
}

@incollection{phillipsPMTBRFamilyApproximate2008,
  title = {{{PMTBR}}: {{A Family}} of {{Approximate Principal-components-like Reduction Algorithms}}},
  shorttitle = {{{PMTBR}}},
  booktitle = {Model {{Order Reduction}}: {{Theory}}, {{Research Aspects}} and {{Applications}}},
  author = {Phillips, Joel R. and Zhu, Zhenhai and Silveira, L. Miguel},
  editor = {Bock, Hans-Georg and De Hoog, Frank and Friedman, Avner and Gupta, Arvind and Neunzert, Helmut and Pulleyblank, William R. and Rusten, Torgeir and Santosa, Fadil and Tornberg, Anna-Karin and Bonilla, Luis L. and Mattheij, Robert and Scherzer, Otmar and Schilders, Wilhelmus H. A. and Van Der Vorst, Henk A. and Rommes, Joost},
  year = {2008},
  volume = {13},
  pages = {111--132},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-78841-6_6},
  urldate = {2023-05-26},
  isbn = {978-3-540-78840-9 978-3-540-78841-6},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\VZTES3MJ\Phillips 等 - 2008 - PMTBR A Family of Approximate Principal-component.pdf}
}

@article{pinedaGeneralizationBackPropagation,
  title = {Generalization of {{Back}} Propagation to {{Recurrent}} and {{Higher Order Neural Networks}}},
  author = {Pineda, Fernando J},
  abstract = {A general method for deriving backpropagation algorithms for networks with recurrent and higher order networks is introduced. The propagation of activation in these networks is determined by dissipative differential equations. The error signal is backpropagated by integrating an associated differential equation. The method is introduced by applying it to the recurrent generalization of the feedforward backpropagation network. The method is extended to the case of higher order networks and to a constrained dynamical system for training a content addressable memory. The essential feature of the adaptive algorithms is that adaptive equation has a simple outer product form. Preliminary experiments suggest that learning can occur very rapidly in networks with recurrent connections. The continuous formalism makes the new approach more suitable for implementation in VLSI.},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\T228R7U9\Pineda - Generalization of Back propagation to Recurrent an.pdf}
}

@misc{pontiXCOPAMultilingualDataset2020,
  title = {{{XCOPA}}: {{A Multilingual Dataset}} for {{Causal Commonsense Reasoning}}},
  shorttitle = {{{XCOPA}}},
  author = {Ponti, Edoardo Maria and Glava{\v s}, Goran and Majewska, Olga and Liu, Qianchu and Vuli{\'c}, Ivan and Korhonen, Anna},
  year = {2020},
  month = oct,
  number = {arXiv:2005.00333},
  eprint = {2005.00333},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-18},
  abstract = {In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur{\textbackslash}'imac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\HSRHJ9BN\\Ponti 等 - 2020 - XCOPA A Multilingual Dataset for Causal Commonsen.pdf;C\:\\Users\\AA\\Zotero\\storage\\FJ2TP3CR\\2005.html}
}

@misc{prabhavalkarCompressionRecurrentNeural2016,
  title = {On the {{Compression}} of {{Recurrent Neural Networks}} with an {{Application}} to {{LVCSR}} Acoustic Modeling for {{Embedded Speech Recognition}}},
  author = {Prabhavalkar, Rohit and Alsharif, Ouais and Bruguier, Antoine and McGraw, Ian},
  year = {2016},
  month = may,
  number = {arXiv:1603.08042},
  eprint = {1603.08042},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-12},
  abstract = {We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {titleTranslation:\\
titleTranslation:\\
titleTranslation:\\
titleTranslation:\\
titleTranslation:\\
titleTranslation:\\
titleTranslation:},
  file = {C:\Users\AA\Zotero\storage\4CH3KW6T\Prabhavalkar 等 - 2016 - On the Compression of Recurrent Neural Networks wi.pdf}
}

@article{radfordImprovingLanguageUnderstanding2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {OpenAI},
  urldate = {2023-12-26},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\E9DBDJUD\Radford 等 - 2018 - Improving language understanding by generative pre.pdf}
}

@article{radfordImprovingLanguageUnderstanding2018a,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec},
  year = {2018},
  urldate = {2025-01-03},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\BLEIA9RD\Radford - 2018 - Improving language understanding by generative pre.pdf}
}

@article{radfordLanguageModelsAre2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  urldate = {2023-12-19},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\5H5HNHN8\Radford 等 - 2019 - Language models are unsupervised multitask learner.pdf}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning Transferable Visual Models from Natural Language Supervision},
  booktitle = {International Conference on Machine Learning},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack},
  year = {2021},
  pages = {8748--8763},
  publisher = {PMLR},
  urldate = {2023-11-13},
  file = {C:\Users\AA\Zotero\storage\QM8USXL9\Radford 等 - 2021 - Learning transferable visual models from natural l.pdf}
}

@article{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  journal = {The Journal of Machine Learning Research},
  volume = {21},
  number = {1},
  pages = {5485--5551},
  publisher = {JMLRORG},
  urldate = {2023-12-26},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\ZTFVD64S\Raffel 等 - 2020 - Exploring the limits of transfer learning with a u.pdf}
}

@misc{rajkumarEvaluatingTexttoSQLCapabilities2022,
  title = {Evaluating the {{Text-to-SQL Capabilities}} of {{Large Language Models}}},
  author = {Rajkumar, Nitarshan and Li, Raymond and Bahdanau, Dzmitry},
  year = {2022},
  month = mar,
  number = {arXiv:2204.00498},
  eprint = {2204.00498},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-19},
  abstract = {We perform an empirical evaluation of Text-to-SQL capabilities of the Codex language model. We find that, without any finetuning, Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models finetuned on such few-shot examples.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\UEJ2IFFB\\Rajkumar et al_2022_Evaluating the Text-to-SQL Capabilities of Large Language Models.pdf;C\:\\Users\\AA\\Zotero\\storage\\2TCGA3IK\\2204.html}
}

@article{reidSubformerExploringWeight2021,
  title = {Subformer: {{Exploring}} Weight Sharing for Parameter Efficiency in Generative Transformers},
  shorttitle = {Subformer},
  author = {Reid, Machel and {Marrese-Taylor}, Edison and Matsuo, Yutaka},
  year = {2021},
  journal = {arXiv preprint arXiv:2101.00234},
  eprint = {2101.00234},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C\:\\Users\\AA\\Zotero\\storage\\PJI84WLR\\Reid et al_2021_Subformer.pdf;C\:\\Users\\AA\\Zotero\\storage\\REUJAP45\\2101.html}
}

@article{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, N.},
  year = {2019},
  journal = {arXiv preprint arXiv:1908.10084},
  eprint = {1908.10084},
  urldate = {2024-09-02},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\LK6ZKYGX\Reimers - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf}
}

@article{reisPassivitypreservingBalancedTruncation,
  title = {Passivity-Preserving Balanced Truncation for Electrical Circuits},
  author = {Reis, Timo and Stykel, Tatjana},
  doi = {10.1109/TCAD.2010.2059330},
  abstract = {We present a passivity-preserving balanced truncation model reduction method for differential-algebraic equations arising in circuit simulation. This method is based on balancing the solutions of projected Lur'e equations. By making use of the special structure of circuit equations, we can reduce the numerical effort for balanced truncation significantly. It is shown that the property of reciprocity is also preserved in the reduced-order model. Network topological interpretations of certain circuit effects are given. The presented theory is illustrated by a numerical example.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\XRXMNI57\Reis 和 Stykel - Passivity-preserving balanced truncation for elect.pdf}
}

@article{rubanovaLatentODEsIrregularlySampled,
  title = {Latent {{ODEs}} for {{Irregularly-Sampled Time Series}}},
  author = {Rubanova, Yulia and Chen, Ricky T Q and Duvenaud, David},
  abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\5EPPLFSV\Rubanova 等 - Latent ODEs for Irregularly-Sampled Time Series.pdf}
}

@article{sarzynska-wawerDetectingFormalThought2021,
  title = {Detecting Formal Thought Disorder by Deep Contextualized Word Representations},
  author = {{Sarzynska-Wawer}, Justyna and Wawer, Aleksander and Pawlak, Aleksandra and Szymanowska, Julia and Stefaniak, Izabela and Jarkiewicz, Michal and Okruszek, Lukasz},
  year = {2021},
  journal = {Psychiatry Research},
  volume = {304},
  pages = {114135},
  publisher = {Elsevier},
  doi = {10.1016/j.psychres.2021.114135},
  urldate = {2023-12-26},
  file = {C:\Users\AA\Zotero\storage\4YGLV7GW\Sarzynska-Wawer 等 - 2021 - Detecting formal thought disorder by deep contextu.pdf}
}

@article{schaferRECURRENTNEURALNETWORKS2007,
  title = {{{RECURRENT NEURAL NETWORKS ARE UNIVERSAL APPROXIMATORS}}},
  author = {Sch{\"a}fer, Anton Maximilian and Zimmermann, Hans-Georg},
  year = {2007},
  month = aug,
  journal = {International Journal of Neural Systems},
  volume = {17},
  number = {04},
  pages = {253--263},
  issn = {0129-0657, 1793-6462},
  doi = {10.1142/S0129065707001111},
  urldate = {2023-05-26},
  abstract = {Recurrent Neural Networks (RNN) have been developed for a better understanding and analysis of open dynamical systems. Still the question often arises if RNN are able to map every open dynamical system, which would be desirable for a broad spectrum of applications. In this article we give a proof for the universal approximation ability of RNN in state space model form and even extend it to Error Correction and Normalized Recurrent Neural Networks.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\EI49JILQ\Schäfer 和 Zimmermann - 2007 - RECURRENT NEURAL NETWORKS ARE UNIVERSAL APPROXIMAT.pdf}
}

@article{schonSystemIdentificationNonlinear2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  journal = {Automatica},
  volume = {47},
  number = {1},
  pages = {39--49},
  issn = {00051098},
  doi = {10.1016/j.automatica.2010.10.013},
  urldate = {2023-05-26},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provide arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\5Y8HRGI2\Schön 等 - 2011 - System identification of nonlinear state-space mod.pdf}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-15},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\HBR9BQCD\\Schulman 等 - 2017 - Proximal Policy Optimization Algorithms.pdf;C\:\\Users\\AA\\Zotero\\storage\\NERSNPIY\\1707.html}
}

@inproceedings{senguptaBackgroundMattingWorld2020,
  title = {Background {{Matting}}: {{The World Is Your Green Screen}}},
  shorttitle = {Background {{Matting}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sengupta, Soumyadip and Jayaram, Vivek and Curless, Brian and Seitz, Steven M. and {Kemelmacher-Shlizerman}, Ira},
  year = {2020},
  month = jun,
  pages = {2288--2297},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00236},
  urldate = {2023-05-06},
  abstract = {We propose a method for creating a matte -- the per-pixel foreground color and alpha -- of a person by taking photos or videos in an everyday setting with a handheld camera. Most existing matting methods require a green screen background or a manually created trimap to produce a good matte. Automatic, trimap-free methods are appearing, but are not of comparable quality. In our trimap free approach, we ask the user to take an additional photo of the background without the subject at the time of capture. This step requires a small amount of foresight but is far less timeconsuming than creating a trimap. We train a deep network with an adversarial loss to predict the matte. We first train a matting network with supervised loss on ground truth data with synthetic composites. To bridge the domain gap to real imagery with no labeling, we train another matting network guided by the first network and by a discriminator that judges the quality of composites. We demonstrate results on a wide variety of photos and videos and show significant improvement over the state of the art.},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\NVIG4DA8\Sengupta 等 - 2020 - Background Matting The World Is Your Green Screen.pdf}
}

@misc{shazeerFastTransformerDecoding2019,
  title = {Fast {{Transformer Decoding}}: {{One Write-Head}} Is {{All You Need}}},
  shorttitle = {Fast {{Transformer Decoding}}},
  author = {Shazeer, Noam},
  year = {2019},
  month = nov,
  number = {arXiv:1911.02150},
  eprint = {1911.02150},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-18},
  abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\AA\Zotero\storage\JXGIP5CN\Shazeer - 2019 - Fast Transformer Decoding One Write-Head is All Y.pdf}
}

@article{sherstinskyFundamentalsRecurrentNeural2020,
  title = {Fundamentals of {{Recurrent Neural Network}} ({{RNN}}) and {{Long Short-Term Memory}} ({{LSTM}}) {{Network}}},
  author = {Sherstinsky, Alex},
  year = {2020},
  month = mar,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {404},
  eprint = {1808.03314},
  primaryclass = {cs, stat},
  pages = {132306},
  issn = {01672789},
  doi = {10.1016/j.physd.2019.132306},
  urldate = {2023-05-26},
  abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of ``unrolling'' an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the ``Vanilla LSTM''1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\9HCQWYVN\Sherstinsky - 2020 - Fundamentals of Recurrent Neural Network (RNN) and.pdf}
}

@inproceedings{shiDetectingOrientedText2017,
  title = {Detecting Oriented Text in Natural Images by Linking Segments},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Shi, Baoguang and Bai, Xiang and Belongie, Serge},
  year = {2017},
  pages = {2550--2558},
  urldate = {2025-02-20},
  file = {C:\Users\AA\Zotero\storage\LVT8AIPD\Shi 等 - 2017 - Detecting oriented text in natural images by linki.pdf}
}

@article{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical Networks for Few-Shot Learning},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  urldate = {2023-12-26},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\WVXTFMWJ\Snell 等 - 2017 - Prototypical networks for few-shot learning.pdf}
}

@inproceedings{socherRecursiveDeepModels2013,
  title = {Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y. and Potts, Christopher},
  year = {2013},
  pages = {1631--1642},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\57SX5ITZ\Socher et al_2013_Recursive deep models for semantic compositionality over a sentiment treebank.pdf}
}

@misc{srivastavaImitationGameQuantifying2023,
  title = {Beyond the {{Imitation Game}}: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  shorttitle = {Beyond the {{Imitation Game}}},
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and {Garriga-Alonso}, Adri{\`a} and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka{\c s}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ram{\'i}rez, C{\'e}sar Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and {Callison-Burch}, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and Gonz{\'a}lez, Daniel Mosegu{\'i} and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and {Mart{\'i}nez-Plumed}, Fernando and Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germ{\'a}n and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and {Jaimovitch-L{\'o}pez}, Gonzalo and Betz, Gregor and {Gur-Ari}, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Sch{\"u}tze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern{\'a}ndez and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Koco{\'n}, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and {Sohl-Dickstein}, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and {Hernandez-Orallo}, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and {Contreras-Ochando}, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Col{\'o}n, Luis Oliveros and Metz, Luke and {\c S}enel, L{\"u}tfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ram{\'i}rez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw{\k e}drowski, Micha{\l} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan A. and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Milli{\`e}re, Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th{\'e}o and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
  year = {2023},
  month = jun,
  number = {arXiv:2206.04615},
  eprint = {2206.04615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.04615},
  urldate = {2024-12-21},
  abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\NMLJ72VX\\Srivastava 等 - 2023 - Beyond the Imitation Game Quantifying and extrapo.pdf;C\:\\Users\\AA\\Zotero\\storage\\5FEYK92G\\2206.html}
}

@inproceedings{stantonDoesKnowledgeDistillation2021,
  title = {Does {{Knowledge Distillation Really Work}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
  year = {2021},
  volume = {34},
  pages = {6906--6919},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-07-08},
  abstract = {Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher --- and that more closely matching the teacher paradoxically does not always lead to better student generalization.},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\PTWMCD9D\Stanton et al_2021_Does Knowledge Distillation Really Work.pdf}
}

@article{steinbrecherModelOrderReduction2013,
  title = {Model Order Reduction of Nonlinear Circuit Equations: {{MODEL ORDER REDUCTION OF NONLINEAR CIRCUIT EQUATIONS}}},
  shorttitle = {Model Order Reduction of Nonlinear Circuit Equations},
  author = {Steinbrecher, Andreas and Stykel, Tatjana},
  year = {2013},
  month = dec,
  journal = {International Journal of Circuit Theory and Applications},
  volume = {41},
  number = {12},
  pages = {1226--1247},
  issn = {00989886},
  doi = {10.1002/cta.1821},
  urldate = {2023-05-26},
  abstract = {In this paper, we present a model order reduction approach for nonlinear circuit equations with a small number of nonlinear elements. This approach is based on the decoupling of linear and nonlinear subcircuits and reducing the linear part using balancing-related model reduction techniques. The efficiency and applicability of the proposed model reduction approach is demonstrated on numerical examples.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\UPYDVZ6U\Steinbrecher 和 Stykel - 2013 - Model order reduction of nonlinear circuit equatio.pdf}
}

@article{stykelBalancedTruncationModel2006,
  title = {Balanced Truncation Model Reduction for Semidiscretized {{Stokes}} Equation},
  author = {Stykel, Tatjana},
  year = {2006},
  month = jun,
  journal = {Linear Algebra and its Applications},
  volume = {415},
  number = {2-3},
  pages = {262--289},
  issn = {00243795},
  doi = {10.1016/j.laa.2004.01.015},
  urldate = {2023-05-26},
  abstract = {We discuss model reduction of linear continuous-time descriptor systems that arise in the control of semidiscretized Stokes equations. Balanced truncation model reduction methods for descriptor systems are presented. These methods are closely related to the proper and improper controllability and observability Gramians and Hankel singular values of descriptor systems. The Gramians can be computed by solving projected generalized Lyapunov equations. Important properties of the balanced truncation approach are that the asymptotic stability is preserved in the reduced order system and there is an a priori bound on the approximation error. We demonstrate the application of balanced truncation model reduction to the semidiscretized Stokes equation.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\YVWLHJNL\Stykel - 2006 - Balanced truncation model reduction for semidiscre.pdf}
}

@article{stykelModelReductionNonlinear,
  title = {Model Reduction of Nonlinear Circuit Equations},
  author = {Stykel, Tatjana},
  langid = {english},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\ES5KF8CD\Stykel - Model reduction of nonlinear circuit equations.pdf}
}

@misc{sunERNIEEnhancedRepresentation2019,
  title = {{{ERNIE}}: {{Enhanced Representation}} through {{Knowledge Integration}}},
  shorttitle = {{{ERNIE}}},
  author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and Tian, Hao and Wu, Hua},
  year = {2019},
  month = apr,
  number = {arXiv:1904.09223},
  eprint = {1904.09223},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LA7EKYQ5\\Sun 等 - 2019 - ERNIE Enhanced Representation through Knowledge I.pdf;C\:\\Users\\AA\\Zotero\\storage\\A2FNCICQ\\1904.html}
}

@misc{sunRetentiveNetworkSuccessor2023,
  title = {Retentive {{Network}}: {{A Successor}} to {{Transformer}} for {{Large Language Models}}},
  shorttitle = {Retentive {{Network}}},
  author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  year = {2023},
  month = aug,
  number = {arXiv:2307.08621},
  eprint = {2307.08621},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-09},
  abstract = {In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\DTXVXT2B\Sun 等 - 2023 - Retentive Network A Successor to Transformer for .pdf}
}

@misc{sutskeverSequenceSequenceLearning2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.3215},
  urldate = {2025-01-03},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\M3E27WZU\\Sutskever 等 - 2014 - Sequence to Sequence Learning with Neural Networks.pdf;C\:\\Users\\AA\\Zotero\\storage\\8QY6RYM9\\1409.html}
}

@article{takabaGeneralizedLyapunovTheorem1995,
  title = {A Generalized {{Lyapunov}} Theorem for Descriptor System},
  author = {Takaba, Kiyotsugu and Morihira, Naoki and Katayama, Tohru},
  year = {1995},
  month = jan,
  journal = {Systems \& Control Letters},
  volume = {24},
  number = {1},
  pages = {49--51},
  issn = {01676911},
  doi = {10.1016/0167-6911(94)00041-S},
  urldate = {2023-05-26},
  abstract = {We present a new generalized Lyapunov theorem for a continuous-time descriptor system by extending the theorem due to Lewis (1985). The result is applied to the infinite-horizon descriptor LQ regulator problem.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\KRTIGZWJ\Takaba 等 - 1995 - A generalized Lyapunov theorem for descriptor syst.pdf}
}

@misc{teamGeminiFamilyHighly2024,
  title = {Gemini: {{A Family}} of {{Highly Capable Multimodal Models}}},
  shorttitle = {Gemini},
  author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Krawczyk, Jack and Du, Cosmo and Chi, Ed and Cheng, Heng-Tze and Ni, Eric and Shah, Purvi and Kane, Patrick and Chan, Betty and Faruqui, Manaal and Severyn, Aliaksei and Lin, Hanzhao and Li, YaGuang and Cheng, Yong and Ittycheriah, Abe and Mahdieh, Mahdis and Chen, Mia and Sun, Pei and Tran, Dustin and Bagri, Sumit and Lakshminarayanan, Balaji and Liu, Jeremiah and Orban, Andras and G{\"u}ra, Fabian and Zhou, Hao and Song, Xinying and Boffy, Aurelien and Ganapathy, Harish and Zheng, Steven and Choe, HyunJeong and Weisz, {\'A}goston and Zhu, Tao and Lu, Yifeng and Gopal, Siddharth and Kahn, Jarrod and Kula, Maciej and Pitman, Jeff and Shah, Rushin and Taropa, Emanuel and Merey, Majd Al and Baeuml, Martin and Chen, Zhifeng and Shafey, Laurent El and Zhang, Yujing and Sercinoglu, Olcan and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Ana{\"i}s and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and {Barth-Maron}, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Tomar, Gaurav Singh and Senter, Evan and Chadwick, Martin and Kornakov, Ilya and Attaluri, Nithya and Iturrate, I{\~n}aki and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Garcia, Xavier and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adri{\`a} Puigdom{\`e}nech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Addanki, Ravi and Miech, Antoine and Louis, Annie and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, S{\'e}bastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozi{\'n}ska, Dominika and Nikolaev, Vitaliy and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Gim{\'e}nez, Mai and Yeung, Legg and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and {Castro-Ros}, Alex and van den Driessche, George and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lu{\v c}i{\'c}, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Rapha{\"e}l Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sj{\"o}sund, Lars Lowe and Cevey, S{\'e}bastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, L{\'e}onard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adri{\`a} and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, V{\'i}ctor Campos and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and {\"U}nl{\"u}, {\c C}a{\u g}lar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and {Gu-Lemberg}, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and {Cobon-Kerr}, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Raki{\'c}evi{\'c}, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and Cao, Nicola De and Chen, Charlie and Mudgal, Sidharth and Stella, Romina and Brooks, Kevin and Vasudevan, Gautam and Liu, Chenxi and Chain, Mainak and Melinkeri, Nivedita and Cohen, Aaron and Wang, Venus and Seymore, Kristie and Zubkov, Sergey and Goel, Rahul and Yue, Summer and Krishnakumaran, Sai and Albert, Brian and Hurley, Nate and Sano, Motoki and Mohananey, Anhad and Joughin, Jonah and Filonov, Egor and K{\k e}pa, Tomasz and Eldawy, Yomna and Lim, Jiawern and Rishi, Rahul and Badiezadegan, Shirin and Bos, Taylor and Chang, Jerry and Jain, Sanil and Padmanabhan, Sri Gayatri Sundara and Puttagunta, Subha and Krishna, Kalpesh and Baker, Leslie and Kalb, Norbert and Bedapudi, Vamsi and Kurzrok, Adam and Lei, Shuntong and Yu, Anthony and Litvin, Oren and Zhou, Xiang and Wu, Zhichun and Sobell, Sam and Siciliano, Andrea and Papir, Alan and Neale, Robby and Bragagnolo, Jonas and Toor, Tej and Chen, Tina and Anklin, Valentin and Wang, Feiran and Feng, Richie and Gholami, Milad and Ling, Kevin and Liu, Lijuan and Walter, Jules and Moghaddam, Hamid and Kishore, Arun and Adamek, Jakub and Mercado, Tyler and Mallinson, Jonathan and Wandekar, Siddhinita and Cagle, Stephen and Ofek, Eran and Garrido, Guillermo and Lombriser, Clemens and Mukha, Maksim and Sun, Botu and Mohammad, Hafeezul Rahman and Matak, Josip and Qian, Yadi and Peswani, Vikas and Janus, Pawel and Yuan, Quan and Schelin, Leif and David, Oana and Garg, Ankur and He, Yifan and Duzhyi, Oleksii and {\"A}lgmyr, Anton and Lottaz, Timoth{\'e}e and Li, Qi and Yadav, Vikas and Xu, Luyao and Chinien, Alex and Shivanna, Rakesh and Chuklin, Aleksandr and Li, Josie and Spadine, Carrie and Wolfe, Travis and Mohamed, Kareem and Das, Subhabrata and Dai, Zihang and He, Kyle and von Dincklage, Daniel and Upadhyay, Shyam and Maurya, Akanksha and Chi, Luyan and Krause, Sebastian and Salama, Khalid and Rabinovitch, Pam G. and M, Pavan Kumar Reddy and Selvan, Aarush and Dektiarev, Mikhail and Ghiasi, Golnaz and Guven, Erdem and Gupta, Himanshu and Liu, Boyi and Sharma, Deepak and Shtacher, Idan Heimlich and Paul, Shachi and Akerlund, Oscar and Aubet, Fran{\c c}ois-Xavier and Huang, Terry and Zhu, Chen and Zhu, Eric and Teixeira, Elico and Fritze, Matthew and Bertolini, Francesco and Marinescu, Liana-Eleonora and B{\"o}lle, Martin and Paulus, Dominik and Gupta, Khyatti and Latkar, Tejasi and Chang, Max and Sanders, Jason and Wilson, Roopa and Wu, Xuewei and Tan, Yi-Xuan and Thiet, Lam Nguyen and Doshi, Tulsee and Lall, Sid and Mishra, Swaroop and Chen, Wanming and Luong, Thang and Benjamin, Seth and Lee, Jasmine and Andrejczuk, Ewa and Rabiej, Dominik and Ranjan, Vipul and Styrc, Krzysztof and Yin, Pengcheng and Simon, Jon and Harriott, Malcolm Rose and Bansal, Mudit and Robsky, Alexei and Bacon, Geoff and Greene, David and Mirylenka, Daniil and Zhou, Chen and Sarvana, Obaid and Goyal, Abhimanyu and Andermatt, Samuel and Siegler, Patrick and Horn, Ben and Israel, Assaf and Pongetti, Francesco and Chen, Chih-Wei "Louis" and Selvatici, Marco and Silva, Pedro and Wang, Kathie and Tolins, Jackson and Guu, Kelvin and Yogev, Roey and Cai, Xiaochen and Agostini, Alessandro and Shah, Maulik and Nguyen, Hung and Donnaile, Noah {\'O} and Pereira, S{\'e}bastien and Friso, Linda and Stambler, Adam and Kurzrok, Adam and Kuang, Chenkai and Romanikhin, Yan and Geller, Mark and Yan, Z. J. and Jang, Kane and Lee, Cheng-Chun and Fica, Wojciech and Malmi, Eric and Tan, Qijun and Banica, Dan and Balle, Daniel and Pham, Ryan and Huang, Yanping and Avram, Diana and Shi, Hongzhi and Singh, Jasjot and Hidey, Chris and Ahuja, Niharika and Saxena, Pranab and Dooley, Dan and Potharaju, Srividya Pranavi and O'Neill, Eileen and Gokulchandran, Anand and Foley, Ryan and Zhao, Kai and Dusenberry, Mike and Liu, Yuan and Mehta, Pulkit and Kotikalapudi, Ragha and {Safranek-Shrader}, Chalence and Goodman, Andrew and Kessinger, Joshua and Globen, Eran and Kolhar, Prateek and Gorgolewski, Chris and Ibrahim, Ali and Song, Yang and Eichenbaum, Ali and Brovelli, Thomas and Potluri, Sahitya and Lahoti, Preethi and Baetu, Cip and Ghorbani, Ali and Chen, Charles and Crawford, Andy and Pal, Shalini and Sridhar, Mukund and Gurita, Petru and Mujika, Asier and Petrovski, Igor and Cedoz, Pierre-Louis and Li, Chenmei and Chen, Shiyuan and Santo, Niccol{\`o} Dal and Goyal, Siddharth and Punjabi, Jitesh and Kappaganthu, Karthik and Kwak, Chester and LV, Pallavi and Velury, Sarmishta and Choudhury, Himadri and Hall, Jamie and Shah, Premal and Figueira, Ricardo and Thomas, Matt and Lu, Minjie and Zhou, Ting and Kumar, Chintu and Jurdi, Thomas and Chikkerur, Sharat and Ma, Yenai and Yu, Adams and Kwak, Soo and {\"A}hdel, Victor and Rajayogam, Sujeevan and Choma, Travis and Liu, Fei and Barua, Aditya and Ji, Colin and Park, Ji Ho and Hellendoorn, Vincent and Bailey, Alex and Bilal, Taylan and Zhou, Huanjie and Khatir, Mehrdad and Sutton, Charles and Rzadkowski, Wojciech and Macintosh, Fiona and Shagin, Konstantin and Medina, Paul and Liang, Chen and Zhou, Jinjing and Shah, Pararth and Bi, Yingying and Dankovics, Attila and Banga, Shipra and Lehmann, Sabine and Bredesen, Marissa and Lin, Zifan and Hoffmann, John Eric and Lai, Jonathan and Chung, Raynald and Yang, Kai and Balani, Nihal and Bra{\v z}inskas, Arthur and Sozanschi, Andrei and Hayes, Matthew and Alcalde, H{\'e}ctor Fern{\'a}ndez and Makarov, Peter and Chen, Will and Stella, Antonio and Snijders, Liselotte and Mandl, Michael and K{\"a}rrman, Ante and Nowak, Pawe{\l} and Wu, Xinyi and Dyck, Alex and Vaidyanathan, Krishnan and R, Raghavender and Mallet, Jessica and Rudominer, Mitch and Johnston, Eric and Mittal, Sushil and Udathu, Akhil and Christensen, Janara and Verma, Vishal and Irving, Zach and Santucci, Andreas and Elsayed, Gamaleldin and Davoodi, Elnaz and Georgiev, Marin and Tenney, Ian and Hua, Nan and Cideron, Geoffrey and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Scandinaro, Dylan and Jiang, Heinrich and Snoek, Jasper and Sundararajan, Mukund and Wang, Xuezhi and Ontiveros, Zack and Karo, Itay and Cole, Jeremy and Rajashekhar, Vinu and Tumeh, Lara and {Ben-David}, Eyal and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Bunyan, Oskar and Wu, Shimu and Zhang, John and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Park, Jane and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Irving, Geoffrey and Loper, Edward and Fink, Michael and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Petrychenko, Ivan and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Palmer, Evan and Suganthan, Paul and Casta{\~n}o, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybi{\'n}ski, Miko{\l}aj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Perng, Ginger and Abellan, Elena Allica and Zhang, Mingyang and Dasgupta, Ishita and Kushman, Nate and Penchev, Ivo and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Andor, Daniel and Valenzuela, Pedro and Lui, Minnie and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Franko, Ken and Bulanova, Anna and Leblond, R{\'e}mi and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Omernick, Mark and Bishop, Colton and Sterneck, Rachel and Jain, Rohan and Xia, Jiawei and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Mankowitz, Daniel J. and Polozov, Alex and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Geist, Matthieu and tan Girgin, Ser and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and {Lee-Thorp}, James and Yew, Christopher and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Wu, Kathy and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Wu, Diane and {Owusu-Afriyie}, Denese and Du, Cosmo and Thornton, Chloe and {Pont-Tuset}, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Saaber and Wieting, John and Ajmeri, Omar and Uria, Benigno and Ko, Yeongil and Knight, Laura and H{\'e}liou, Am{\'e}lie and Niu, Ning and Gu, Shane and Pang, Chenxi and Li, Yeqing and Levine, Nir and Stolovich, Ariel and {Santamaria-Fernandez}, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Deck, Charlie and Lee, Hyo and Li, Zonglin and Levin, Kyle and Hoffmann, Raphael and {Holtmann-Rice}, Dan and Bachem, Olivier and Arora, Sho and Koh, Christy and Yeganeh, Soheil Hassas and P{\~o}der, Siim and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Koverkathu, Vinod and {Choquette-Choo}, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivi{\`e}re, Morgane and Walton, Alanna and Crepy, Cl{\'e}ment and Parrish, Alicia and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and {Latorre-Chimoto}, Eri and {Klimczak-Pluci{\'n}ska}, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Webb, Lynette and Dua, Sahil and Li, Dong and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Eltyshev, Evgenii and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Angermueller, Christof and Li, Xiaowei and Sinha, Anoop and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Zhou, Denny and Jalan, Komal and Li, Dinghua and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Milan, Kieran and Mikulik, Vladimir and Franco, Juliana and Green, Tim and Nguyen, Nam and Kelley, Joe and Mahendru, Aroma and Hu, Andrea and Howland, Joshua and Vargas, Ben and Hui, Jeffrey and Bansal, Kshitij and Rao, Vikram and Ghiya, Rakesh and Wang, Emma and Ye, Ke and Sarr, Jean Michel and Preston, Melanie Moranski and Elish, Madeleine and Li, Steve and Kaku, Aakash and Gupta, Jigar and Pasupat, Ice and Juan, Da-Cheng and Someswar, Milan and M, Tejvi and Chen, Xinyun and Amini, Aida and Fabrikant, Alex and Chu, Eric and Dong, Xuanyi and Muthal, Amruta and Buthpitiya, Senaka and Jauhari, Sarthak and Hua, Nan and Khandelwal, Urvashi and Hitron, Ayal and Ren, Jie and Rinaldi, Larissa and Drath, Shahar and Dabush, Avigail and Jiang, Nan-Jiang and Godhia, Harshal and Sachs, Uli and Chen, Anthony and Fan, Yicheng and Taitelbaum, Hagai and Noga, Hila and Dai, Zhuyun and Wang, James and Liang, Chen and Hamer, Jenny and Ferng, Chun-Sung and Elkind, Chenel and Atias, Aviel and Lee, Paulina and List{\'i}k, V{\'i}t and Carlen, Mathias and van de Kerkhof, Jan and Pikus, Marcin and Zaher, Krunoslav and M{\"u}ller, Paul and Zykova, Sasha and Stefanec, Richard and Gatsko, Vitaly and Hirnschall, Christoph and Sethi, Ashwin and Xu, Xingyu Federico and Ahuja, Chetan and Tsai, Beth and Stefanoiu, Anca and Feng, Bo and Dhandhania, Keshav and Katyal, Manish and Gupta, Akshay and Parulekar, Atharva and Pitta, Divya and Zhao, Jing and Bhatia, Vivaan and Bhavnani, Yashodha and Alhadlaq, Omar and Li, Xiaolin and Danenberg, Peter and Tu, Dennis and Pine, Alex and Filippova, Vera and Ghosh, Abhipso and Limonchik, Ben and Urala, Bhargava and Lanka, Chaitanya Krishna and Clive, Derik and Sun, Yi and Li, Edward and Wu, Hao and Hongtongsak, Kevin and Li, Ianna and Thakkar, Kalind and Omarov, Kuanysh and Majmundar, Kushal and Alverson, Michael and Kucharski, Michael and Patel, Mohak and Jain, Mudit and Zabelin, Maksim and Pelagatti, Paolo and Kohli, Rohan and Kumar, Saurabh and Kim, Joseph and Sankar, Swetha and Shah, Vineet and Ramachandruni, Lakshmi and Zeng, Xiangkai and Bariach, Ben and Weidinger, Laura and Vu, Tu and Andreev, Alek and He, Antoine and Hui, Kevin and Kashem, Sheleem and Subramanya, Amar and Hsiao, Sissie and Hassabis, Demis and Kavukcuoglu, Koray and Sadovsky, Adam and Le, Quoc and Strohman, Trevor and Wu, Yonghui and Petrov, Slav and Dean, Jeffrey and Vinyals, Oriol},
  year = {2024},
  month = jun,
  number = {arXiv:2312.11805},
  eprint = {2312.11805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11805},
  urldate = {2024-12-21},
  abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\AA\\Zotero\\storage\\6JAL9BBL\\Team 等 - 2024 - Gemini A Family of Highly Capable Multimodal Mode.pdf;C\:\\Users\\AA\\Zotero\\storage\\BNRDYSUW\\2312.html}
}

@misc{thorpeDeepLimitsResidual2022,
  title = {Deep {{Limits}} of {{Residual Neural Networks}}},
  author = {Thorpe, Matthew and {van Gennip}, Yves},
  year = {2022},
  month = nov,
  number = {arXiv:1810.11741},
  eprint = {1810.11741},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2023-04-12},
  abstract = {Neural networks have been very successful in many applications; we often, however, lack a theoretical understanding of what the neural networks are actually learning. This problem emerges when trying to generalise to new data sets. The contribution of this paper is to show that, for the residual neural network model, the deep layer limit coincides with a parameter estimation problem for a nonlinear ordinary differential equation. In particular, whilst it is known that the residual neural network model is a discretisation of an ordinary differential equation, we show convergence in a variational sense. This implies that optimal parameters converge in the deep layer limit. This is a stronger statement than saying for a fixed parameter the residual neural network model converges (the latter does not in general imply the former). Our variational analysis provides a discrete-tocontinuum {$\Gamma$}-convergence result for the objective function of the residual neural network training step to a variational problem constrained by a system of ordinary differential equations; this rigorously connects the discrete setting to a continuum problem.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\7U4HUU9X\Thorpe 和 van Gennip - 2022 - Deep Limits of Residual Neural Networks.pdf}
}

@inproceedings{tianDeepTestAutomatedTesting2018,
  title = {{{DeepTest}}: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars},
  shorttitle = {{{DeepTest}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}},
  author = {Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi},
  year = {2018},
  month = may,
  pages = {303--314},
  publisher = {ACM},
  address = {Gothenburg Sweden},
  doi = {10.1145/3180155.3180220},
  urldate = {2025-02-19},
  isbn = {978-1-4503-5638-1},
  langid = {english},
  file = {C\:\\Users\\AA\\Zotero\\storage\\J7BVHIPV\\tian2018.pdf.pdf;C\:\\Users\\AA\\Zotero\\storage\\JZAPN9WD\\Tian 等 - 2018 - DeepTest automated testing of deep-neural-network.pdf}
}

@incollection{tianDetectingTextNatural2016,
  title = {Detecting {{Text}} in {{Natural Image}} with {{Connectionist Text Proposal Network}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2016},
  author = {Tian, Zhi and Huang, Weilin and He, Tong and He, Pan and Qiao, Yu},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  volume = {9912},
  pages = {56--72},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46484-8_4},
  urldate = {2025-02-20},
  isbn = {978-3-319-46483-1 978-3-319-46484-8},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\HJ6UKPWT\Tian 等 - 2016 - Detecting Text in Natural Image with Connectionist.pdf}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-13},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\UFGPLJDR\\Touvron 等 - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf;C\:\\Users\\AA\\Zotero\\storage\\FA794V45\\2302.html}
}

@article{tuDynamicModeDecomposition2014,
  title = {On {{Dynamic Mode Decomposition}}: {{Theory}} and {{Applications}}},
  shorttitle = {On {{Dynamic Mode Decomposition}}},
  author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
  year = {2014},
  journal = {Journal of Computational Dynamics},
  volume = {1},
  number = {2},
  eprint = {1312.0041},
  primaryclass = {physics},
  pages = {391--421},
  issn = {2158-2505},
  doi = {10.3934/jcd.2014.1.391},
  urldate = {2023-05-26},
  abstract = {Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken. We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator. This generalizes DMD to a larger class of datasets, including nonsequential time series. We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational efficiency and mitigate the effects of noise, respectively. We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples. Such computations are not considered in the existing literature, but can be understood using our more general framework. In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory. It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science. We show that under certain conditions, DMD is equivalent to LIM.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\7JWSE9GE\Tu 等 - 2014 - On Dynamic Mode Decomposition Theory and Applicat.pdf}
}

@misc{varonaPracticableSimulationFreeModel2019,
  title = {Practicable {{Simulation-Free Model Order Reduction}} by {{Nonlinear Moment Matching}}},
  author = {Varona, Maria Cruz and Gebhart, Raphael and Suk, Julian and Lohmann, Boris},
  year = {2019},
  month = jan,
  number = {arXiv:1901.10750},
  eprint = {1901.10750},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {In this paper, a practicable simulation-free model order reduction method by nonlinear moment matching is developed. Based on the steady-state interpretation of linear moment matching, we comprehensively explain the extension of this reduction concept to nonlinear systems presented in [1], provide some new insights and propose some simplifications to achieve a feasible and numerically efficient nonlinear model reduction algorithm. This algorithm relies on the solution of nonlinear systems of equations rather than on the expensive simulation of the original model or the difficult solution of a nonlinear partial differential equation.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\MEZSCH8Q\Varona 等 - 2019 - Practicable Simulation-Free Model Order Reduction .pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-05-08},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {C:\Users\AA\Zotero\storage\KAITBKNJ\Vaswani et al_2017_Attention is All you Need.pdf}
}

@article{vonengelhardtSexspecificEffectsYolk2006,
  title = {Sex-Specific Effects of Yolk Testosterone on Survival, Begging and Growth of Zebra Finches},
  author = {Von Engelhardt, Nikolaus and Carere, Claudio and Dijkstra, Cor and GG Groothuis, Ton},
  year = {2006},
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {273},
  number = {1582},
  pages = {65--70},
  publisher = {The Royal Society London},
  doi = {10.1098/rspb.2005.3274},
  file = {C:\Users\AA\Zotero\storage\S8PUSM88\PMC1560008.html}
}

@misc{wallaceUniversalAdversarialTriggers2021,
  title = {Universal {{Adversarial Triggers}} for {{Attacking}} and {{Analyzing NLP}}},
  author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  year = {2021},
  month = jan,
  number = {arXiv:1908.07125},
  eprint = {1908.07125},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of "why" questions in SQuAD to be answered "to kill american people", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\4FTIM45H\\Wallace 等 - 2021 - Universal Adversarial Triggers for Attacking and A.pdf;C\:\\Users\\AA\\Zotero\\storage\\35P6Y9G3\\1908.html}
}

@misc{wangChainofTableEvolvingTables2024,
  title = {Chain-of-{{Table}}: {{Evolving Tables}} in the {{Reasoning Chain}} for {{Table Understanding}}},
  shorttitle = {Chain-of-{{Table}}},
  author = {Wang, Zilong and Zhang, Hao and Li, Chun-Liang and Eisenschlos, Julian Martin and Perot, Vincent and Wang, Zifeng and Miculicich, Lesly and Fujii, Yasuhisa and Shang, Jingbo and Lee, Chen-Yu and Pfister, Tomas},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04398},
  eprint = {2401.04398},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-18},
  abstract = {Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\4XNS2BCD\\Wang et al_2024_Chain-of-Table.pdf;C\:\\Users\\AA\\Zotero\\storage\\VMTRT8DL\\2401.html}
}

@inproceedings{wangDistXploreDistributionGuidedTesting2023,
  title = {{{DistXplore}}: {{Distribution-Guided Testing}} for {{Evaluating}} and {{Enhancing Deep Learning Systems}}},
  shorttitle = {{{DistXplore}}},
  booktitle = {Proceedings of the 31st {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Wang, Longtian and Xie, Xiaofei and Du, Xiaoning and Tian, Meng and Guo, Qing and Yang, Zheng and Shen, Chao},
  year = {2023},
  month = nov,
  pages = {68--80},
  publisher = {ACM},
  address = {San Francisco CA USA},
  doi = {10.1145/3611643.3616266},
  urldate = {2025-02-19},
  isbn = {9798400703270},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\XVP5QFJZ\Wang 等 - 2023 - DistXplore Distribution-Guided Testing for Evalua.pdf}
}

@misc{wangDriveDreamerRealworlddrivenWorld2023,
  title = {{{DriveDreamer}}: {{Towards Real-world-driven World Models}} for {{Autonomous Driving}}},
  shorttitle = {{{DriveDreamer}}},
  author = {Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Chen, Xinze and Lu, Jiwen},
  year = {2023},
  month = sep,
  number = {arXiv:2309.09777},
  eprint = {2309.09777},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-17},
  abstract = {World models, especially in autonomous driving, are trending and drawing extensive attention due to their capacity for comprehending driving environments. The established world model holds immense potential for the generation of high-quality driving videos, and driving policies for safe maneuvering. However, a critical limitation in relevant research lies in its predominant focus on gaming environments or simulated settings, thereby lacking the representation of real-world driving scenarios. Therefore, we introduce DriveDreamer, a pioneering world model entirely derived from real-world driving scenarios. Regarding that modeling the world in intricate driving scenes entails an overwhelming search space, we propose harnessing the powerful diffusion model to construct a comprehensive representation of the complex environment. Furthermore, we introduce a two-stage training pipeline. In the initial phase, DriveDreamer acquires a deep understanding of structured traffic constraints, while the subsequent stage equips it with the ability to anticipate future states. The proposed DriveDreamer is the first world model established from real-world driving scenarios. We instantiate DriveDreamer on the challenging nuScenes benchmark, and extensive experiments verify that DriveDreamer empowers precise, controllable video generation that faithfully captures the structural constraints of real-world traffic scenarios. Additionally, DriveDreamer enables the generation of realistic and reasonable driving policies, opening avenues for interaction and practical applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\AA\\Zotero\\storage\\MK2L8FBG\\Wang 等 - 2023 - DriveDreamer Towards Real-world-driven World Mode.pdf;C\:\\Users\\AA\\Zotero\\storage\\Y2T5PBNL\\2309.html}
}

@misc{wangInteractiveNaturalLanguage2023,
  title = {Interactive {{Natural Language Processing}}},
  author = {Wang, Zekun and Zhang, Ge and Yang, Kexin and Shi, Ning and Zhou, Wangchunshu and Hao, Shaochun and Xiong, Guangzheng and Li, Yizhi and Sim, Mong Yuan and Chen, Xiuying and Zhu, Qingqing and Yang, Zhenzhu and Nik, Adam and Liu, Qi and Lin, Chenghua and Wang, Shi and Liu, Ruibo and Chen, Wenhu and Xu, Ke and Liu, Dayiheng and Guo, Yike and Fu, Jie},
  year = {2023},
  month = may,
  number = {arXiv:2305.13246},
  eprint = {2305.13246},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Interactive Natural Language Processing (iNLP) has emerged as a novel paradigm within the field of NLP, aimed at addressing limitations in existing frameworks while aligning with the ultimate goals of artificial intelligence. This paradigm considers language models as agents capable of observing, acting, and receiving feedback iteratively from external entities. Specifically, language models in this context can: (1) interact with humans for better understanding and addressing user needs, personalizing responses, aligning with human values, and improving the overall user experience; (2) interact with knowledge bases for enriching language representations with factual knowledge, enhancing the contextual relevance of responses, and dynamically leveraging external information to generate more accurate and informed responses; (3) interact with models and tools for effectively decomposing and addressing complex tasks, leveraging specialized expertise for specific subtasks, and fostering the simulation of social behaviors; and (4) interact with environments for learning grounded representations of language, and effectively tackling embodied tasks such as reasoning, planning, and decision-making in response to environmental observations. This paper offers a comprehensive survey of iNLP, starting by proposing a unified definition and framework of the concept. We then provide a systematic classification of iNLP, dissecting its various components, including interactive objects, interaction interfaces, and interaction methods. We proceed to delve into the evaluation methodologies used in the field, explore its diverse applications, scrutinize its ethical and safety issues, and discuss prospective research directions. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\EUEV2JL4\\Wang 等 - 2023 - Interactive Natural Language Processing.pdf;C\:\\Users\\AA\\Zotero\\storage\\33JEJ64H\\2305.html}
}

@article{wangKnowledgeGraphPrompting2023,
  title = {Knowledge Graph Prompting for Multi-Document Question Answering},
  author = {Wang, Yu and Lipka, Nedim and Rossi, Ryan A. and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler},
  year = {2023},
  journal = {arXiv preprint arXiv:2308.11730},
  eprint = {2308.11730},
  urldate = {2024-03-26},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\B5WR9QJH\Wang et al_2023_Knowledge graph prompting for multi-document question answering.pdf}
}

@misc{wangKnowledgeGraphPrompting2023a,
  title = {Knowledge {{Graph Prompting}} for {{Multi-Document Question Answering}}},
  author = {Wang, Yu and Lipka, Nedim and Rossi, Ryan A. and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler},
  year = {2023},
  month = dec,
  number = {arXiv:2308.11730},
  eprint = {2308.11730},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-26},
  abstract = {The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in opendomain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLMbased graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\RNZQAZ62\Wang 等 - 2023 - Knowledge Graph Prompting for Multi-Document Quest.pdf}
}

@misc{wangKnowledGPTEnhancingLarge2023,
  title = {{{KnowledGPT}}: {{Enhancing Large Language Models}} with {{Retrieval}} and {{Storage Access}} on {{Knowledge Bases}}},
  shorttitle = {{{KnowledGPT}}},
  author = {Wang, Xintao and Yang, Qianwen and Qiu, Yongting and Liang, Jiaqing and He, Qianyu and Gu, Zhouhong and Xiao, Yanghua and Wang, Wei},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11761},
  eprint = {2308.11761},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.11761},
  urldate = {2024-12-21},
  abstract = {Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LUGHL35U\\Wang 等 - 2023 - KnowledGPT Enhancing Large Language Models with R.pdf;C\:\\Users\\AA\\Zotero\\storage\\VBB69ME5\\2308.html}
}

@misc{wangSurveyLargeLanguage2023,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2023},
  month = sep,
  number = {arXiv:2308.11432},
  eprint = {2308.11432},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\SKRA6LPU\\Wang 等 - 2023 - A Survey on Large Language Model based Autonomous .pdf;C\:\\Users\\AA\\Zotero\\storage\\CR5ZN4W7\\2308.html}
}

@article{weiChainofthoughtPromptingElicits2022,
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24824--24837},
  urldate = {2023-11-30},
  file = {C:\Users\AA\Zotero\storage\JNNSQ4HR\Wei 等 - 2022 - Chain-of-thought prompting elicits reasoning in la.pdf}
}

@misc{weiFinetunedLanguageModels2022,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2022},
  month = feb,
  number = {arXiv:2109.01652},
  eprint = {2109.01652},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-07},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\GFC8UFYG\\Wei 等 - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf;C\:\\Users\\AA\\Zotero\\storage\\JKTHH7VB\\2109.html}
}

@misc{wenLearningIntrinsicSparse2018,
  title = {Learning {{Intrinsic Sparse Structures}} within {{Long Short-Term Memory}}},
  author = {Wen, Wei and He, Yuxiong and Rajbhandari, Samyam and Zhang, Minjia and Wang, Wenhan and Liu, Fang and Hu, Bin and Chen, Yiran and Li, Hai},
  year = {2018},
  month = feb,
  number = {arXiv:1709.05027},
  eprint = {1709.05027},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59{\texttimes} speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to nonLSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is available1.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\BDJ35KE8\Wen 等 - 2018 - Learning Intrinsic Sparse Structures within Long S.pdf}
}

@misc{wenRoadGPT4VisionEarly2023,
  title = {On the {{Road}} with {{GPT-4V}}(Ision): {{Early Explorations}} of {{Visual-Language Model}} on {{Autonomous Driving}}},
  shorttitle = {On the {{Road}} with {{GPT-4V}}(Ision)},
  author = {Wen, Licheng and Yang, Xuemeng and Fu, Daocheng and Wang, Xiaofeng and Cai, Pinlong and Li, Xin and Ma, Tao and Li, Yingxuan and Xu, Linran and Shang, Dengke and Zhu, Zheng and Sun, Shaoyan and Bai, Yeqi and Cai, Xinyu and Dou, Min and Hu, Shuanglu and Shi, Botian},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05332},
  eprint = {2311.05332},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-15},
  abstract = {The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, {\textbackslash}modelnamefull, and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that {\textbackslash}modelname demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: {\textbackslash}url\{https://github.com/PJLab-ADG/GPT4V-AD-Exploration\}},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\AA\Zotero\storage\MAMZJ9CU\Wen 等 - 2023 - On the Road with GPT-4V(ision) Early Explorations.pdf}
}

@misc{wuAutoGenEnablingNextGen2023,
  title = {{{AutoGen}}: {{Enabling Next-Gen LLM Applications}} via {{Multi-Agent Conversation}}},
  shorttitle = {{{AutoGen}}},
  author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
  year = {2023},
  month = oct,
  number = {arXiv:2308.08155},
  eprint = {2308.08155},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-14},
  abstract = {AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\X6CEQ34Y\\Wu 等 - 2023 - AutoGen Enabling Next-Gen LLM Applications via Mu.pdf;C\:\\Users\\AA\\Zotero\\storage\\LUMV5WLK\\2308.html}
}

@misc{wuLiteTransformerLongShort2020,
  title = {Lite {{Transformer}} with {{Long-Short Range Attention}}},
  author = {Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  year = {2020},
  month = apr,
  number = {arXiv:2004.11886},
  eprint = {2004.11886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.11886},
  urldate = {2023-04-17},
  abstract = {Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\L4TK9UU6\\Wu et al_2020_Lite Transformer with Long-Short Range Attention.pdf;C\:\\Users\\AA\\Zotero\\storage\\LTUC97LD\\2004.html}
}

@article{xiaAutonomousSystemFlexible2023,
  title = {Towards Autonomous System: Flexible Modular Production System Enhanced with Large Language Model Agents},
  shorttitle = {Towards Autonomous System},
  author = {Xia, Yuchen and Shenoy, Manthan and Jazdi, Nasser and Weyrich, Michael},
  year = {2023},
  journal = {arXiv preprint arXiv:2304.14721},
  eprint = {2304.14721},
  urldate = {2024-01-13},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\CK832WRH\Xia 等 - 2023 - Towards autonomous system flexible modular produc.pdf}
}

@article{xiRisePotentialLarge2023,
  title = {The Rise and Potential of Large Language Model Based Agents: {{A}} Survey},
  shorttitle = {The Rise and Potential of Large Language Model Based Agents},
  author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu},
  year = {2023},
  journal = {arXiv preprint arXiv:2309.07864},
  eprint = {2309.07864},
  urldate = {2024-01-13},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\F8JVRQZW\Xi et al_2023_The rise and potential of large language model based agents.pdf}
}

@misc{xiRisePotentialLarge2023a,
  title = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}: {{A Survey}}},
  shorttitle = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}},
  author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
  year = {2023},
  month = sep,
  number = {arXiv:2309.07864},
  eprint = {2309.07864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\SXM2RVZT\\Xi 等 - 2023 - The Rise and Potential of Large Language Model Bas.pdf;C\:\\Users\\AA\\Zotero\\storage\\BGNJN36T\\2309.html}
}

@article{xuDriveGPT4InterpretableEndtoend2023,
  title = {{{DriveGPT4}}: {{Interpretable End-to-end Autonomous Driving}} via {{Large Language Model}}},
  shorttitle = {{{DriveGPT4}}},
  author = {Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kenneth KY and Li, Zhenguo and Zhao, Hengshuang},
  year = {2023},
  journal = {arXiv preprint arXiv:2310.01412},
  eprint = {2310.01412},
  urldate = {2023-11-10},
  archiveprefix = {arXiv},
  keywords = {No DOI found},
  file = {C:\Users\AA\Zotero\storage\PVE2H9ZQ\Xu 等 - 2023 - DriveGPT4 Interpretable End-to-end Autonomous Dri.pdf}
}

@article{xueRestructuringDeepNeural,
  title = {Restructuring of {{Deep Neural Network Acoustic Models}} with {{Singular Value Decomposition}}},
  author = {Xue, Jian and Li, Jinyu and Gong, Yifan},
  doi = {10.21437/Interspeech.2013-552},
  abstract = {Recently proposed deep neural network (DNN) obtains significant accuracy improvements in many large vocabulary continuous speech recognition (LVCSR) tasks. However, DNN requires much more parameters than traditional systems, which brings huge cost during online evaluation, and also limits the application of DNN in a lot of scenarios. In this paper we present our new effort on DNN aiming at reducing the model size while keeping the accuracy improvements. We apply singular value decomposition (SVD) on the weight matrices in DNN, and then restructure the model based on the inherent sparseness of the original matrices. After restructuring we can reduce the DNN model size significantly with negligible accuracy loss. We also fine-tune the restructured model using the regular back-propagation method to get the accuracy back when reducing the DNN model size heavily. The proposed method has been evaluated on two LVCSR tasks, with context-dependent DNN hidden Markov model (CD-DNN-HMM). Experimental results show that the proposed approach dramatically reduces the DNN model size by more than 80\% without losing any accuracy.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\RX3RVQVP\Xue 等 - Restructuring of Deep Neural Network Acoustic Mode.pdf}
}

@inproceedings{xueSequenceModelCompression2023,
  title = {A {{Sequence Model Compression Method Based}} on {{Proper Orthogonal Decomposition}}},
  booktitle = {2023 3rd {{International Conference}} on {{Computer Science}} and {{Blockchain}} ({{CCSB}})},
  author = {Xue, Jianing and Wu, Dakui and Liu, Yajun},
  year = {2023},
  month = nov,
  pages = {1--4},
  doi = {10.1109/CCSB60789.2023.10398824},
  urldate = {2024-02-03},
  abstract = {We propose a method that can compress the parameters and floating point operations (FLOPs) of a Long Short-Term Memory (LSTM) model to a third of the original model and get an acceptable error without fine-tuning. The LSTM can be treated as a discrete nonlinear dynamical system and use a kind of model order reduction method which is named proper orthogonal decomposition (POD) to deal with it. Approximating the hidden state reduces the dimensionality of the hidden state. We design a small LSTM model that can be considered as a time series classification model and a public dataset to validate the method. The number of parameters of the original model can be compressed to 34.4\% and the FLOPs can be reduced to 25.6\% with a 5\% loss in accuracy.},
  file = {C:\Users\AA\Zotero\storage\YUPJLHI5\10398824.html}
}

@inproceedings{xueSingularValueDecomposition2014,
  title = {Singular Value Decomposition Based Low-Footprint Speaker Adaptation and Personalization for Deep Neural Network},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Xue, Jian and Li, Jinyu and Yu, Dong and Seltzer, Mike and Gong, Yifan},
  year = {2014},
  month = may,
  pages = {6359--6363},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2014.6854828},
  abstract = {The large number of parameters in deep neural networks (DNN) for automatic speech recognition (ASR) makes speaker adaptation very challenging. It also limits the use of speaker personalization due to the huge storage cost in large-scale deployments. In this paper we address DNN adaptation and personalization issues by presenting two methods based on the singular value decomposition (SVD). The first method uses an SVD to replace the weight matrix of a speaker independent DNN by the product of two low rank matrices. Adaptation is then performed by updating a square matrix inserted between the two low-rank matrices. In the second method, we adapt the full weight matrix but only store the delta matrix - the difference between the original and adapted weight matrices. We decrease the footprint of the adapted model by storing a reduced rank version of the delta matrix via an SVD. The proposed methods were evaluated on short message dictation task. Experimental results show that we can obtain similar accuracy improvements as the previously proposed Kullback-Leibler divergence (KLD) regularized method with far fewer parameters, which only requires 0.89\% of the original model storage.}
}

@misc{xuRetrievalMeetsLong2024,
  title = {Retrieval Meets {{Long Context Large Language Models}}},
  author = {Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
  year = {2024},
  month = jan,
  number = {arXiv:2310.03025},
  eprint = {2310.03025},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03025},
  urldate = {2024-12-20},
  abstract = {Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\66BSF7BZ\\Xu 等 - 2024 - Retrieval meets Long Context Large Language Models.pdf;C\:\\Users\\AA\\Zotero\\storage\\DW5HEPZE\\2310.html}
}

@misc{xuSurveyModelCompression2022,
  title = {A {{Survey}} on {{Model Compression}} and {{Acceleration}} for {{Pretrained Language Models}}},
  author = {Xu, Canwen and McAuley, Julian},
  year = {2022},
  month = nov,
  number = {arXiv:2202.07105},
  eprint = {2202.07105},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.07105},
  urldate = {2023-06-16},
  abstract = {Despite achieving state-of-the-art performance on many NLP tasks, the high energy cost and long inference delay prevent Transformer-based pretrained language models (PLMs) from seeing broader adoption including for edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression and acceleration for pretrained language models, including benchmarks, metrics and methodology.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\DPFKKVZT\\Xu_McAuley_2022_A Survey on Model Compression and Acceleration for Pretrained Language Models.pdf;C\:\\Users\\AA\\Zotero\\storage\\ZDM8BDIA\\2202.html}
}

@misc{xuWizardLMEmpoweringLarge2023,
  title = {{{WizardLM}}: {{Empowering Large Language Models}} to {{Follow Complex Instructions}}},
  shorttitle = {{{WizardLM}}},
  author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  year = {2023},
  month = jun,
  number = {arXiv:2304.12244},
  eprint = {2304.12244},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90{\textbackslash}\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\G2QPNRPE\\Xu 等 - 2023 - WizardLM Empowering Large Language Models to Foll.pdf;C\:\\Users\\AA\\Zotero\\storage\\9Z8DDKIA\\2304.html}
}

@misc{yangBEVControlAccuratelyControlling2023,
  title = {{{BEVControl}}: {{Accurately Controlling Street-view Elements}} with {{Multi-perspective Consistency}} via {{BEV Sketch Layout}}},
  shorttitle = {{{BEVControl}}},
  author = {Yang, Kairui and Ma, Enhui and Peng, Jibin and Guo, Qing and Lin, Di and Yu, Kaicheng},
  year = {2023},
  month = sep,
  number = {arXiv:2308.01661},
  eprint = {2308.01661},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-17},
  abstract = {Using synthesized images to boost the performance of perception models is a long-standing research challenge in computer vision. It becomes more eminent in visual-centric autonomous driving systems with multi-view cameras as some long-tail scenarios can never be collected. Guided by the BEV segmentation layouts, the existing generative networks seem to synthesize photo-realistic street-view images when evaluated solely on scene-level metrics. However, once zoom-in, they usually fail to produce accurate foreground and background details such as heading. To this end, we propose a two-stage generative method, dubbed BEVControl, that can generate accurate foreground and background contents. In contrast to segmentation-like input, it also supports sketch style input, which is more flexible for humans to edit. In addition, we propose a comprehensive multi-level evaluation protocol to fairly compare the quality of the generated scene, foreground object, and background geometry. Our extensive experiments show that our BEVControl surpasses the state-of-the-art method, BEVGen, by a significant margin, from 5.89 to 26.80 on foreground segmentation mIoU. In addition, we show that using images generated by BEVControl to train the downstream perception model, it achieves on average 1.29 improvement in NDS score.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\AA\\Zotero\\storage\\JHW68JNR\\Yang 等 - 2023 - BEVControl Accurately Controlling Street-view Ele.pdf;C\:\\Users\\AA\\Zotero\\storage\\TIZL8TNY\\2308.html}
}

@article{yangChat3DInteractive2023,
  title = {Chat {{3D}}: {{Interactive 3D Reconstruction With Assistance}} of {{Large Language Model}}},
  author = {Yang, Jingqi and Shen, Qi and Xie, Cheng},
  year = {2023},
  month = sep,
  journal = {Image and Vision Computing},
  volume = {137},
  pages = {104758},
  issn = {02628856},
  doi = {10.1016/j.imavis.2023.104758},
  urldate = {2023-11-16},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\4K44B5VR\Yang 等 - 2023 - Generation-based contrastive model with semantic a.pdf}
}

@misc{yangSurveyLargeLanguage2023,
  title = {A {{Survey}} of {{Large Language Models}} for {{Autonomous Driving}}},
  author = {Yang, Zhenjie and Jia, Xiaosong and Li, Hongyang and Yan, Junchi},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01043},
  eprint = {2311.01043},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-15},
  abstract = {Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional modulebased systems are constrained by cumulative errors among cascaded modules and inflexible preset rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their ``black box'' nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about Large Language Models for Autonomous Driving (LLM4AD). This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github. com/Thinklab-SJTU/Awesome-LLM4AD.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C:\Users\AA\Zotero\storage\HRF8ZJUF\Yang 等 - 2023 - A Survey of Large Language Models for Autonomous D.pdf}
}

@inproceedings{yaoDetectingTextsArbitrary2012,
  title = {Detecting Texts of Arbitrary Orientations in Natural Images},
  booktitle = {2012 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Yao, Cong and Bai, Xiang and Liu, Wenyu and Ma, Yi and Tu, Zhuowen},
  year = {2012},
  pages = {1083--1090},
  publisher = {IEEE},
  urldate = {2025-02-20},
  file = {C:\Users\AA\Zotero\storage\M4CP5H45\Yao 等 - 2012 - Detecting texts of arbitrary orientations in natur.pdf}
}

@misc{yaoEditingLargeLanguage2023,
  title = {Editing {{Large Language Models}}: {{Problems}}, {{Methods}}, and {{Opportunities}}},
  shorttitle = {Editing {{Large Language Models}}},
  author = {Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  year = {2023},
  month = nov,
  number = {arXiv:2305.13172},
  eprint = {2305.13172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13172},
  urldate = {2024-12-21},
  abstract = {Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at https://github.com/zjunlp/EasyEdit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\LNGT4JC9\\Yao 等 - 2023 - Editing Large Language Models Problems, Methods, .pdf;C\:\\Users\\AA\\Zotero\\storage\\GEVIXKBU\\2305.html}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-03},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\3DZC3R5X\Yao 等 - 2023 - ReAct Synergizing Reasoning and Acting in Languag.pdf}
}

@misc{yeLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Versatile Decomposers}}: {{Decompose Evidence}} and {{Questions}} for {{Table-based Reasoning}}},
  shorttitle = {Large {{Language Models}} Are {{Versatile Decomposers}}},
  author = {Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
  year = {2023},
  month = apr,
  number = {arXiv:2301.13808},
  eprint = {2301.13808},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-20},
  abstract = {Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a "parsing-execution-filling" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\CX2RYVKS\\Ye et al_2023_Large Language Models are Versatile Decomposers.pdf;C\:\\Users\\AA\\Zotero\\storage\\GTGNS4WL\\2301.html}
}

@inproceedings{yihValueSemanticParse2016,
  title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Yih, Wen-tau and Richardson, Matthew and Meek, Christopher and Chang, Ming-Wei and Suh, Jina},
  year = {2016},
  pages = {201--206},
  urldate = {2024-12-18},
  file = {C:\Users\AA\Zotero\storage\GRHQNH3Q\Yih 等 - 2016 - The value of semantic parse labeling for knowledge.pdf}
}

@inproceedings{yimGiftKnowledgeDistillation2017,
  title = {A {{Gift}} from {{Knowledge Distillation}}: {{Fast Optimization}}, {{Network Minimization}} and {{Transfer Learning}}},
  shorttitle = {A {{Gift}} from {{Knowledge Distillation}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
  year = {2017},
  month = jul,
  pages = {7130--7138},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.754},
  urldate = {2023-09-18},
  abstract = {We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN maps from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model; (2) the student DNN outperforms the original DNN; and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\BK7756RM\Yim 等 - 2017 - A Gift from Knowledge Distillation Fast Optimizat.pdf}
}

@misc{yuSpiderLargeScaleHumanLabeled2019,
  title = {Spider: {{A Large-Scale Human-Labeled Dataset}} for {{Complex}} and {{Cross-Domain Semantic Parsing}} and {{Text-to-SQL Task}}},
  shorttitle = {Spider},
  author = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and Zhang, Zilin and Radev, Dragomir},
  year = {2019},
  month = feb,
  number = {arXiv:1809.08887},
  eprint = {1809.08887},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1809.08887},
  urldate = {2024-12-18},
  abstract = {We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4\% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\X2AZL2WH\\Yu 等 - 2019 - Spider A Large-Scale Human-Labeled Dataset for Co.pdf;C\:\\Users\\AA\\Zotero\\storage\\C4CXVVZI\\1809.html}
}

@misc{zengGLM130BOpenBilingual2023,
  title = {{{GLM-130B}}: {{An Open Bilingual Pre-trained Model}}},
  shorttitle = {{{GLM-130B}}},
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  year = {2023},
  month = oct,
  number = {arXiv:2210.02414},
  eprint = {2210.02414},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4\${\textbackslash}times\$RTX 3090 (24G) or 8\${\textbackslash}times\$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at {\textbackslash}url\{https://github.com/THUDM/GLM-130B/\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\AA\\Zotero\\storage\\8V7ZGLGS\\Zeng 等 - 2023 - GLM-130B An Open Bilingual Pre-trained Model.pdf;C\:\\Users\\AA\\Zotero\\storage\\3RRTNHTX\\2210.html}
}

@misc{zhangDataCopilotBridgingBillions2023,
  title = {Data-{{Copilot}}: {{Bridging Billions}} of {{Data}} and {{Humans}} with {{Autonomous Workflow}}},
  shorttitle = {Data-{{Copilot}}},
  author = {Zhang, Wenqi and Shen, Yongliang and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = jun,
  number = {arXiv:2306.07209},
  eprint = {2306.07209},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-27},
  abstract = {Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner. Based on this belief, we propose Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting like an experienced expert, Data-Copilot autonomously transforms raw data into visualization results that best match the user's intent. Specifically, Data-Copilot autonomously designs versatile interfaces (tools) for data management, processing, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces step by step for the user's request. The interface design and deployment processes are fully controlled by Data-Copilot itself, without human assistance. Besides, we create a Data-Copilot demo that links abundant data from different domains (stock, fund, company, economics, and live news) and accurately respond to diverse requests, serving as a reliable AI assistant.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\4ZJCLUQW\\Zhang et al_2023_Data-Copilot.pdf;C\:\\Users\\AA\\Zotero\\storage\\LERZ7RWA\\2306.html}
}

@misc{zhangHowLargeLanguage2023,
  title = {How {{Do Large Language Models Capture}} the {{Ever-changing World Knowledge}}? {{A Review}} of {{Recent Advances}}},
  shorttitle = {How {{Do Large Language Models Capture}} the {{Ever-changing World Knowledge}}?},
  author = {Zhang, Zihan and Fang, Meng and Chen, Ling and {Namazi-Rad}, Mohammad-Reza and Wang, Jun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07343},
  eprint = {2310.07343},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07343},
  urldate = {2024-12-21},
  abstract = {Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning LLMs with the ever-changing world knowledge without re-training from scratch. We categorize research works systemically and provide in-depth comparisons and discussion. We also discuss existing challenges and highlight future directions to facilitate research in this field. We release the paper list at https://github.com/hyintell/awesome-refreshing-llms},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\K4YK63EZ\\Zhang 等 - 2023 - How Do Large Language Models Capture the Ever-chan.pdf;C\:\\Users\\AA\\Zotero\\storage\\HBCSUXRK\\2310.html}
}

@misc{zhangLLMaAAMakingLarge2023,
  title = {{{LLMaAA}}: {{Making Large Language Models}} as {{Active Annotators}}},
  shorttitle = {{{LLMaAA}}},
  author = {Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.19596},
  eprint = {2310.19596},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\C4EY6W56\\Zhang 等 - 2023 - LLMaAA Making Large Language Models as Active Ann.pdf;C\:\\Users\\AA\\Zotero\\storage\\J7I82H2Q\\2310.html}
}

@misc{zhangNaturalLanguageInterfaces2023,
  title = {Natural {{Language Interfaces}} for {{Tabular Data Querying}} and {{Visualization}}: {{A Survey}}},
  shorttitle = {Natural {{Language Interfaces}} for {{Tabular Data Querying}} and {{Visualization}}},
  author = {Zhang, Weixu and Wang, Yifei and Song, Yuanfeng and Wei, Victor Junqiu and Tian, Yuxing and Qi, Yiyan and Chan, Jonathan H. and Wong, Raymond Chi-Wing and Yang, Haiqin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17894},
  eprint = {2310.17894},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17894},
  urldate = {2024-04-17},
  abstract = {The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\2J5UXYN7\\Zhang et al_2023_Natural Language Interfaces for Tabular Data Querying and Visualization.pdf;C\:\\Users\\AA\\Zotero\\storage\\7GRUK5DT\\2310.html}
}

@inproceedings{zhangVariationalReasoningQuestion2018,
  title = {Variational Reasoning for Question Answering with Knowledge Graph},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Zhang, Yuyu and Dai, Hanjun and Kozareva, Zornitsa and Smola, Alexander and Song, Le},
  year = {2018},
  volume = {32},
  doi = {10.1609/aaai.v32i1.12057},
  urldate = {2024-12-18}
}

@misc{zhaoSurveyLargeLanguage2023,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = sep,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\D8HCWTT4\\Zhao 等 - 2023 - A Survey of Large Language Models.pdf;C\:\\Users\\AA\\Zotero\\storage\\Q7R8JMJ3\\2303.html}
}

@misc{zhaTableGPTUnifyingTables2023,
  title = {{{TableGPT}}: {{Towards Unifying Tables}}, {{Nature Language}} and {{Commands}} into {{One GPT}}},
  shorttitle = {{{TableGPT}}},
  author = {Zha, Liangyu and Zhou, Junlin and Li, Liyao and Wang, Rui and Huang, Qingyi and Yang, Saisai and Yuan, Jing and Su, Changbao and Li, Xiang and Su, Aofeng and Zhang, Tao and Zhou, Chen and Shou, Kaizhe and Wang, Miao and Zhu, Wufang and Lu, Guoshan and Ye, Chao and Ye, Yali and Ye, Wentao and Zhang, Yiming and Deng, Xinglong and Xu, Jie and Wang, Haobo and Chen, Gang and Zhao, Junbo},
  year = {2023},
  month = aug,
  number = {arXiv:2307.08674},
  eprint = {2307.08674},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-16},
  abstract = {Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.},
  archiveprefix = {arXiv},
  file = {C\:\\Users\\AA\\Zotero\\storage\\99LA2HF5\\Zha et al_2023_TableGPT.pdf;C\:\\Users\\AA\\Zotero\\storage\\LKAYRYKM\\2307.html}
}

@article{ZhengJiaQiJinCouShuJuJieGouZhuanTiXuYan2024,
  title = {{紧凑数据结构专题序言}},
  author = {郑嘉琦, 谢鲲},
  year = {2024},
  month = apr,
  journal = {计算机科学},
  volume = {51},
  number = {4},
  pages = {1--3},
  issn = {1002-137X},
  doi = {10.11896/jsjkx.qy20240401},
  urldate = {2024-04-22},
  langid = {cn},
  file = {C:\Users\AA\Zotero\storage\KNDKWV5N\郑嘉琦_2024_紧凑数据结构专题序言.pdf}
}

@misc{zhengMultimodalTableUnderstanding2024,
  title = {Multimodal {{Table Understanding}}},
  author = {Zheng, Mingyu and Feng, Xinwei and Si, Qingyi and She, Qiaoqiao and Lin, Zheng and Jiang, Wenbin and Wang, Weiping},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08100},
  eprint = {2406.08100},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08100},
  urldate = {2025-02-17},
  abstract = {Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various tablerelated requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at https: //github.com/SpursGoZmy/Table-LLaVA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\AA\Zotero\storage\99TL6BEJ\Zheng 等 - 2024 - Multimodal Table Understanding.pdf}
}

@inproceedings{zhengMultimodalTableUnderstanding2024a,
  title = {Multimodal {{Table Understanding}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Mingyu and Feng, Xinwei and Si, Qingyi and She, Qiaoqiao and Lin, Zheng and Jiang, Wenbin and Wang, Weiping},
  year = {2024},
  pages = {9102--9124},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.493},
  urldate = {2025-02-17},
  abstract = {Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various tablerelated requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings. The code and data is available at https: //github.com/SpursGoZmy/Table-LLaVA.},
  langid = {english},
  file = {C:\Users\AA\Zotero\storage\LQHHMW9E\Zheng 等 - 2024 - Multimodal Table Understanding.pdf}
}

@misc{zhongFactualProbingMASK2021,
  title = {Factual {{Probing Is}} [{{MASK}}]: {{Learning}} vs. {{Learning}} to {{Recall}}},
  shorttitle = {Factual {{Probing Is}} [{{MASK}}]},
  author = {Zhong, Zexuan and Friedman, Dan and Chen, Danqi},
  year = {2021},
  month = dec,
  number = {arXiv:2104.05240},
  eprint = {2104.05240},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-26},
  abstract = {Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4\% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle "learning" from "learning to recall", providing a more detailed picture of what different prompts can reveal about pre-trained language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\9WSZ7UAX\\Zhong 等 - 2021 - Factual Probing Is [MASK] Learning vs. Learning t.pdf;C\:\\Users\\AA\\Zotero\\storage\\FVRPBYEH\\2104.html}
}

@misc{zhongSeq2SQLGeneratingStructured2017,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  shorttitle = {{{Seq2SQL}}},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year = {2017},
  month = nov,
  number = {arXiv:1709.00103},
  eprint = {1709.00103},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.00103},
  urldate = {2024-12-18},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\YSV2HVQP\\Zhong 等 - 2017 - Seq2SQL Generating Structured Queries from Natura.pdf;C\:\\Users\\AA\\Zotero\\storage\\5678WK25\\1709.html}
}

@misc{zhongSeq2SQLGeneratingStructured2017a,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  shorttitle = {{{Seq2SQL}}},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year = {2017},
  month = nov,
  number = {arXiv:1709.00103},
  eprint = {1709.00103},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.00103},
  urldate = {2024-12-18},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\3CITK6LZ\\Zhong 等 - 2017 - Seq2SQL Generating Structured Queries from Natura.pdf;C\:\\Users\\AA\\Zotero\\storage\\LU72WRRF\\1709.html}
}

@misc{zhongSeq2SQLGeneratingStructured2017b,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  shorttitle = {{{Seq2SQL}}},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year = {2017},
  month = nov,
  number = {arXiv:1709.00103},
  eprint = {1709.00103},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.00103},
  urldate = {2024-12-18},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\NIMFTNME\\Zhong 等 - 2017 - Seq2SQL Generating Structured Queries from Natura.pdf;C\:\\Users\\AA\\Zotero\\storage\\ZB2KJA95\\1709.html}
}

@article{zhongSeq2SQLGeneratingStructured2017c,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  shorttitle = {{{Seq2SQL}}},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year = {2017},
  doi = {10.48550/arXiv.1709.00103},
  urldate = {2024-12-18},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  file = {C:\Users\AA\Zotero\storage\GTBL8BD8\show.html}
}

@misc{zhongSeq2SQLGeneratingStructured2017d,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  shorttitle = {{{Seq2SQL}}},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year = {2017},
  month = nov,
  number = {arXiv:1709.00103},
  eprint = {1709.00103},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.00103},
  urldate = {2024-12-18},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\AA\\Zotero\\storage\\UYVAYR6P\\Zhong 等 - 2017 - Seq2SQL Generating Structured Queries from Natura.pdf;C\:\\Users\\AA\\Zotero\\storage\\A7UYUNVX\\1709.html}
}

@misc{zhouVisionLanguageModels2023,
  title = {Vision {{Language Models}} in {{Autonomous Driving}} and {{Intelligent Transportation Systems}}},
  author = {Zhou, Xingcheng and Liu, Mingyu and Zagar, Bare Luka and Yurtsever, Ekim and Knoll, Alois C.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.14414},
  eprint = {2310.14414},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-17},
  abstract = {The applications of Vision-Language Models (VLMs) in the fields of Autonomous Driving (AD) and Intelligent Transportation Systems (ITS) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By integrating language data, the vehicles, and transportation systems are able to deeply understand real-world environments, improving driving safety and efficiency. In this work, we present a comprehensive survey of the advances in language models in this domain, encompassing current models and datasets. Additionally, we explore the potential applications and emerging research directions. Finally, we thoroughly discuss the challenges and research gap. The paper aims to provide researchers with the current work and future trends of VLMs in AD and ITS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\AA\\Zotero\\storage\\33WBKSBE\\Zhou 等 - 2023 - Vision Language Models in Autonomous Driving and I.pdf;C\:\\Users\\AA\\Zotero\\storage\\N5E4S9RV\\2310.html}
}
@article{robertsonProbabilisticRelevanceFramework2009,
  title = {The Probabilistic Relevance Framework: {{BM25}} and Beyond},
  shorttitle = {The Probabilistic Relevance Framework},
  author = {Robertson, Stephen and Zaragoza, Hugo},
  year = {2009},
  journal = {Foundations and Trends{\textregistered} in Information Retrieval},
  volume = {3},
  number = {4},
  pages = {333--389},
  publisher = {Now Publishers, Inc.},
  urldate = {2025-03-17},
  file = {C:\Users\AA\Zotero\storage\DZE5XMA4\Robertson 和 Zaragoza - 2009 - The probabilistic relevance framework BM25 and be.pdf}
}
@inproceedings{houlsbyParameterefficientTransferLearning2019,
  title = {Parameter-Efficient Transfer Learning for {{NLP}}},
  booktitle = {International Conference on Machine Learning},
  author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  year = {2019},
  pages = {2790--2799},
  publisher = {PMLR},
  urldate = {2025-03-17},
  file = {C:\Users\AA\Zotero\storage\MCLC67P7\Houlsby 等 - 2019 - Parameter-efficient transfer learning for NLP.pdf}
}
@article{johnsonBillionscaleSimilaritySearch2019,
  title = {Billion-Scale Similarity Search with {{GPUs}}},
  author = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  year = {2019},
  journal = {IEEE Transactions on Big Data},
  volume = {7},
  number = {3},
  pages = {535--547},
  publisher = {IEEE},
  urldate = {2025-03-17}
}
@mastersthesis{1024532352.nh,
author = {何果},
 title = {基于大模型的洪涝灾害防御数字化孪生系统的研究与实现},
school = {西安理工大学},
year = {2024}
}    
@mastersthesis{1024008988.nh,
author = {孙维纬},
 title = {知识检索增强的对话系统研究},
school = {山东大学},
year = {2023}
}    
@mastersthesis{1024744443.nh,
author = {王文倩},
 title = {基于大模型的电力营销报告问答研究与应用},
school = {东华大学},
year = {2024}
}    
@mastersthesis{1024917032.nh,
author = {李诗叶},
 title = {基于知识图谱和大模型的法律领域问答系统设计与实现},
school = {武汉邮电科学研究院},
year = {2024}
}    
@phdthesis{1024651093.nh,
author = {李进},
 title = {鼠疫知识图谱和基于大模型的智能问答研究},
school = {内蒙古农业大学},
year = {2024}
}    
