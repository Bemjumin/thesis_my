\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\expandafter\ifx\csname urlstyle\endcsname\relax\else
  \urlstyle{same}\fi
\expandafter\ifx\csname href\endcsname\relax
  \DeclareUrlCommand\doi{\urlstyle{rm}}
  \def\eprint#1#2{#2}
\else
  \def\doi#1{\href{https://doi.org/#1}{\nolinkurl{#1}}}
  \let\eprint\href
\fi

\bibitem[孙维纬(2023)]{1024008988.nh}
孙维纬.
\newblock 知识检索增强的对话系统研究\allowbreak[D].
\newblock 山东大学, 2023.

\bibitem[何果(2024)]{1024532352.nh}
何果.
\newblock 基于大模型的洪涝灾害防御数字化孪生系统的研究与实现\allowbreak[D].
\newblock 西安理工大学, 2024.

\bibitem[李进(2024)]{1024651093.nh}
李进.
\newblock 鼠疫知识图谱和基于大模型的智能问答研究\allowbreak[D].
\newblock 内蒙古农业大学, 2024.

\bibitem[王文倩(2024)]{1024744443.nh}
王文倩.
\newblock 基于大模型的电力营销报告问答研究与应用\allowbreak[D].
\newblock 东华大学, 2024.

\bibitem[李诗叶(2024)]{1024917032.nh}
李诗叶.
\newblock 基于知识图谱和大模型的法律领域问答系统设计与实现\allowbreak[D].
\newblock 武汉邮电科学研究院, 2024.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{chen2018neural}
CHEN R~T, RUBANOVA Y, BETTENCOURT J, et~al.
\newblock Neural ordinary differential equations\allowbreak[J].
\newblock Advances in neural information processing systems, 2018, 31.

\bibitem[He et~al.(2024)He, Lin, Gong, Jin, Zhang, Lin, Jiao, Yiu, Duan, and Chen]{he2024annollm}
HE X, LIN Z, GONG Y, et~al.
\newblock Annollm: Making large language models to be better crowdsourced annotators\allowbreak[C]//\allowbreak
Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track).
\newblock 2024: 165-190.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{weifinetuned}
WEI J, BOSMA M, ZHAO V, et~al.
\newblock Finetuned language models are zero-shot learners\allowbreak[C]//\allowbreak
International Conference on Learning Representations.
\newblock 2022.

\bibitem[Chen et~al.(2023)Chen, Mao, Wen, Han, Jin, Zhang, Liu, and Tang]{chen2023label}
CHEN Z, MAO H, WEN H, et~al.
\newblock Label-free node classification on graphs with large language models (llms)\allowbreak[EB/OL].
\newblock arXiv:2310.04668, 2023.
\newblock \url{https://arxiv.org/abs/2310.04668}.

\bibitem[Chung et~al.(2024)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2024scaling}
CHUNG H~W, HOU L, LONGPRE S, et~al.
\newblock Scaling instruction-finetuned language models\allowbreak[J].
\newblock Journal of Machine Learning Research, 2024, 25\allowbreak (70): 1-53.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan}
LONGPRE S, HOU L, VU T, et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning\allowbreak[C]//\allowbreak
International Conference on Machine Learning.
\newblock PMLR, 2023: 22631-22648.

\bibitem[Floridi et~al.(2020)Floridi and Chiriatti]{floridi2020gpt}
FLORIDI L, CHIRIATTI M.
\newblock Gpt-3: Its nature, scope, limits, and consequences\allowbreak[J].
\newblock Minds and Machines, 2020, 30: 681-694.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
TOUVRON H, LAVRIL T, IZACARD G, et~al.
\newblock Llama: Open and efficient foundation language models\allowbreak[EB/OL].
\newblock arXiv: 2302.13971, 2023.
\newblock \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, et~al.]{team2023gemini}
TEAM G, ANIL R, BORGEAUD S, et~al.
\newblock Gemini: a family of highly capable multimodal models\allowbreak[EB/OL].
\newblock arXiv: 2312.11805, 2023.
\newblock \url{https://arxiv.org/abs/2312.11805}.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{lu2018beyond}
LU Y, ZHONG A, LI Q, et~al.
\newblock Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations\allowbreak[C]//\allowbreak
International conference on machine learning.
\newblock PMLR, 2018: 3276-3285.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycksmeasuring}
HENDRYCKS D, BURNS C, BASART S, et~al.
\newblock Measuring massive multitask language understanding\allowbreak[C]//\allowbreak
International Conference on Learning Representations.
\newblock 2021.

\bibitem[Zhang et~al.(2023)Zhang, Fang, Chen, Namazi-Rad, and Wang]{zhang2023large}
ZHANG Z, FANG M, CHEN L, et~al.
\newblock How do large language models capture the ever-changing world knowledge? a review of recent advances\allowbreak[C]//\allowbreak
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.
\newblock 2023: 8289-8311.

\bibitem[Kandpal et~al.(2023)Kandpal, Deng, Roberts, Wallace, and Raffel]{kandpal2023large}
KANDPAL N, DENG H, ROBERTS A, et~al.
\newblock Large language models struggle to learn long-tail knowledge\allowbreak[C]//\allowbreak
International Conference on Machine Learning.
\newblock PMLR, 2023: 15696-15707.

\bibitem[Gao et~al.(2023)Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, Wang, and Wang]{gao2023retrieval}
GAO Y, XIONG Y, GAO X, et~al.
\newblock Retrieval-augmented generation for large language models: A survey\allowbreak[EB/OL].
\newblock arXiv:2312.10997, 2023.
\newblock \url{https://arxiv.org/abs/2312.10997}.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih]{karpukhin2020dense}
KARPUKHIN V, OGUZ B, MIN S, et~al.
\newblock Dense passage retrieval for open-domain question answering.\allowbreak[C]//\allowbreak
EMNLP (1).
\newblock 2020: 6769-6781.

\bibitem[Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{izacard2023atlas}
IZACARD G, LEWIS P, LOMELI M, et~al.
\newblock Atlas: Few-shot learning with retrieval augmented language models\allowbreak[J].
\newblock Journal of Machine Learning Research, 2023, 24\allowbreak (251): 1-43.

\bibitem[Yao et~al.(2023)Yao, Wang, Tian, Cheng, Li, Deng, Chen, and Zhang]{yao2023editing}
YAO Y, WANG P, TIAN B, et~al.
\newblock Editing large language models: Problems, methods, and opportunities\allowbreak[C]//\allowbreak
The 2023 Conference on Empirical Methods in Natural Language Processing.
\newblock 2023.

\bibitem[Bang et~al.(2023)Bang, Cahyawijaya, Lee, Dai, Su, Wilie, Lovenia, Ji, Yu, Chung, et~al.]{bang2023multitask}
BANG Y, CAHYAWIJAYA S, LEE N, et~al.
\newblock A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\allowbreak[C]//\allowbreak
Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers).
\newblock 2023: 675-718.

\bibitem[He et~al.(2024)He, Tian, Sun, Chawla, Laurent, LeCun, Bresson, and Hooi]{he2024g}
HE X, TIAN Y, SUN Y, et~al.
\newblock G-retriever: Retrieval-augmented generation for textual graph understanding and question answering\allowbreak[J].
\newblock Advances in Neural Information Processing Systems, 2024, 37: 132876-132907.

\bibitem[Wang et~al.(2024)Wang, Lipka, Rossi, Siu, Zhang, and Derr]{wang2024knowledge}
WANG Y, LIPKA N, ROSSI R~A, et~al.
\newblock Knowledge graph prompting for multi-document question answering\allowbreak[C]//\allowbreak
Proceedings of the AAAI Conference on Artificial Intelligence: Vol.~38.
\newblock 2024: 19206-19214.

\bibitem[Chen et~al.(2024)Chen, Wang, Chen, Yu, Ma, Zhao, Zhang, and Yu]{chen2024dense}
CHEN T, WANG H, CHEN S, et~al.
\newblock Dense x retrieval: What retrieval granularity should we use?\allowbreak[C]//\allowbreak
Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing.
\newblock 2024: 15159-15177.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Geng, Tao, Ma, Lin, and Jiang]{luo2023augmented}
LUO Z, XU C, ZHAO P, et~al.
\newblock Augmented large language models with parametric knowledge guiding\allowbreak[EB/OL].
\newblock arXiv: 2305.04757, 2023.
\newblock \url{https://arxiv.org/abs/2305.04757}.

\bibitem[Luo et~al.(2023)Luo and Surdeanu]{luo2023divide}
LUO F, SURDEANU M.
\newblock Divide \& conquer for entailment-aware multi-hop evidence retrieval\allowbreak[EB/OL].
\newblock arXiv: 2311.02616, 2023.
\newblock \url{https://arxiv.org/abs/2311.02616}.

\bibitem[Wang et~al.(2023)Wang, Yang, Qiu, Liang, He, Gu, Xiao, and Wang]{wang2308knowledgpt}
WANG X, YANG Q, QIU Y, et~al.
\newblock Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases\allowbreak[EB/OL].
\newblock arXiv: 2308.11761, 2023.
\newblock \url{https://arxiv.org/abs/2308.11761}.

\bibitem[Cheng et~al.(2023)Cheng, Huang, Bi, Zhan, Liu, Wang, Sun, Wei, Deng, and Zhang]{cheng2023uprise}
CHENG D, HUANG S, BI J, et~al.
\newblock Uprise: Universal prompt retrieval for improving zero-shot evaluation\allowbreak[C]//\allowbreak
Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.
\newblock 2023: 12318-12337.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
DEVLIN J, CHANG M~W, LEE K, et~al.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding\allowbreak[C]//\allowbreak
Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers).
\newblock 2019: 4171-4186.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
RAFFEL C, SHAZEER N, ROBERTS A, et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer\allowbreak[J].
\newblock Journal of machine learning research, 2020, 21\allowbreak (140): 1-67.

\bibitem[Liu et~al.(2024)Liu, Zheng, Du, Ding, Qian, Yang, and Tang]{liu2024gpt}
LIU X, ZHENG Y, DU Z, et~al.
\newblock Gpt understands, too\allowbreak[J].
\newblock AI Open, 2024, 5: 208-215.

\bibitem[Chen(2023)]{chen2023large}
CHEN W.
\newblock Large language models are few (1)-shot table reasoners\allowbreak[C]//\allowbreak
Findings of the Association for Computational Linguistics: EACL 2023.
\newblock 2023: 1120-1130.

\bibitem[Chen et~al.(2023)Chen, Ma, Wang, and Cohen]{chenprogram}
CHEN W, MA X, WANG X, et~al.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks\allowbreak[J].
\newblock Transactions on Machine Learning Research, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
WEI J, WANG X, SCHUURMANS D, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models\allowbreak[J].
\newblock Advances in neural information processing systems, 2022, 35: 24824-24837.

\bibitem[Rajkumar et~al.(2022)Rajkumar, Li, and Bahdanau]{rajkumar2022evaluating}
RAJKUMAR N, LI R, BAHDANAU D.
\newblock Evaluating the text-to-sql capabilities of large language models\allowbreak[EB/OL].
\newblock arXiv: 2204.00498, 2022.
\newblock \url{https://arxiv.org/abs/2204.00498}.

\bibitem[Ye et~al.(2023)Ye, Hui, Yang, Li, Huang, and Li]{ye2023large}
YE Y, HUI B, YANG M, et~al.
\newblock Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning\allowbreak[C]//\allowbreak
Proceedings of the 46th international ACM SIGIR conference on research and development in information retrieval.
\newblock 2023: 174-184.

\bibitem[Wang et~al.(2024)Wang, Zhang, Li, Eisenschlos, Perot, Wang, Miculicich, Fujii, Shang, Lee, et~al.]{wang2024chain}
WANG Z, ZHANG H, LI C~L, et~al.
\newblock Chain-of-table: Evolving tables in the reasoning chain for table understanding\allowbreak[C]//\allowbreak
The Eleventh International Conference on Learning Representations.
\newblock 2024.

\bibitem[Cheng et~al.(2023)Cheng, Xie, Shi, Li, Nadkarni, Hu, Xiong, Radev, Ostendorf, Zettlemoyer, et~al.]{chengbinding}
CHENG Z, XIE T, SHI P, et~al.
\newblock Binding language models in symbolic languages\allowbreak[C]//\allowbreak
The Eleventh International Conference on Learning Representations.
\newblock 2023.

\bibitem[Zhang et~al.(2023)Zhang, Shen, Lu, and Zhuang]{zhang2024data}
ZHANG W, SHEN Y, LU W, et~al.
\newblock Data-copilot: Bridging billions of data and humans with autonomous workflow\allowbreak[C]//\allowbreak
ICLR 2024 Workshop on Large Language Model (LLM) Agents.
\newblock 2023.

\bibitem[Zha et~al.(2023)Zha, Zhou, Li, Wang, Huang, Yang, Yuan, Su, Li, Su, et~al.]{zha2023tablegpt}
ZHA L, ZHOU J, LI L, et~al.
\newblock Tablegpt: Towards unifying tables, nature language and commands into one gpt\allowbreak[EB/OL].
\newblock arXiv: 2307.08674, 2023.
\newblock \url{https://arxiv.org/abs/2307.08674}.

\bibitem[Jiang et~al.(2023)Jiang, Zhou, Ye, Zhao, Wen, et~al.]{jiang2023structgpt}
JIANG J, ZHOU K, YE K, et~al.
\newblock Structgpt: A general framework for large language model to reason over structured data\allowbreak[C]//\allowbreak
The 2023 Conference on Empirical Methods in Natural Language Processing.
\newblock 2023.

\bibitem[Kong et~al.(2024)Kong, Zhang, Shen, Srinivasan, Lei, Faloutsos, Rangwala, and Karypis]{kongopentab}
KONG K, ZHANG J, SHEN Z, et~al.
\newblock Opentab: Advancing large language models as open-domain table reasoners\allowbreak[C]//\allowbreak
The Twelfth International Conference on Learning Representations.
\newblock 2024.

\bibitem[Chen et~al.(2020)Chen, Wang, Chen, Zhang, Wang, Li, Zhou, and Wang]{chentabfact}
CHEN W, WANG H, CHEN J, et~al.
\newblock Tabfact: A large-scale dataset for table-based fact verification\allowbreak[C]//\allowbreak
International Conference on Learning Representations.
\newblock 2020.

\bibitem[Pasupat et~al.(2015)Pasupat and Liang]{pasupat2015compositional}
PASUPAT P, LIANG P.
\newblock Compositional semantic parsing on semi-structured tables\allowbreak[C]//\allowbreak
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
\newblock 2015: 1470-1480.

\bibitem[Nan et~al.(2022)Nan, Hsieh, Mao, Lin, Verma, Zhang, Kry{\'s}ci{\'n}ski, Schoelkopf, Kong, Tang, et~al.]{nan2022fetaqa}
NAN L, HSIEH C, MAO Z, et~al.
\newblock Fetaqa: Free-form table question answering\allowbreak[J].
\newblock Transactions of the Association for Computational Linguistics, 2022, 10: 35-49.

\bibitem[Yih et~al.(2016)Yih, Richardson, Meek, Chang, and Suh]{yih2016value}
YIH W~T, RICHARDSON M, MEEK C, et~al.
\newblock The value of semantic parse labeling for knowledge base question answering\allowbreak[C]//\allowbreak
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).
\newblock 2016: 201-206.

\bibitem[Zhang et~al.(2018)Zhang, Dai, Kozareva, Smola, and Song]{zhang2018variational}
ZHANG Y, DAI H, KOZAREVA Z, et~al.
\newblock Variational reasoning for question answering with knowledge graph\allowbreak[C]//\allowbreak
Proceedings of the AAAI conference on artificial intelligence: Vol.~32.
\newblock 2018.

\bibitem[Zhong et~al.(2017)Zhong, Xiong, and Socher]{zhong2017seq2sql}
ZHONG V, XIONG C, SOCHER R.
\newblock Seq2sql: Generating structured queries from natural language using reinforcement learning\allowbreak[EB/OL].
\newblock arXiv: 1709.00103, 2017.
\newblock \url{https://arxiv.org/abs/1709.00103}.

\bibitem[Yu et~al.(2018)Yu, Zhang, Yang, Yasunaga, Wang, Li, Ma, Li, Yao, Roman, et~al.]{yu2018spider}
YU T, ZHANG R, YANG K, et~al.
\newblock Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task\allowbreak[C]//\allowbreak
2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018.
\newblock Association for Computational Linguistics, 2018: 3911-3921.

\bibitem[Gan et~al.(2021)Gan, Chen, Huang, Purver, Woodward, Xie, and Huang]{gan2021towards}
GAN Y, CHEN X, HUANG Q, et~al.
\newblock Towards robustness of text-to-sql models against synonym substitution\allowbreak[C]//\allowbreak
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).
\newblock 2021: 2505-2515.

\bibitem[Kweon et~al.(2023)Kweon, Kwon, Cho, Jo, and Choi]{kweon2023open}
KWEON S, KWON Y, CHO S, et~al.
\newblock Open-wikitable: Dataset for open domain question answering with complex reasoning over table\allowbreak[C]//\allowbreak
61st Annual Meeting of the Association for Computational Linguistics, ACL 2023.
\newblock Association for Computational Linguistics (ACL), 2023: 8285-8297.

\bibitem[Deng et~al.(2021)Deng, Awadallah, Meek, Polozov, Sun, and Richardson]{deng2021structure}
DENG X, AWADALLAH A~H, MEEK C, et~al.
\newblock Structure-grounded pretraining for text-to-sql\allowbreak[C]//\allowbreak
The 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics.
\newblock 2021.

\bibitem[Aly et~al.(2021)Aly, Guo, Schlichtkrull, Thorne, Vlachos, Christodoulopoulos, Cocarascu, and Mittal]{aly2021fact}
ALY R, GUO Z, SCHLICHTKRULL M~S, et~al.
\newblock The fact extraction and verification over unstructured and structured information (feverous) shared task\allowbreak[C]//\allowbreak
Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER).
\newblock Association for Computational Linguistics, 2021.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
CHOWDHERY A, NARANG S, DEVLIN J, et~al.
\newblock Palm: Scaling language modeling with pathways\allowbreak[J].
\newblock Journal of Machine Learning Research, 2023, 24\allowbreak (240): 1-113.

\bibitem[Chen et~al.(2023)Chen, Jiang, Chen, Wang, Yu, Chen, Zhang, Liang, Zhang, Zhang, et~al.]{chen2023phoenix}
CHEN Z, JIANG F, CHEN J, et~al.
\newblock Phoenix: Democratizing chatgpt across languages\allowbreak[EB/OL].
\newblock arXiv: 2304.10453, 2023.
\newblock \url{https://arxiv.org/abs/2304.10453}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
OUYANG L, WU J, JIANG X, et~al.
\newblock Training language models to follow instructions with human feedback\allowbreak[J].
\newblock Advances in neural information processing systems, 2022, 35: 27730-27744.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
HE K, ZHANG X, REN S, et~al.
\newblock Deep residual learning for image recognition\allowbreak[C]//\allowbreak
Proceedings of the IEEE conference on computer vision and pattern recognition.
\newblock 2016: 770-778.

\bibitem[Habiba et~al.(2020)Habiba and Pearlmutter]{habiba2020neural}
HABIBA M, PEARLMUTTER B~A.
\newblock Neural ordinary differential equation based recurrent neural network model\allowbreak[C]//\allowbreak
2020 31st Irish signals and systems conference (ISSC).
\newblock IEEE, 2020: 1-6.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
RUBANOVA Y, CHEN R~T, DUVENAUD D~K.
\newblock Latent ordinary differential equations for irregularly-sampled time series\allowbreak[J].
\newblock Advances in neural information processing systems, 2019, 32.

\bibitem[Li et~al.(2021)Li, Du, Zhou, Zhou, Zeng, Xiao, and Zhu]{li2021ode}
LI B, DU Q, ZHOU T, et~al.
\newblock Ode transformer: An ordinary differential equation-inspired model for neural machine translation\allowbreak[EB/OL].
\newblock arXiv: 2104.02308, 2021.
\newblock \url{https://arxiv.org/abs/2104.02308}.

\bibitem[E(2017)]{weinan2017proposal}
E W.
\newblock A proposal on machine learning via dynamical systems\allowbreak[J].
\newblock Communications in Mathematics and Statistics, 2017, 5\allowbreak (1): 1-11.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
CYBENKO G.
\newblock Approximation by superpositions of a sigmoidal function\allowbreak[J].
\newblock Mathematics of control, signals and systems, 1989, 2\allowbreak (4): 303-314.

\bibitem[Li et~al.(2022)Li, Lin, and Shen]{li2022deep}
LI Q, LIN T, SHEN Z.
\newblock Deep learning via dynamical systems: An approximation perspective\allowbreak[J].
\newblock Journal of the European Mathematical Society, 2022, 25\allowbreak (5): 1671-1709.

\bibitem[Cao et~al.(2018)Cao, Wang, Li, Zhou, Li, and Li]{cao2018brits}
CAO W, WANG D, LI J, et~al.
\newblock Brits: Bidirectional recurrent imputation for time series\allowbreak[J].
\newblock Advances in neural information processing systems, 2018, 31.

\bibitem[Pearlmutter(1989)]{pearlmutter1989learning}
PEARLMUTTER.
\newblock Learning state space trajectories in recurrent neural networks\allowbreak[C]//\allowbreak
International 1989 Joint Conference on Neural Networks.
\newblock IEEE, 1989: 365-372.

\bibitem[Neil et~al.(2016)Neil, Pfeiffer, and Liu]{neil2016phased}
NEIL D, PFEIFFER M, LIU S~C.
\newblock Phased lstm: Accelerating recurrent network training for long or event-based sequences\allowbreak[J].
\newblock Advances in neural information processing systems, 2016, 29.

\bibitem[Che et~al.(2018)Che, Purushotham, Cho, Sontag, and Liu]{che2018recurrent}
CHE Z, PURUSHOTHAM S, CHO K, et~al.
\newblock Recurrent neural networks for multivariate time series with missing values\allowbreak[J].
\newblock Scientific reports, 2018, 8\allowbreak (1): 6085.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
VASWANI A, SHAZEER N, PARMAR N, et~al.
\newblock Attention is all you need\allowbreak[J].
\newblock Advances in neural information processing systems, 2017, 30.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2015neural}
BAHDANAU D, CHO K~H, BENGIO Y.
\newblock Neural machine translation by jointly learning to align and translate\allowbreak[C]//\allowbreak
3rd International Conference on Learning Representations, ICLR 2015.
\newblock 2015.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{sutskever2014sequence}
SUTSKEVER I, VINYALS O, LE Q~V.
\newblock Sequence to sequence learning with neural networks\allowbreak[J].
\newblock Advances in neural information processing systems, 2014, 27.

\bibitem[Fan et~al.(2018)Fan, Lewis, and Dauphin]{fan2018hierarchical}
FAN A, LEWIS M, DAUPHIN Y.
\newblock Hierarchical neural story generation\allowbreak[C]//\allowbreak
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
\newblock Association for Computational Linguistics, 2018.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzmancurious}
HOLTZMAN A, BUYS J, DU L, et~al.
\newblock The curious case of neural text degeneration\allowbreak[C]//\allowbreak
International Conference on Learning Representations.
\newblock 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
RADFORD A, WU J, CHILD R, et~al.
\newblock Language models are unsupervised multitask learners\allowbreak[J].
\newblock OpenAI blog, 2019, 1\allowbreak (8): 9.

\bibitem[Liu et~al.(2023)Liu, Fan, Wu, and Zhou]{liu2023filter}
LIU Y, FAN K, WU D, et~al.
\newblock Filter pruning by quantifying feature similarity and entropy of feature maps\allowbreak[J].
\newblock Neurocomputing, 2023, 544: 126297.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and Kalenichenko]{jacob2018quantization}
JACOB B, KLIGYS S, CHEN B, et~al.
\newblock Quantization and training of neural networks for efficient integer-arithmetic-only inference\allowbreak[C]//\allowbreak
Proceedings of the IEEE conference on computer vision and pattern recognition.
\newblock 2018: 2704-2713.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
HINTON G, VINYALS O, DEAN J.
\newblock Distilling the knowledge in a neural network\allowbreak[J].
\newblock stat, 2015, 1050: 9.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
HAN S, MAO H, DALLY W~J.
\newblock Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding\allowbreak[EB/OL].
\newblock arXiv: 1510.00149, 2015.
\newblock \url{https://arxiv.org/abs/1510.00149}.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
HOULSBY N, GIURGIU A, JASTRZEBSKI S, et~al.
\newblock Parameter-efficient transfer learning for nlp\allowbreak[C]//\allowbreak
International conference on machine learning.
\newblock PMLR, 2019: 2790-2799.

\bibitem[Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu2022p}
LIU X, JI K, FU Y, et~al.
\newblock P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks\allowbreak[C]//\allowbreak
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).
\newblock 2022: 61-68.

\bibitem[Feng et~al.(2023)Feng, Ma, Yu, Huang, Wang, Chen, Peng, Feng, Qin, et~al.]{feng2023trends}
FENG Z, MA W, YU W, et~al.
\newblock Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications\allowbreak[EB/OL].
\newblock arXiv: 2311.05876, 2023.
\newblock \url{https://arxiv.org/abs/2311.05876}.

\bibitem[Xu et~al.(2023)Xu, Ping, Wu, McAfee, Zhu, Liu, Subramanian, Bakhturina, Shoeybi, and Catanzaro]{xu2023retrieval}
XU P, PING W, WU X, et~al.
\newblock Retrieval meets long context large language models\allowbreak[C]//\allowbreak
The Twelfth International Conference on Learning Representations.
\newblock 2023.

\bibitem[Balaguer et~al.(2024)Balaguer, Benara, Cunha, Hendry, Holstein, Marsman, Mecklenburg, Malvar, Nunes, Padilha, et~al.]{balaguer2024rag}
BALAGUER A, BENARA V, CUNHA R~L~D~F, et~al.
\newblock Rag vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture\allowbreak[EB/OL].
\newblock arXiv: 2401.08406, 2024.
\newblock \url{https://arxiv.org/abs/2401.08406}.

\bibitem[Robertson et~al.(2009)Robertson, Zaragoza, et~al.]{robertson2009probabilistic}
ROBERTSON S, ZARAGOZA H, et~al.
\newblock The probabilistic relevance framework: Bm25 and beyond\allowbreak[J].
\newblock Foundations and Trends{\textregistered} in Information Retrieval, 2009, 3\allowbreak (4): 333-389.

\bibitem[Mozer et~al.(2017)Mozer, Kazakov, and Lindsey]{mozer2017discrete}
MOZER M~C, KAZAKOV D, LINDSEY R~V.
\newblock Discrete event, continuous time rnns\allowbreak[EB/OL].
\newblock arXiv: 1710.04110, 2017.
\newblock \url{https://arxiv.org/abs/1710.04110}.

\bibitem[Deng et~al.(2020)Deng, Li, Han, Shi, and Xie]{deng2020model}
DENG L, LI G, HAN S, et~al.
\newblock Model compression and hardware acceleration for neural networks: A comprehensive survey\allowbreak[J].
\newblock Proceedings of the IEEE, 2020, 108\allowbreak (4): 485-532.

\bibitem[Nomura et~al.(1993)Nomura and Elghobashi]{nomura1993structure}
NOMURA K, ELGHOBASHI S.
\newblock The structure of inhomogeneous turbulence in variable density nonpremixed flames\allowbreak[J].
\newblock Theoretical and Computational Fluid Dynamics, 1993, 5\allowbreak (4): 153-175.

\bibitem[Chatterjee(2000)]{chatterjee2000introduction}
CHATTERJEE A.
\newblock An introduction to the proper orthogonal decomposition\allowbreak[J].
\newblock Current science, 2000: 808-817.

\bibitem[Hochreiter et~al.(1997)Hochreiter and Schmidhuber]{hochreiter1997long}
HOCHREITER S, SCHMIDHUBER J.
\newblock Long short-term memory\allowbreak[J].
\newblock Neural computation, 1997, 9\allowbreak (8): 1735-1780.

\bibitem[Anguita et~al.(2012)Anguita, Ghio, Oneto, Parra, and Reyes-Ortiz]{anguita2012human}
ANGUITA D, GHIO A, ONETO L, et~al.
\newblock Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine\allowbreak[C]//\allowbreak
Ambient Assisted Living and Home Care: 4th International Workshop, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings 4.
\newblock Springer, 2012: 216-223.

\bibitem[Xue et~al.(2013)Xue, Li, and Gong]{xue2013restructuring}
XUE J, LI J, GONG Y.
\newblock Restructuring of deep neural network acoustic models with singular value decomposition\allowbreak[J].
\newblock Interspeech 2013, 2013.

\bibitem[Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou]{johnson2019billion}
JOHNSON J, DOUZE M, J{\'E}GOU H.
\newblock Billion-scale similarity search with gpus\allowbreak[J].
\newblock IEEE Transactions on Big Data, 2019, 7\allowbreak (3): 535-547.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
BAI J, BAI S, CHU Y, et~al.
\newblock Qwen technical report\allowbreak[EB/OL].
\newblock arXiv: 2309.16609, 2023.
\newblock \url{https://arxiv.org/abs/2309.16609}.

\bibitem[Sarzynska-Wawer et~al.(2021)Sarzynska-Wawer, Wawer, Pawlak, Szymanowska, Stefaniak, Jarkiewicz, and Okruszek]{sarzynska2021detecting}
SARZYNSKA-WAWER J, WAWER A, PAWLAK A, et~al.
\newblock Detecting formal thought disorder by deep contextualized word representations\allowbreak[J].
\newblock Psychiatry Research, 2021, 304: 114135.

\bibitem[Tian et~al.(2016)Tian, Huang, He, He, and Qiao]{tian2016detecting}
TIAN Z, HUANG W, HE T, et~al.
\newblock Detecting text in natural image with connectionist text proposal network\allowbreak[C]//\allowbreak
Computer vision--ECCV 2016: 14th European conference, amsterdam, the netherlands, October 11-14, 2016, proceedings, part VIII 14.
\newblock Springer, 2016: 56-72.

\end{thebibliography}
